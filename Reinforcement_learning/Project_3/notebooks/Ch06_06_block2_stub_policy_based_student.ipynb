{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6174cb79",
   "metadata": {},
   "source": [
    "# 06 — Projekt pod blok 2 (policy-based) — bez pełnych implementacji\n",
    "\n",
    "Ten notebook to **most organizacyjny**: po bloku 1 (MC/TD/TD(λ) + aproksymacja) można płynnie wejść w:\n",
    "- Policy Gradient (REINFORCE),\n",
    "- Actor-Critic,\n",
    "- PPO,\n",
    "- i na końcu projekt MuJoCo.\n",
    "\n",
    "Nie implementujemy tu jeszcze algorytmów w całości — to jest „notatnik prowadzącego + TODO”.\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Dlaczego przechodzimy do policy-based?\n",
    "\n",
    "**Problem z value-based w akcjach ciągłych**:  \n",
    "w Q-learning potrzebujesz $\\arg\\max_a Q(s,a)$.  \n",
    "Jeśli $a$ jest ciągłe (wektor float), to:\n",
    "- nie da się po prostu przeszukać wszystkich akcji,\n",
    "- potrzebujesz osobnej optymalizacji / aktora, który maksymalizuje Q,\n",
    "- co praktycznie prowadzi do metod typu Actor-Critic (DDPG/TD3/SAC) albo PPO (policy-based).\n",
    "\n",
    "**Wniosek dydaktyczny:**  \n",
    "Tablice → aproksymacja → ciągłe akcje → uczymy **politykę**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Policy Gradient (REINFORCE) — idea\n",
    "\n",
    "Cel:\n",
    "$\n",
    "J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\sum_t \\gamma^t r_t\\right]\n",
    "$\n",
    "\n",
    "Wersja intuicyjna:\n",
    "- generujesz epizody z \\(\\pi_\\theta\\),\n",
    "- wzmacniasz log-prob akcji, które prowadziły do dużego zwrotu.\n",
    "\n",
    "Warto zwrócić uwagę:\n",
    "- to jest MC-style (czeka na zwrot),\n",
    "- dlatego dodaje się baseline → mniejsza wariancja.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Actor-Critic — to co już znasz + aktor\n",
    "\n",
    "- **Critic**: TD (jak w bloku 1), ale dla \\(\\hat V(s;w)\\) lub \\(\\hat Q(s,a;w)\\)\n",
    "- **Actor**: aktualizuje \\(\\theta\\) w kierunku, który zwiększa prawdopodobieństwo dobrych akcji\n",
    "\n",
    "Kluczowe spoiwo: **advantage**\n",
    "$\n",
    "A(s,a) \\approx r + \\gamma V(s') - V(s)\n",
    "$\n",
    "To jest dokładnie TD error.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) PPO — praktyczny standard\n",
    "\n",
    "PPO stabilizuje policy gradient przez ograniczenie, jak bardzo polityka może się zmienić w jednym kroku.\n",
    "Najczęściej spotkasz „clipped objective”.\n",
    "\n",
    "W bloku 2:\n",
    "- pokażemy najpierw czemu czyste REINFORCE bywa niestabilne,\n",
    "- potem Actor-Critic,\n",
    "- a na końcu PPO jako „ładnie działającą” wersję.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) (Opcjonalnie) GRPO?\n",
    "\n",
    "Skrót „GRPO” spotyka się w kontekście RLHF dla modeli językowych (inne realia niż MuJoCo).\n",
    "Dla klasycznego continuous-control w RL najczęściej pojawiają się:\n",
    "- PPO,\n",
    "- SAC,\n",
    "- TD3.\n",
    "W tym kursie naturalnie: PPO (bo proste do wdrożenia i stabilne).\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Projekt MuJoCo — checklista\n",
    "\n",
    "**Minimalny projekt, który wygląda profesjonalnie:**\n",
    "- wybór zadania (np. Hopper / HalfCheetah / Walker2d),\n",
    "- metryki: average return + success rate (jeśli ma sens),\n",
    "- seedowanie i powtarzalność,\n",
    "- logowanie krzywych uczenia,\n",
    "- 2–3 ablation studies (np. różne γ, różne clip range, różne entropy bonus),\n",
    "- porównanie z baseline (np. random / prosty heurystyczny).\n",
    "\n",
    "**Sanity checks** (zwykle ratują życie):\n",
    "- czy reward rośnie w ogóle?\n",
    "- czy gradients nie są NaN?\n",
    "- czy akcje są w sensownym zakresie?\n",
    "- czy normalizacja obserwacji pomaga?\n",
    "\n",
    "---\n",
    "\n",
    "## TODO na blok 2\n",
    "\n",
    "1) Ustalić wspólny szkielet:\n",
    "- `collect_trajectories()`\n",
    "- `compute_returns_and_advantages()`\n",
    "- `update_policy()`\n",
    "- `update_value()`\n",
    "\n",
    "2) Zrobić REINFORCE (najpierw bez baseline, potem z baseline)\n",
    "\n",
    "3) Przerobić na Actor-Critic (TD(0) jako critic)\n",
    "\n",
    "4) Zrobić PPO (clip + minibatches)\n",
    "\n",
    "5) Przełączyć środowisko na MuJoCo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd706436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# dodaj katalog główny repo do ścieżki importów\n",
    "REPO_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if REPO_ROOT not in sys.path:\n",
    "    sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a79740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block2Config(env_id='Hopper-v4', gamma=0.99, seed=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Placeholdery (nie odpalamy jeszcze algorytmów policy-based)\n",
    "# W bloku 2 wstawimy tu implementację na bazie np. PyTorch.\n",
    "# Na razie: tylko struktura, żeby przejście było płynne.\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Block2Config:\n",
    "    env_id: str = \"Hopper-v4\"\n",
    "    gamma: float = 0.99\n",
    "    seed: int = 0\n",
    "\n",
    "cfg = Block2Config()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb017d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca6408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
