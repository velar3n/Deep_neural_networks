{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39323717",
   "metadata": {},
   "source": [
    "Autor: Natalia Kiełbasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a4d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bedd2b",
   "metadata": {},
   "source": [
    "## Mini-projekt (DP): „Co zmienia optymalną politykę i wartość startu?”\n",
    "\n",
    "### Kontekst\n",
    "W rozdziale 4 zrobiliśmy Dynamic Programming dla tablicowych MDP:\n",
    "- mamy **model** środowiska w postaci `P[s][a] = [(p, s2, r, terminated), ...]`,\n",
    "- umiemy liczyć:\n",
    "  - `v_π` (policy evaluation),\n",
    "  - poprawiać politykę (policy improvement),\n",
    "  - znajdować optimum: `π*`, `v*` (policy iteration / value iteration).\n",
    "\n",
    "W tym mini-projekcie wykonasz **krótkie badanie**: jak zmiana parametrów środowiska lub zadania wpływa na:\n",
    "- `v*(start)` — wartość stanu startowego,\n",
    "- oraz (opcjonalnie) kształt polityki `π*` (strzałki).\n",
    "\n",
    "---\n",
    "\n",
    "## Co to jest `v(start)` w skrócie (potem rozwinięte, jeśli ktoś potrzebuje)?\n",
    "`v(start)` to wartość funkcji wartości w stanie startowym:\n",
    "\n",
    "$\n",
    "v(start) = V_\\pi(s_{\\text{start}})\n",
    "$\n",
    "\n",
    "czyli oczekiwana (zdyskontowana) suma nagród, gdy startujesz w `start` i działasz według polityki `π`.\n",
    "W praktyce to jedna liczba odpowiadająca na pytanie:\n",
    "\n",
    "> **„Jak dobry jest ten sposób działania, jeśli startuję z typowego stanu startowego?”**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2689595",
   "metadata": {},
   "source": [
    "## Co oznacza dokładnie `v(start)` i jak policzyć (powtórzenie powtórzenia :) )?\n",
    "\n",
    "`v(start)` to **wartość stanu startowego** (czyli wartość funkcji wartości w stanie, od którego zwykle zaczynamy):\n",
    "\n",
    "$\n",
    "v(start) = V_\\pi(s_{\\text{start}})\n",
    "$\n",
    "\n",
    "### Definicja (intuicja)\n",
    "Jeśli zaczynamy w stanie `start` i dalej postępujemy zgodnie z polityką \\(\\pi\\),\n",
    "to `v(start)` jest **oczekiwaną (zdyskontowaną) sumą przyszłych nagród**:\n",
    "\n",
    "$\n",
    "V_\\pi(s_{\\text{start}})=\\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t R_{t+1}\\ \\middle|\\ S_0=s_{\\text{start}}\\right]\n",
    "$\n",
    "\n",
    "Czyli to jedna liczba, która odpowiada na pytanie:\n",
    "\n",
    "> **„Jak dobry jest ten sposób działania (polityka), jeśli zaczynam od stanu startowego?”**\n",
    "\n",
    "---\n",
    "\n",
    "## Jak interpretować `v(start)` w naszych środowiskach?\n",
    "\n",
    "### 1) FrozenLake (epizodyczne, nagroda na końcu)\n",
    "W FrozenLake nagroda jest zwykle:\n",
    "- `+1` za wejście na pole `G` (goal),\n",
    "- `0` w pozostałych przypadkach,\n",
    "- epizod kończy się na `H` (hole) lub `G`.\n",
    "\n",
    "W takim układzie `v(start)` można interpretować jako:\n",
    "\n",
    "> **zdyskontowaną “szansę sukcesu”** (dotarcia do celu), uwzględniając ryzyko i γ.\n",
    "\n",
    "- `slippery=True` obniża `v(start)`, bo ruch jest mniej kontrolowalny → częściej kończysz w dziurze.\n",
    "- większe `γ` sprawia, że bardziej „opłaca się” dążyć do odległego celu (mniej karzesz opóźnienie).\n",
    "\n",
    "---\n",
    "\n",
    "## Dlaczego `v(start)` jest przydatne?\n",
    "\n",
    "To **jedna liczba** (prosty wskaźnik), dzięki której można:\n",
    "- porównywać różne polityki (losowa vs optymalna),\n",
    "- porównywać różne ustawienia środowiska (np. `slippery`),\n",
    "- porównywać różne parametry (`γ`, kary, nagrody teleportów),\n",
    "- robić wykresy typu: *parametr → jakość zachowania*.\n",
    "\n",
    "W praktyce to DP-odpowiednik „średniego returnu” znanego z RL,\n",
    "tylko że tu liczymy go z modelu (bez próbkowania epizodów).\n",
    "\n",
    "---\n",
    "\n",
    "## Jak policzyć `v(start)` w notebooku?\n",
    "\n",
    "Zakładamy, że masz:\n",
    "- `v` jako wektor wartości stanów (np. wynik policy/value iteration albo policy evaluation),\n",
    "- `start_state` (u nas zwykle `0`).\n",
    "\n",
    "Wtedy (po dowolnej metodzie ___ iteration):\n",
    "\n",
    "```python\n",
    "v_start = float(v[start_state])\n",
    "print(\"v(start) =\", v_start)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05722c92",
   "metadata": {},
   "source": [
    "## Przykład rozgrzewkowy (template): wpływ γ na `v*(start)` w FrozenLake\n",
    "\n",
    "### W tym przykładzie **Krok 1 i Krok 2 są już wykonane**:\n",
    "- środowisko jest wybrane (`P_fl_slip`),\n",
    "- w każdej iteracji liczymy optimum (`π*`, `v*`) metodą `value_iteration`.\n",
    "\n",
    "Poniższy kod jest **wzorcem**, który skopiujesz później dla Gridworld.\n",
    "Przyjmujemy konwencję `start_state = 0` (lewy górny róg / pole `S`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4603c9e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'value_iteration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m start_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m gammas:\n\u001b[1;32m----> 9\u001b[0m     pi_star, v_star \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_iteration\u001b[49m(P_fl_slip, gamma\u001b[38;5;241m=\u001b[39mg, theta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-10\u001b[39m)\n\u001b[0;32m     10\u001b[0m     vstarts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(v_star[start_state]))\n\u001b[0;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'value_iteration' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "gammas = [0.8, 0.9, 0.95, 0.99,0.999]\n",
    "vstarts = []\n",
    "\n",
    "start_state = 0\n",
    "for g in gammas:\n",
    "    pi_star, v_star = value_iteration(P_fl_slip, gamma=g, theta=1e-10)\n",
    "    vstarts.append(float(v_star[start_state]))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(gammas, vstarts, marker='o')\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"v*(start)\")\n",
    "plt.title(\"FrozenLake slippery: v*(start) vs gamma\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(list(zip(gammas, vstarts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894e5ee",
   "metadata": {},
   "source": [
    "## Przykładowa ścieżka rozwiązania pytania badawczego A (porównawcze): Jak `γ` działa w zadaniu epizodycznym vs continuing?\n",
    "W ramach projektu można po prostu ją przeprowadzić wg opisu (prawie gotowe :) )\n",
    "tylko trzeba przeprowadzić te eksperymety, zrozumieć i opisać swoimi słowami.\n",
    "\n",
    "### Cel\n",
    "Porównaj wpływ parametru dyskontowania `γ` na `v*(start)` w dwóch różnych typach zadań:\n",
    "\n",
    "- **FrozenLake** (epizodyczne): nagroda jest na końcu epizodu (dotarcie do `G`)\n",
    "- **Gridworld** (continuing): proces trwa bez końca (zbieramy nagrody w nieskończonym horyzoncie)\n",
    "\n",
    "### Dlaczego to jest ciekawe?\n",
    "`γ` kontroluje, jak mocno cenisz przyszłość.  \n",
    "W epizodycznym zadaniu jego wpływ wygląda inaczej niż w continuing, bo:\n",
    "- w FrozenLake nagroda zwykle pojawia się po kilku krokach (krótka trajektoria),\n",
    "- w Gridworld liczy się „długoterminowe zachowanie” w nieskończonym horyzoncie.\n",
    "\n",
    "### Co mierzymy\n",
    "Dla każdej wartości `γ` liczymy:\n",
    "- `v*(start)` w FrozenLake (det lub slippery),\n",
    "- `v*(start)` w Gridworld.\n",
    "\n",
    "Następnie robimy wykres:\n",
    "- oś X: `γ`,\n",
    "- oś Y: `v*(start)`.\n",
    "\n",
    "### Zakres γ (proponowany)\n",
    "Użyj np. listy:\n",
    "`gammas = [0.8, 0.9, 0.95, 0.99]`\n",
    "\n",
    "> Uwaga: w continuing zadaniach DP zakładamy `γ < 1`, bo wtedy operator Bellmana jest kontrakcją\n",
    "> i wartości są dobrze określone.\n",
    "\n",
    "---\n",
    "\n",
    "## Przykład (wzór implementacyjny) — wykonany dla FrozenLake\n",
    "\n",
    "Poniższa komórka pokazuje, jak zrobić sweep po `γ` dla jednego środowiska.  \n",
    "W kolejnym kroku zastosujesz ten sam wzór dla Gridworld.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf40ce7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'value_iteration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m vstarts_fl \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m gammas:\n\u001b[1;32m----> 8\u001b[0m     pi_star, v_star \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_iteration\u001b[49m(P_fl_det, gamma\u001b[38;5;241m=\u001b[39mg, theta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-10\u001b[39m)\n\u001b[0;32m      9\u001b[0m     vstarts_fl\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(v_star[start_state]))\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'value_iteration' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gammas = [0.8, 0.9, 0.95, 0.99]\n",
    "start_state = 0\n",
    "\n",
    "vstarts_fl = []\n",
    "for g in gammas:\n",
    "    pi_star, v_star = value_iteration(P_fl_det, gamma=g, theta=1e-10)\n",
    "    vstarts_fl.append(float(v_star[start_state]))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(gammas, vstarts_fl, marker='o')\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"v*(start)\")\n",
    "plt.title(\"FrozenLake (det): v*(start) vs gamma\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(list(zip(gammas, vstarts_fl)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7384637",
   "metadata": {},
   "source": [
    "## Twoje zadanie: zrób to samo dla Gridworld i porównaj\n",
    "\n",
    "1) Powtórz sweep po `γ` dla **Gridworld** (ten sam `gammas`, ten sam `start_state=0`).\n",
    "2) Zrób jeden wspólny wykres z dwiema krzywymi:\n",
    "   - FrozenLake (det lub slippery),\n",
    "   - Gridworld.\n",
    "\n",
    "### Kod (szablon — uzupełnij tylko brakujące fragmenty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02cb6e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'v_star' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m vstarts_gw \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m gammas:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# TODO: policz v* w Gridworld (value_iteration albo policy_iteration)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Wykonaj algorytm i wstaw do vstarts_gw - można skorzystać z wcześniejszych implementacji/wywołań - wyżej jest przykład\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     vstarts_gw\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(\u001b[43mv_star\u001b[49m[start_state]))\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(gammas, vstarts_fl, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'v_star' is not defined"
     ]
    }
   ],
   "source": [
    "vstarts_gw = []\n",
    "for g in gammas:\n",
    "    # TODO: policz v* w Gridworld (value_iteration albo policy_iteration)\n",
    "    # Wykonaj algorytm i wstaw do vstarts_gw - można skorzystać z wcześniejszych implementacji/wywołań - wyżej jest przykład\n",
    "    vstarts_gw.append(float(v_star[start_state]))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(gammas, vstarts_fl, marker='o', label=\"FrozenLake\")\n",
    "plt.plot(gammas, vstarts_gw, marker='o', label=\"Gridworld\")\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"v*(start)\")\n",
    "plt.title(\"Porównanie: FrozenLake vs Gridworld (wpływ gamma)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"FrozenLake:\", list(zip(gammas, vstarts_fl)))\n",
    "print(\"Gridworld:\", list(zip(gammas, vstarts_gw)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37c1186",
   "metadata": {},
   "source": [
    "## Pytania do analizy (napisz 4–8 zdań)\n",
    "\n",
    "1) Jak zmienia się `v*(start)` wraz z `γ` w FrozenLake?  \n",
    "   (Podpowiedź: w det często widać zależność ~ `γ^L`, gdzie L to długość najlepszej ścieżki.)\n",
    "\n",
    "2) Jak wygląda wpływ `γ` w Gridworld? Czy jest „silniejszy” / „słabszy” niż w FrozenLake?\n",
    "\n",
    "3) Dlaczego te dwa światy zachowują się inaczej?  \n",
    "   Użyj pojęć: epizodyczne vs continuing, horyzont czasowy, nagroda na końcu vs nagrody w nieskończonym czasie.\n",
    "\n",
    "4) Czy przy bardzo dużym `γ` wartości w Gridworld robią się bardzo duże?  \n",
    "   Jeśli tak, co to mówi o naturze continuing zadań i roli dyskontowania?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc0ad2",
   "metadata": {},
   "source": [
    "## Zadanie (obowiązkowe)\n",
    "Twoim celem jest wykonać **eksperyment + analizę** (badanie). Zrób to w krokach:\n",
    "\n",
    "### Krok 1 — wybierz środowisko\n",
    "Wybierz jedno (albo zrób oba jako bonus):\n",
    "- FrozenLake (`P_fl_det` lub `P_fl_slip`) — epizodyczne (Start - Goal, pamiętacie?:) ),\n",
    "- Gridworld (`P_gw` lub większy Gridworld, jeśli jest w notebooku) — continuing (jak wyżej :) ).\n",
    "\n",
    "Ustal `start_state` (zwykle `0`).\n",
    " v(start) to jedna liczba, która pozwala porównywać eksperymenty.\n",
    "Musisz więc wskazać z jakiego stanu startujesz.\n",
    "\n",
    "### Krok 2 — policz optimum (punkt odniesienia)\n",
    "Policz `π*` i `v*` metodą DP:\n",
    "- `value_iteration(...)` lub `policy_iteration(...)`.\n",
    "\n",
    "Zapisz `v*(start)`.\n",
    "\n",
    "### Krok 3 — wybierz 2 pytania badawcze i wykonaj eksperymenty\n",
    "Wybierz **dwa** pytania z listy poniżej i wykonaj pomiary (sweep parametrów).\n",
    "\n",
    "### Krok 4 — zrób wykresy\n",
    "Dla każdego pytania przygotuj co najmniej **1 wykres**.\n",
    "Minimalnie: wykres parametru (X) → `v*(start)` (Y).\n",
    "\n",
    "### Krok 5 — opisz wnioski (krótko, ale konkretnie)\n",
    "Napisz 4–8 zdań:\n",
    "- co zmieniałeś,\n",
    "- co zmierzyłeś,\n",
    "- co wyszło i dlaczego (intuicja + odniesienie do definicji Bellmana).\n",
    "\n",
    "---\n",
    "\n",
    "## Pytania badawcze (wybierz 2)\n",
    "\n",
    "### Pytanie A — wpływ `γ` (gamma)\n",
    "**Hipoteza do sprawdzenia:** większe `γ` zwiększa znaczenie przyszłości i zmienia `v*(start)` oraz czasem politykę.\n",
    "\n",
    "Zrób sweep po `γ`, np. `[0.8, 0.9, 0.95, 0.99]`.\n",
    "Dla każdego `γ` policz `v*(start)`.\n",
    "\n",
    "**Wykres:** `γ` vs `v*(start)`.\n",
    "\n",
    "---\n",
    "\n",
    "### Pytanie B — deterministyczne vs slippery (FrozenLake)\n",
    "**Hipoteza do sprawdzenia:** stochastyczność (`slippery=True`) obniża `v*(start)` i może zmienić `π*`.\n",
    "\n",
    "Porównaj:\n",
    "- `P_fl_det` vs `P_fl_slip` (dla tego samego `γ`).\n",
    "\n",
    "**Wykres:** dwa punkty/kolumny (det vs slippery) dla `v*(start)` (albo dwa przebiegi vs γ).\n",
    "\n",
    "---\n",
    "\n",
    "### Pytanie C — wpływ kar/nagród w Gridworld\n",
    "**Hipoteza do sprawdzenia:** większe kary za ścianę lub mniejsze nagrody teleportów zmieniają `v*` i „kierunek” polityki.\n",
    "\n",
    "Zrób sweep po jednym parametrze środowiska (jeśli masz builder świata):\n",
    "- `wall_penalty ∈ {-1, -2, -5}`,\n",
    "- lub nagrody teleportów (np. zmniejsz/zwieksz).\n",
    "\n",
    "**Wykres:** parametr vs `v*(start)`.\n",
    "\n",
    "---\n",
    "\n",
    "### Pytanie D — PI vs VI jako metody numeryczne\n",
    "**Hipoteza do sprawdzenia:** PI ma mniej iteracji „zewnętrznych”, a VI robi więcej prostych backupów.\n",
    "\n",
    "Zmierz czas działania (np. `time.time()`) dla PI i VI na tym samym środowisku.\n",
    "\n",
    "**Wykres:** słupki czasu (PI vs VI) lub liczba iteracji (jeśli zapisujesz liczbę iteracji).\n",
    "\n",
    "> Uwaga: polityki mogą się różnić w stanach z remisami, ale `v*` powinno być takie samo (w granicach numeryki).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e29f7",
   "metadata": {},
   "source": [
    "## Krok 1 - wybór środowiska - FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bbc8526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dodanie potrzebnych metod\n",
    "\n",
    "def pretty_matrix_as_grid(v: np.ndarray, nrow: int, ncol: int, decimals: int = 1):\n",
    "    \"\"\"Wyświetla wektor wartości jako siatkę nrow x ncol.\"\"\"\n",
    "    grid = np.asarray(v, dtype=float).reshape(nrow, ncol)\n",
    "    with np.printoptions(precision=decimals, suppress=True):\n",
    "        print(grid)\n",
    "\n",
    "def action_arrows(pi_det: np.ndarray, nrow: int, ncol: int, arrows):\n",
    "    \"\"\"Wyświetla deterministyczną politykę jako strzałki w siatce.\"\"\"\n",
    "    out = []\n",
    "    for r in range(nrow):\n",
    "        row = []\n",
    "        for c in range(ncol):\n",
    "            s = r * ncol + c\n",
    "            a = int(pi_det[s])\n",
    "            row.append(arrows.get(a, '?'))\n",
    "        out.append(' '.join(row))\n",
    "    print('\\n'.join(out))\n",
    "\n",
    "\n",
    "def build_P_r_for_policy(P, pi: np.ndarray):\n",
    "    \"\"\"Buduje (P_pi, r_pi) dla danej polityki pi na podstawie modelu P[s][a].\n",
    "\n",
    "    Zwraca:\n",
    "    - P_pi: macierz przejść (nS x nS)\n",
    "    - r_pi: wektor nagród oczekiwanych (nS,)\n",
    "\n",
    "    Uwaga: dla zadań epizodycznych (FrozenLake) działa poprawnie, o ile stany terminalne są absorbujące\n",
    "    z nagrodą 0 (tak jest w standardowym FrozenLake).\n",
    "    \"\"\"\n",
    "    nS, nA = pi.shape\n",
    "    P_pi = np.zeros((nS, nS), dtype=float)\n",
    "    r_pi = np.zeros(nS, dtype=float)\n",
    "\n",
    "    for s in range(nS):\n",
    "        for a in range(nA):\n",
    "            w = float(pi[s, a])\n",
    "            if w == 0.0:\n",
    "                continue\n",
    "            for (p, s2, r, terminated) in P[s][a]:\n",
    "                P_pi[s, int(s2)] += w * float(p)\n",
    "                r_pi[s] += w * float(p) * float(r)\n",
    "    return P_pi, r_pi\n",
    "\n",
    "\n",
    "def evaluate_policy_linear_system(P_pi: np.ndarray, r_pi: np.ndarray, gamma: float) -> np.ndarray:\n",
    "    \"\"\"Dokładna ewaluacja polityki przez układ liniowy: (I - gamma P_pi) v = r_pi.\"\"\"\n",
    "    nS = P_pi.shape[0]\n",
    "    I = np.eye(nS)\n",
    "    return np.linalg.solve(I - gamma * P_pi, r_pi)\n",
    "\n",
    "\n",
    "def build_frozenlake_P(desc, is_slippery: bool = False):\n",
    "    \"\"\"\n",
    "    Buduje model FrozenLake w formacie P[s][a] bez użycia Gym.\n",
    "\n",
    "    desc: lista stringów opisujących mapę, np.\n",
    "          [\"SFFF\",\n",
    "           \"FHFH\",\n",
    "           \"FFFH\",\n",
    "           \"HFFG\"]\n",
    "\n",
    "    Konwencja akcji (jak w Gym):\n",
    "      0 = LEFT, 1 = DOWN, 2 = RIGHT, 3 = UP\n",
    "\n",
    "    Nagroda:\n",
    "      +1 za wejście na G, w pozostałych przypadkach 0.\n",
    "\n",
    "    Stany terminalne:\n",
    "      H (hole) i G (goal).\n",
    "    \"\"\"\n",
    "\n",
    "    # Zamieniamy opis mapy na tablicę znaków\n",
    "    desc = np.asarray([list(row) for row in desc], dtype='<U1')\n",
    "    nrow, ncol = desc.shape\n",
    "    nS, nA = nrow * ncol, 4   # liczba stanów i akcji\n",
    "\n",
    "    # Indeksy akcji (zgodne z Gym)\n",
    "    LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "    # Wektory ruchu dla każdej akcji\n",
    "    moves = {\n",
    "        LEFT:  (0, -1),\n",
    "        DOWN:  (1,  0),\n",
    "        RIGHT: (0,  1),\n",
    "        UP:    (-1, 0),\n",
    "    }\n",
    "\n",
    "    # Zamiana (r, c) -> indeks stanu\n",
    "    def to_s(r, c):\n",
    "        return r * ncol + c\n",
    "\n",
    "    # Jeden krok z pozycji (r, c) dla akcji a\n",
    "    def step_from(r, c, a):\n",
    "        dr, dc = moves[a]\n",
    "        r2, c2 = r + dr, c + dc\n",
    "\n",
    "        # FrozenLake: wyjście poza planszę = zostajemy w miejscu\n",
    "        if (r2 < 0) or (r2 >= nrow) or (c2 < 0) or (c2 >= ncol):\n",
    "            r2, c2 = r, c\n",
    "\n",
    "        return r2, c2\n",
    "\n",
    "    # Inicjalizacja struktury P[s][a]\n",
    "    P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "    # Iterujemy po wszystkich polach planszy\n",
    "    for r in range(nrow):\n",
    "        for c in range(ncol):\n",
    "            s = to_s(r, c)\n",
    "            tile = desc[r, c]\n",
    "\n",
    "            # Stany terminalne (H i G):\n",
    "            # są absorbujące — niezależnie od akcji zostajemy w tym samym stanie\n",
    "            if tile in ('H', 'G'):\n",
    "                for a in range(nA):\n",
    "                    P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                continue\n",
    "\n",
    "            # Stany nieterminalne\n",
    "            for a in range(nA):\n",
    "                outcomes = []\n",
    "\n",
    "                # Wersja śliska: akcja nie realizuje się dokładnie\n",
    "                if is_slippery:\n",
    "                    # Faktyczna akcja to a-1, a, a+1 z prawd. 1/3\n",
    "                    candidates = [(a - 1) % 4, a, (a + 1) % 4]\n",
    "                    probs = [1/3, 1/3, 1/3]\n",
    "                else:\n",
    "                    # Wersja deterministyczna\n",
    "                    candidates = [a]\n",
    "                    probs = [1.0]\n",
    "\n",
    "                # Dla każdej możliwej „rzeczywistej” akcji\n",
    "                for a_real, p in zip(candidates, probs):\n",
    "                    r2, c2 = step_from(r, c, a_real)\n",
    "                    s2 = to_s(r2, c2)\n",
    "                    tile2 = desc[r2, c2]\n",
    "\n",
    "                    # Sprawdzamy, czy trafiliśmy w stan terminalny\n",
    "                    terminated = tile2 in ('H', 'G')\n",
    "\n",
    "                    # Nagroda tylko za wejście na G\n",
    "                    reward = 1.0 if tile2 == 'G' else 0.0\n",
    "\n",
    "                    outcomes.append((float(p), int(s2), float(reward), bool(terminated)))\n",
    "\n",
    "                # Scalanie identycznych następstw\n",
    "                # (różne a_real mogą prowadzić do tego samego s2)\n",
    "                merged = {}\n",
    "                for p, s2, rwd, term in outcomes:\n",
    "                    key = (s2, rwd, term)\n",
    "                    merged[key] = merged.get(key, 0.0) + p\n",
    "\n",
    "                # Finalna lista wyników dla (s, a)\n",
    "                P[s][a] = [\n",
    "                    (p, s2, rwd, term)\n",
    "                    for (s2, rwd, term), p in merged.items()\n",
    "                ]\n",
    "\n",
    "    return P, nS, nA, nrow, ncol, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e4c69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ustalamy wstępne start_state\n",
    "start_state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87dde86",
   "metadata": {},
   "source": [
    "## Krok 2 - policzenie optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "867d4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implpementacja funkcji greedy_policy_from_v na podstawie drugiego notebooka\n",
    "def greedy_policy_from_v(P, v: np.ndarray, gamma: float = 0.9) -> np.ndarray:\n",
    "    \"\"\"Zachłanna poprawa polityki (policy improvement).\n",
    "\n",
    "    (Ćwiczenie 2)\n",
    "    Zaimplementuj funkcję, która dla danego `v` buduje politykę deterministyczną `pi_det`:\n",
    "        pi_det[s] = argmax_a Q(s,a)\n",
    "\n",
    "    gdzie:\n",
    "        Q(s,a) = E[ r + gamma * v(s') ]\n",
    "\n",
    "    Wymagania:\n",
    "    - zwróć `pi_det` jako wektor intów o kształcie (nS,)\n",
    "    - użyj pełnego modelu: przejścia są w `P[s][a]` jako (p, s2, r, terminated)\n",
    "    - obsłuż `terminated` identycznie jak w policy evaluation:\n",
    "        jeśli `terminated=True`, bootstrap = 0\n",
    "\n",
    "    Wskazówki:\n",
    "    - pętla po stanach `s`, wewnątrz pętla po akcjach `a`\n",
    "    - dla każdej akcji policz `q` jako sumę po wpisach w `P[s][a]`\n",
    "\n",
    "    Wejście:\n",
    "    - P: model środowiska w postaci P[s][a] = [(p, s2, r, terminated), ...]\n",
    "    - v: wektor wartości stanów (np. v_pi), shape (nS,)\n",
    "    - gamma: współczynnik dyskontowania\n",
    "\n",
    "    Wyjście:\n",
    "    - pi_det: deterministyczna polityka, shape (nS,),\n",
    "              pi_det[s] to indeks najlepszej akcji w stanie s.\n",
    "\n",
    "    Co robimy:\n",
    "    - Dla każdego stanu s liczymy Q(s,a) dla wszystkich akcji a,\n",
    "      używając aktualnego v jako przybliżenia \"przyszłości\".\n",
    "    - Wybieramy akcję o największym Q(s,a).\n",
    "    \"\"\"\n",
    "\n",
    "    # Wyznacz nS oraz nA\n",
    "    nS = len(P)\n",
    "    nA = len(P[0])\n",
    "\n",
    "    pi_det = np.zeros(nS, dtype=np.int64)\n",
    "\n",
    "    # Dla każdego stanu policz argmax po akcjach\n",
    "    for s in range(nS):\n",
    "        q_values = np.zeros(nA, dtype=np.float64)\n",
    "\n",
    "        for a in range(nA):\n",
    "            q = 0.0\n",
    "            for prob, s2, r, terminated in P[s][a]:\n",
    "                if terminated:\n",
    "                    q += prob * r\n",
    "                else:\n",
    "                    q += prob * (r + gamma * v[s2])\n",
    "            q_values[a] = q\n",
    "\n",
    "        pi_det[s] = int(np.argmax(q_values))\n",
    "\n",
    "    return pi_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a66c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementacja value iteration\n",
    "def value_iteration(P, gamma: float = 0.9, theta: float = 1e-10, max_iters: int = 100_000):\n",
    "    \"\"\"Value Iteration: bezpośrednio przybliża v* przez backup optymalności (max).\n",
    "\n",
    "    (Ćwiczenie 4)\n",
    "    Zaimplementuj value iteration:\n",
    "        v(s) <- max_a E[ r + gamma * v(s') ]\n",
    "\n",
    "    Wymagania:\n",
    "    - iteruj aż max zmiana w iteracji < theta\n",
    "    - obsłuż `terminated` (bootstrap = 0)\n",
    "    - po zbieżności odczytaj politykę zachłanną względem v* (np. przez greedy_policy_from_v)\n",
    "    - zwróć (pi_det, v)\n",
    "\n",
    "    Wskazówka: to prawie to samo co policy evaluation, tylko zamiast średniej po akcjach masz `max`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Wyznacz nS, nA; inicjalizacja v\n",
    "    nS = len(P)\n",
    "    nA = len(P[0])\n",
    "    v = np.zeros(nS)\n",
    "\n",
    "    # Pętla aktualizacji wartości z max po akcjach\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        v_new = np.copy(v)\n",
    "\n",
    "        for s in range(nS):\n",
    "            q_vals = []\n",
    "\n",
    "            for a in range(nA):\n",
    "                q = 0.0\n",
    "                for prob, s_next, reward, terminated in P[s][a]:\n",
    "                    bootstrap = 0.0 if terminated else v[s_next]\n",
    "                    q += prob * (reward + gamma * bootstrap)\n",
    "\n",
    "                q_vals.append(q)\n",
    "\n",
    "            v_new[s] = max(q_vals)\n",
    "            delta = max(delta, abs(v_new[s] - v[s]))\n",
    "\n",
    "        v = v_new\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "\n",
    "    # Po zbieżności buduje się pi_det\n",
    "    pi_det = greedy_policy_from_v(P, v, gamma)\n",
    "    return pi_det, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liczymy  `π*` i `v*` metodą DP i zapisujemy v*(start)--> upewnic się czy to ta metoda xd\n",
    "\n",
    "gammas = [0.8, 0.9, 0.95, 0.99]\n",
    "start_state = 0\n",
    "\n",
    "vstarts_fl = []\n",
    "for g in gammas:\n",
    "    pi_star, v_star = value_iteration(P_fl_det, gamma=g, theta=1e-10)\n",
    "    vstarts_fl.append(float(v_star[start_state]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea50baa",
   "metadata": {},
   "source": [
    "## Kroki 3 + 4 + 5 - wybór dwóch pytań badawczych, wykonanie eksperymentów, wykonanie wykresów i analiza całości"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a163824",
   "metadata": {},
   "source": [
    "### Pytanie A — wpływ `γ` (gamma)\n",
    "**Hipoteza do sprawdzenia:** większe `γ` zwiększa znaczenie przyszłości i zmienia `v*(start)` oraz czasem politykę.\n",
    "\n",
    "Zrób sweep po `γ`, np. `[0.8, 0.9, 0.95, 0.99]`.\n",
    "Dla każdego `γ` policz `v*(start)`.\n",
    "\n",
    "**Wykres:** `γ` vs `v*(start)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309c2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a5ca25d",
   "metadata": {},
   "source": [
    "### Pytanie B — deterministyczne vs slippery (FrozenLake)\n",
    "**Hipoteza do sprawdzenia:** stochastyczność (`slippery=True`) obniża `v*(start)` i może zmienić `π*`.\n",
    "\n",
    "Porównaj:\n",
    "- `P_fl_det` vs `P_fl_slip` (dla tego samego `γ`).\n",
    "\n",
    "**Wykres:** dwa punkty/kolumny (det vs slippery) dla `v*(start)` (albo dwa przebiegi vs γ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f142c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
