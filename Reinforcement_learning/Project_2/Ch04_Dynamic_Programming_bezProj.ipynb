{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1041c301",
   "metadata": {},
   "source": [
    "Autor: Natalia Kiełbasa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5150b8",
   "metadata": {},
   "source": [
    "# Rozdział 4 — Programowanie dynamiczne (DP): Gridworld + FrozenLake (wersja studencka — projekt)\n",
    "\n",
    "Lektura: Sutton & Barto, rozdz. 4.\n",
    "\n",
    "**Cel laboratorium (ok. 4h pracy własnej):** zaimplementować i przetestować podstawowe algorytmy DP dla MDP\n",
    "przy założeniu **znanego modelu** (format `P[s][a]`).\n",
    "\n",
    "W tym notebooku zrobisz 4 kroki:\n",
    "- **policy evaluation** (obliczanie $v_\\pi$),\n",
    "- **policy improvement** (zachłanna poprawa),\n",
    "- **policy iteration** (pętla evaluation→improvement),\n",
    "- **value iteration** (bezpośrednio $v_*$).\n",
    "\n",
    "**Jak pracować:** w komórkach oznaczonych `TODO` wpisz własną implementację.\n",
    "Nie ma jednej „jedynej” poprawnej wersji (np. aktualizacja może być synchroniczna lub in-place),\n",
    "ale wyniki powinny być spójne.\n",
    "\n",
    "> Wskazówka organizacyjna: najpierw zaimplementuj funkcje z sekcji 1–4, dopiero potem uruchamiaj komórki z eksperymentami.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab77f26",
   "metadata": {},
   "source": [
    "## DP używa pełnych backupów\n",
    "\n",
    "Programowanie dynamiczne zakłada, że znamy model środowiska i możemy sumować po wszystkich możliwych kolejnych (...):\n",
    "$$\n",
    "\\sum_{s',r} p(s',r\\mid s,a)[\\cdot]\n",
    "$$\n",
    "\n",
    "W praktyce: mamy dostęp do struktury `P[s][a]`, czyli listy wyników `(prob, s2, r, terminated)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac048f52",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Ten notebook nie wymaga Gym/Gymnasium do części z Gridworld.\n",
    "\n",
    "Dla FrozenLake:\n",
    "- Jeśli masz `gymnasium`, możesz używać `env.unwrapped.P`.\n",
    "- Żeby notebook był samowystarczalny, poniżej budujemy także model FrozenLake w czystym Pythonie.\n",
    "\n",
    "> **Colab (opcjonalnie):** jeśli potrzebujesz, zainstaluj:\n",
    "> ```\n",
    "> !pip -q install \"gymnasium[toy-text]\"\n",
    "> ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f1f13b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41643601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_matrix_as_grid(v: np.ndarray, nrow: int, ncol: int, decimals: int = 1):\n",
    "    \"\"\"Wyświetla wektor wartości jako siatkę nrow x ncol.\"\"\"\n",
    "    grid = np.asarray(v, dtype=float).reshape(nrow, ncol)\n",
    "    with np.printoptions(precision=decimals, suppress=True):\n",
    "        print(grid)\n",
    "\n",
    "def action_arrows(pi_det: np.ndarray, nrow: int, ncol: int, arrows):\n",
    "    \"\"\"Wyświetla deterministyczną politykę jako strzałki w siatce.\"\"\"\n",
    "    out = []\n",
    "    for r in range(nrow):\n",
    "        row = []\n",
    "        for c in range(ncol):\n",
    "            s = r * ncol + c\n",
    "            a = int(pi_det[s])\n",
    "            row.append(arrows.get(a, '?'))\n",
    "        out.append(' '.join(row))\n",
    "    print('\\n'.join(out))\n",
    "\n",
    "\n",
    "def build_P_r_for_policy(P, pi: np.ndarray):\n",
    "    \"\"\"Buduje (P_pi, r_pi) dla danej polityki pi na podstawie modelu P[s][a].\n",
    "\n",
    "    Zwraca:\n",
    "    - P_pi: macierz przejść (nS x nS)\n",
    "    - r_pi: wektor nagród oczekiwanych (nS,)\n",
    "\n",
    "    Uwaga: dla zadań epizodycznych (FrozenLake) działa poprawnie, o ile stany terminalne są absorbujące\n",
    "    z nagrodą 0 (tak jest w standardowym FrozenLake).\n",
    "    \"\"\"\n",
    "    nS, nA = pi.shape\n",
    "    P_pi = np.zeros((nS, nS), dtype=float)\n",
    "    r_pi = np.zeros(nS, dtype=float)\n",
    "\n",
    "    for s in range(nS):\n",
    "        for a in range(nA):\n",
    "            w = float(pi[s, a])\n",
    "            if w == 0.0:\n",
    "                continue\n",
    "            for (p, s2, r, terminated) in P[s][a]:\n",
    "                P_pi[s, int(s2)] += w * float(p)\n",
    "                r_pi[s] += w * float(p) * float(r)\n",
    "    return P_pi, r_pi\n",
    "\n",
    "\n",
    "def evaluate_policy_linear_system(P_pi: np.ndarray, r_pi: np.ndarray, gamma: float) -> np.ndarray:\n",
    "    \"\"\"Dokładna ewaluacja polityki przez układ liniowy: (I - gamma P_pi) v = r_pi.\"\"\"\n",
    "    nS = P_pi.shape[0]\n",
    "    I = np.eye(nS)\n",
    "    return np.linalg.solve(I - gamma * P_pi, r_pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85a9ea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gridworld_AB_P():\n",
    "    \"\"\"Gridworld 5x5 z A/B teleportami (jak w rozdz. 3).\n",
    "\n",
    "    Akcje (dla Gridworld): 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT.\n",
    "    Przejścia są deterministyczne: P[s][a] ma jeden wpis z p=1.0.\n",
    "\n",
    "    Zwraca: P, nS, nA, nrow, ncol\n",
    "    \"\"\"\n",
    "    nrow, ncol = 5, 5\n",
    "    nS, nA = nrow * ncol, 4\n",
    "    P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "    # Specjalne pozycje A/B\n",
    "    A = (0, 1); Aprime = (4, 1); reward_A = 10.0\n",
    "    B = (0, 3); Bprime = (2, 3); reward_B = 5.0\n",
    "\n",
    "    def s2pos(s):\n",
    "        return (s // ncol, s % ncol)\n",
    "\n",
    "    def pos2s(r, c):\n",
    "        return r * ncol + c\n",
    "\n",
    "    moves = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}\n",
    "\n",
    "    for s in range(nS):\n",
    "        r, c = s2pos(s)\n",
    "\n",
    "        # teleporty mają priorytet\n",
    "        if (r, c) == A:\n",
    "            s2 = pos2s(*Aprime)\n",
    "            for a in range(nA):\n",
    "                P[s][a] = [(1.0, s2, reward_A, False)]\n",
    "            continue\n",
    "        if (r, c) == B:\n",
    "            s2 = pos2s(*Bprime)\n",
    "            for a in range(nA):\n",
    "                P[s][a] = [(1.0, s2, reward_B, False)]\n",
    "            continue\n",
    "\n",
    "        # standardowe ruchy + kara za wyjście poza planszę\n",
    "        for a in range(nA):\n",
    "            dr, dc = moves[a]\n",
    "            r2, c2 = r + dr, c + dc\n",
    "            if (r2 < 0) or (r2 >= nrow) or (c2 < 0) or (c2 >= ncol):\n",
    "                s2 = s\n",
    "                reward = -1.0\n",
    "            else:\n",
    "                s2 = pos2s(r2, c2)\n",
    "                reward = 0.0\n",
    "            P[s][a] = [(1.0, s2, reward, False)]\n",
    "\n",
    "    return P, nS, nA, nrow, ncol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11170b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frozenlake_P(desc, is_slippery: bool = False):\n",
    "    \"\"\"\n",
    "    Buduje model FrozenLake w formacie P[s][a] bez użycia Gym.\n",
    "\n",
    "    desc: lista stringów opisujących mapę, np.\n",
    "          [\"SFFF\",\n",
    "           \"FHFH\",\n",
    "           \"FFFH\",\n",
    "           \"HFFG\"]\n",
    "\n",
    "    Konwencja akcji (jak w Gym):\n",
    "      0 = LEFT, 1 = DOWN, 2 = RIGHT, 3 = UP\n",
    "\n",
    "    Nagroda:\n",
    "      +1 za wejście na G, w pozostałych przypadkach 0.\n",
    "\n",
    "    Stany terminalne:\n",
    "      H (hole) i G (goal).\n",
    "    \"\"\"\n",
    "\n",
    "    # Zamieniamy opis mapy na tablicę znaków\n",
    "    desc = np.asarray([list(row) for row in desc], dtype='<U1')\n",
    "    nrow, ncol = desc.shape\n",
    "    nS, nA = nrow * ncol, 4   # liczba stanów i akcji\n",
    "\n",
    "    # Indeksy akcji (zgodne z Gym)\n",
    "    LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "    # Wektory ruchu dla każdej akcji\n",
    "    moves = {\n",
    "        LEFT:  (0, -1),\n",
    "        DOWN:  (1,  0),\n",
    "        RIGHT: (0,  1),\n",
    "        UP:    (-1, 0),\n",
    "    }\n",
    "\n",
    "    # Zamiana (r, c) -> indeks stanu\n",
    "    def to_s(r, c):\n",
    "        return r * ncol + c\n",
    "\n",
    "    # Jeden krok z pozycji (r, c) dla akcji a\n",
    "    def step_from(r, c, a):\n",
    "        dr, dc = moves[a]\n",
    "        r2, c2 = r + dr, c + dc\n",
    "\n",
    "        # FrozenLake: wyjście poza planszę = zostajemy w miejscu\n",
    "        if (r2 < 0) or (r2 >= nrow) or (c2 < 0) or (c2 >= ncol):\n",
    "            r2, c2 = r, c\n",
    "\n",
    "        return r2, c2\n",
    "\n",
    "    # Inicjalizacja struktury P[s][a]\n",
    "    P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "    # Iterujemy po wszystkich polach planszy\n",
    "    for r in range(nrow):\n",
    "        for c in range(ncol):\n",
    "            s = to_s(r, c)\n",
    "            tile = desc[r, c]\n",
    "\n",
    "            # Stany terminalne (H i G):\n",
    "            # są absorbujące — niezależnie od akcji zostajemy w tym samym stanie\n",
    "            if tile in ('H', 'G'):\n",
    "                for a in range(nA):\n",
    "                    P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                continue\n",
    "\n",
    "            # Stany nieterminalne\n",
    "            for a in range(nA):\n",
    "                outcomes = []\n",
    "\n",
    "                # Wersja śliska: akcja nie realizuje się dokładnie\n",
    "                if is_slippery:\n",
    "                    # Faktyczna akcja to a-1, a, a+1 z prawd. 1/3\n",
    "                    candidates = [(a - 1) % 4, a, (a + 1) % 4]\n",
    "                    probs = [1/3, 1/3, 1/3]\n",
    "                else:\n",
    "                    # Wersja deterministyczna\n",
    "                    candidates = [a]\n",
    "                    probs = [1.0]\n",
    "\n",
    "                # Dla każdej możliwej „rzeczywistej” akcji\n",
    "                for a_real, p in zip(candidates, probs):\n",
    "                    r2, c2 = step_from(r, c, a_real)\n",
    "                    s2 = to_s(r2, c2)\n",
    "                    tile2 = desc[r2, c2]\n",
    "\n",
    "                    # Sprawdzamy, czy trafiliśmy w stan terminalny\n",
    "                    terminated = tile2 in ('H', 'G')\n",
    "\n",
    "                    # Nagroda tylko za wejście na G\n",
    "                    reward = 1.0 if tile2 == 'G' else 0.0\n",
    "\n",
    "                    outcomes.append((float(p), int(s2), float(reward), bool(terminated)))\n",
    "\n",
    "                # Scalanie identycznych następstw\n",
    "                # (różne a_real mogą prowadzić do tego samego s2)\n",
    "                merged = {}\n",
    "                for p, s2, rwd, term in outcomes:\n",
    "                    key = (s2, rwd, term)\n",
    "                    merged[key] = merged.get(key, 0.0) + p\n",
    "\n",
    "                # Finalna lista wyników dla (s, a)\n",
    "                P[s][a] = [\n",
    "                    (p, s2, rwd, term)\n",
    "                    for (s2, rwd, term), p in merged.items()\n",
    "                ]\n",
    "\n",
    "    return P, nS, nA, nrow, ncol, desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abb92795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gridworld: nS= 25  nA= 4  shape= (5, 5)\n",
      "FrozenLake: nS= 16  nA= 4  shape= (4, 4)\n",
      "FrozenLake map:\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "FrozenLake (slippery): nS= 16  nA= 4  shape= (4, 4)\n",
      "FrozenLake map:\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Środowisko 1: Gridworld z A/B (continuing)\n",
    "P_gw, nS_gw, nA_gw, nrow_gw, ncol_gw = build_gridworld_AB_P()\n",
    "\n",
    "# Środowisko 2: FrozenLake 4x4 (episodic)\n",
    "desc4 = [\n",
    "    \"SFFF\",\n",
    "    \"FHFH\",\n",
    "    \"FFFH\",\n",
    "    \"HFFG\",\n",
    "]\n",
    "P_fl_det, nS_fl, nA_fl, nrow_fl, ncol_fl, desc_fl = build_frozenlake_P(desc4, is_slippery=False)\n",
    "P_fl_slip, _, _, _, _, _ = build_frozenlake_P(desc4, is_slippery=True)\n",
    "\n",
    "print('Gridworld: nS=', nS_gw, ' nA=', nA_gw, ' shape=', (nrow_gw, ncol_gw))\n",
    "print('FrozenLake: nS=', nS_fl, ' nA=', nA_fl, ' shape=', (nrow_fl, ncol_fl))\n",
    "print('FrozenLake map:')\n",
    "print('\\n'.join(''.join(row) for row in desc_fl))\n",
    "print('FrozenLake (slippery): nS=', nS_fl, ' nA=', nA_fl, ' shape=', (nrow_fl, ncol_fl))\n",
    "print('FrozenLake map:')\n",
    "print('\\n'.join(''.join(row) for row in desc_fl))\n",
    "\n",
    "# Mapy akcji -> strzałki (do wizualizacji polityk)\n",
    "arrows_gw = {0:'↑', 1:'→', 2:'↓', 3:'←'}  # Gridworld: UP, RIGHT, DOWN, LEFT\n",
    "arrows_fl = {0:'←', 1:'↓', 2:'→', 3:'↑'}  # FrozenLake: LEFT, DOWN, RIGHT, UP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560b2910",
   "metadata": {},
   "source": [
    "### Co pokazuje ten fragment?\n",
    "\n",
    "Ten kod **porównuje dwa różne środowiska MDP**, które będziemy dalej wykorzystywać w kursie:\n",
    "\n",
    "- **Gridworld z A/B teleportami** — zadanie ciągłe (continuing),\n",
    "- **FrozenLake 4×4** — zadanie epizodyczne (episodic).\n",
    "\n",
    "---\n",
    "\n",
    "### Najważniejsze obserwacje\n",
    "\n",
    "- Oba środowiska są opisane **tym samym modelem MDP**:\n",
    "  - mają `P[s][a]`,\n",
    "  - mają `nS` (liczbę stanów),\n",
    "  - mają `nA` (liczbę akcji).\n",
    "  Dzięki temu **te same algorytmy DP/RL działają bez zmian**.\n",
    "\n",
    "- **Gridworld**:\n",
    "  - nie ma stanów terminalnych,\n",
    "  - agent działa bez końca,\n",
    "  - wartości stabilizuje dyskontowanie (`gamma < 1`).\n",
    "\n",
    "- **FrozenLake**:\n",
    "  - ma stany terminalne (`H`, `G`),\n",
    "  - epizod się kończy po wejściu w dziurę lub do celu,\n",
    "  - to klasyczny przykład zadania epizodycznego.\n",
    "\n",
    "- Dla FrozenLake pokazujemy też dwie wersje:\n",
    "  - deterministyczną (`is_slippery=False`),\n",
    "  - losową (`is_slippery=True`),\n",
    "  co później pozwala zobaczyć wpływ losowości na wartości i polityki.\n",
    "\n",
    "---\n",
    "\n",
    "### Podsumowanie\n",
    "\n",
    "> Ten fragment pokazuje, że różne środowiska mogą mieć bardzo inną dynamikę, ale jeśli są MDP, możemy je analizować tym samym aparatem matematycznym.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc28729",
   "metadata": {},
   "source": [
    "### Uwaga: dlaczego nie analizujemy wersji `slippery`?\n",
    "\n",
    "W tym rozdziale **budujemy** zarówno deterministyczną, jak i losową (`slippery`) wersję FrozenLake,\n",
    "ale **wyniki prezentujemy tylko dla wersji deterministycznej**.\n",
    "\n",
    "Powód jest dydaktyczny:\n",
    "- celem rozdziału 4 jest zrozumienie **mechaniki Dynamic Programming**,\n",
    "- losowość środowiska nie wnosi tu nowej idei, a tylko utrudnia interpretację wyników.\n",
    "\n",
    "Wersja `slippery` wróci później:\n",
    "- gdy będziemy porównywać DP z RL,\n",
    "- oraz gdy losowość stanie się kluczowym elementem problemu.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80333029",
   "metadata": {},
   "source": [
    "### Ramka: deterministyczne vs `slippery` w równaniach Bellmana\n",
    "\n",
    "Wszystkie wzory z zajęć **pozostają bez zmian** niezależnie od tego,\n",
    "czy środowisko jest deterministyczne, czy losowe (`slippery`).\n",
    "\n",
    "#### Bez `slippery` (deterministycznie)\n",
    "- Każda akcja ma **jeden skutek**.\n",
    "- W `P[s][a]` jest pojedynczy wpis z prawdopodobieństwem 1.\n",
    "- Równanie Bellmana redukuje się do:\n",
    "  $$\n",
    "  V(s) = r + \\gamma V(s').\n",
    "  $$\n",
    "\n",
    "#### Z `slippery` (losowo)\n",
    "- Ta sama akcja ma **kilka możliwych skutków**.\n",
    "- W `P[s][a]` jest kilka wpisów z różnymi prawdopodobieństwami.\n",
    "- Równanie Bellmana liczy **wartość oczekiwaną**:\n",
    "  $$\n",
    "  V(s) = \\sum_i p_i \\big(r_i + \\gamma V(s'_i)\\big).\n",
    "  $$\n",
    "\n",
    "#### Co to zmienia w praktyce?\n",
    "- **Nie zmienia teorii ani algorytmów** (policy evaluation, value iteration).\n",
    "- Zmienia **liczby** w obliczeniach i **intuicję decyzji**:\n",
    "  - polityka optymalna zaczyna unikać ryzykownych ruchów,\n",
    "  - „bezpieczne” ścieżki stają się bardziej atrakcyjne.\n",
    "\n",
    "---\n",
    "\n",
    "**Podsumowanie:**  \n",
    "`slippery` nie zmienia równań Bellmana — sprawia tylko, że każda akcja jest oceniana przez wartość oczekiwaną zamiast przez jeden deterministyczny skutek.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e93a5c",
   "metadata": {},
   "source": [
    "## 1. Iteracyjna ewaluacja polityki (policy evaluation)\n",
    "\n",
    "Chcemy policzyć wartość $v_\\pi$ dla ustalonej polityki $\\pi$.\n",
    "\n",
    "Backup Bellmana (expectation):\n",
    "$$\n",
    "v_{k+1}(s) \\leftarrow \\sum_a \\pi(a\\mid s) \\sum_{s',r} p(s',r\\mid s,a)\\,[r + \\gamma v_k(s')].\n",
    "$$\n",
    "\n",
    "W DP robimy to iteracyjnie (kolejne przybliżenia), zamiast od razu rozwiązywać układ liniowy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d5f877",
   "metadata": {},
   "source": [
    "### Ćwiczenie 1 — Iteracyjna ewaluacja polityki\n",
    "\n",
    "**Cel:** policzyć funkcję wartości `v_π` dla *zadanej* polityki `π`, używając\n",
    "iteracyjnego backupu Bellmana (Dynamic Programming), zamiast rozwiązywania układu równań.\n",
    "\n",
    "**Co robimy:**\n",
    "- polityka `π` jest ustalona (brak `max`),\n",
    "- wielokrotnie aktualizujemy wartości stanów zgodnie z definicją Bellmana,\n",
    "- przerywamy, gdy zmiany są bardzo małe (`theta`).\n",
    "\n",
    "**Dlaczego to ważne:**\n",
    "- to dokładnie ten sam obiekt `v_π`, co w rozdz. 3,\n",
    "- metoda iteracyjna skaluje się lepiej i jest fundamentem DP,\n",
    "- przygotowuje grunt pod *policy improvement* i *policy iteration*.\n",
    "\n",
    "**Sanity check:** porównujemy wynik iteracyjny z rozwiązaniem macierzowym — powinny się zgadzać.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebbda97",
   "metadata": {},
   "source": [
    "![Policy evaluation – schemat](policy_evaluation_pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f55d72",
   "metadata": {},
   "source": [
    "Uwaga: w implementacji równania Bellmana występują dwie zagnieżdżone sumy:\n",
    "najpierw po akcjach (ważone przez π), a następnie po możliwych wynikach środowiska (ważone przez p)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe44fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def iterative_policy_evaluation(P, \n",
    "                                pi: np.ndarray,\n",
    "                                gamma: float = 0.9,\n",
    "                                theta: float = 1e-10,\n",
    "                                max_iters: int = 100_000):\n",
    "    \"\"\"Iteracyjna ewaluacja polityki (policy evaluation) dla modelu `P[s][a]`.\n",
    "\n",
    "    (Ćwiczenie 1)\n",
    "    Napisz funkcję, która liczy przybliżenie wartości stanów `v_pi` dla *ustalonej* polityki `pi`.\n",
    "\n",
    "    Definicja (backup Bellmana — expectation):\n",
    "        v(s) <- sum_a pi(a|s) * sum_{s',r} p(s',r|s,a) * [ r + gamma * v(s') ]\n",
    "    Parametry:\n",
    "    - P: model środowiska w postaci P[s][a] = [(p, s2, r, terminated), ...]\n",
    "    - pi: polityka w postaci macierzy (nS x nA), gdzie pi[s,a] = π(a|s)\n",
    "    - gamma: współczynnik dyskontowania\n",
    "    - theta: próg zbieżności (maks. zmiana wartości w iteracji)\n",
    "    - max_iters: maksymalna liczba iteracji (zabezpieczenie)\n",
    "\n",
    "    Zwraca:\n",
    "    - v: wektor wartości stanów v_pi (shape: nS)\n",
    "    Wejście:\n",
    "    - P: model w formacie słownika: P[s][a] = lista krotek (prob, s2, r, terminated)\n",
    "    - pi: macierz (nS, nA) z rozkładami pi(a|s)\n",
    "\n",
    "    Wymagania:\n",
    "    - zatrzymanie po zbieżności: max zmiana w iteracji < theta (zmienna `delta`)\n",
    "    - obsłuż stany terminalne:\n",
    "        jeśli `terminated=True`, to NIE bootstrappuj z v[s2] (zamiast tego użyj 0)\n",
    "    - zwróć wektor v (shape: (nS,))\n",
    "    - Stany terminalne: jeśli przejście kończy epizod, to nie dodajemy składnika γ·v(s').\n",
    "        Wartość po terminalu jest z definicji równa 0.\n",
    "    Wskazówki (bez narzucania implementacji):\n",
    "    - Zacznij od `v = zeros(nS)`.\n",
    "    - W każdej iteracji wykonaj \"sweep\" po wszystkich stanach.\n",
    "    - Możesz aktualizować `v[s]` in-place (często szybciej), albo \n",
    "    synchronicznie (przez kopię).\n",
    "    Zmienna delta mierzy, jak bardzo naruszone jest równanie \n",
    "    Bellmana — gdy delta → 0, wartości są bliskie rozwiązania dokładnego.\n",
    "    Po zaimplementowaniu usuń `raise NotImplementedError`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pobierz nS, nA z `pi.shape`\n",
    "    nS, nA = pi.shape\n",
    "\n",
    "    # Zainicjalizuj `v`\n",
    "    v = np.zeros(nS, dtype=np.float64)\n",
    "\n",
    "    # Pętla po iteracjach, aktualizacja delta, warunek stopu\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "\n",
    "        # Dla danego stanu s\n",
    "        for s in range(nS):\n",
    "            v_old = v[s]\n",
    "            v_new = 0.0\n",
    "\n",
    "            # Policz sumę po akcjach a zgodnie z pi[s,a]\n",
    "            for a in range(nA):\n",
    "                pi_sa = pi[s, a]\n",
    "                if pi_sa == 0.0:\n",
    "                    continue\n",
    "                \n",
    "                # Policz sumę po akcjach a zgodnie z pi[s,a]\n",
    "                for prob, s2, r, terminated in P[s][a]:\n",
    "                    if terminated:\n",
    "                        v_new += pi_sa * prob * r\n",
    "                    else:\n",
    "                        v_new += pi_sa * prob * (r + gamma * v[s2])\n",
    "\n",
    "            v[s] = v_new\n",
    "            delta = max(delta, abs(v_old - v_new))\n",
    "\n",
    "        # Jeśli delta < theta, przerwij iteracje\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Zwróć wektor v\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070cc91d",
   "metadata": {},
   "source": [
    "### Policy evaluation jako problem algebry liniowej (referencyjnie) - wprowadzone na poprzedniej lekcji.\n",
    "\n",
    "Dla ustalonej polityki π równanie Bellmana można zapisać w postaci macierzowej:\n",
    "\n",
    "v = r_π + γ P_π v\n",
    "\n",
    "gdzie:\n",
    "- v ∈ R^{nS} — wektor wartości stanów,\n",
    "- r_π — wektor oczekiwanych nagród,\n",
    "- P_π — macierz przejść pod polityką π.\n",
    "\n",
    "Po przekształceniu:\n",
    "\n",
    "(I − γ P_π) v = r_π\n",
    "\n",
    "Jest to zwykły układ równań liniowych, który można rozwiązać\n",
    "za pomocą metod algebry liniowej (np. eliminacji Gaussa).\n",
    "\n",
    "To rozwiązanie jest:\n",
    "- **dokładne** (w granicach numeryki),\n",
    "- ale wymaga przechowywania macierzy nS × nS\n",
    "  i ma koszt obliczeniowy rzędu O(nS³).\n",
    "\n",
    "Dlatego w praktyce RL stosujemy **metody iteracyjne**,\n",
    "które są znacznie tańsze pamięciowo i skalują się lepiej.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3cea0",
   "metadata": {},
   "source": [
    "### Mini-przykład (2 stany)\n",
    "\n",
    "Załóżmy:\n",
    "- dwa stany: s₀, s₁,\n",
    "- jedna akcja (polityka trywialna),\n",
    "- przejścia:\n",
    "  - s₀ → s₁ z nagrodą 1,\n",
    "  - s₁ → s₁ z nagrodą 0,\n",
    "- γ = 0.9.\n",
    "\n",
    "Równania Bellmana:\n",
    "\n",
    "v(s₀) = 1 + 0.9 v(s₁)\n",
    "v(s₁) = 0 + 0.9 v(s₁)\n",
    "\n",
    "Z drugiego równania:\n",
    "0.1 v(s₁) = 0 ⇒ v(s₁) = 0\n",
    "\n",
    "Podstawiając:\n",
    "v(s₀) = 1\n",
    "\n",
    "To dokładnie ten sam wynik, do którego dochodzi\n",
    "iteracyjna policy evaluation — tylko „na raty”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5084f1",
   "metadata": {},
   "source": [
    "### Od równania Bellmana do układu równań liniowych\n",
    "\n",
    "Dla ustalonej polityki π równanie Bellmana ma postać:\n",
    "\n",
    "v_π = r_π + γ P_π v_π\n",
    "\n",
    "Jest to równanie, w którym niewiadomą jest cały wektor v_π.\n",
    "Przenosząc wyrazy na jedną stronę, otrzymujemy:\n",
    "\n",
    "(I − γ P_π) v_π = r_π\n",
    "\n",
    "To jest zwykły układ równań liniowych, znany z algebry liniowej.\n",
    "\n",
    "- macierz (I − γ P_π) ma rozmiar nS × nS,\n",
    "- wektor r_π ma rozmiar nS,\n",
    "- rozwiązaniem jest wektor wartości stanów v_π.\n",
    "\n",
    "Metoda iteracyjna policy evaluation rozwiązuje dokładnie to samo równanie,\n",
    "ale metodą kolejnych przybliżeń, bez jawnego budowania macierzy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e84ad389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_linear_system_from_model(P, pi, gamma):\n",
    "    \"\"\"\n",
    "    Dokładna ewaluacja polityki przez rozwiązanie układu równań liniowych.\n",
    "\n",
    "    TODO (opcjonalne):\n",
    "    - zbuduj macierz P_pi i wektor r_pi dla danej polityki\n",
    "    - rozwiąż układ (I - gamma * P_pi) v = r_pi\n",
    "\n",
    "    Uwaga:\n",
    "    - metoda referencyjna (sanity-check),\n",
    "    - kosztowna pamięciowo i obliczeniowo,\n",
    "    - sensowna tylko dla małych środowisk.\n",
    "    \"\"\"\n",
    "    P_pi, r_pi = build_P_r_for_policy(P, pi)\n",
    "    I = np.eye(P_pi.shape[0])\n",
    "    v = np.linalg.solve(I - gamma * P_pi, r_pi)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3e2f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "def benchmark_policy_evaluation(P, pi, gamma):\n",
    "    # Iteracyjna\n",
    "    t0 = time.time()\n",
    "    v_iter = iterative_policy_evaluation(P, pi, gamma)\n",
    "    t_iter = time.time() - t0\n",
    "\n",
    "    # Liniowa\n",
    "    t0 = time.time()\n",
    "    v_lin = evaluate_policy_linear_system_from_model(P, pi, gamma)\n",
    "    t_lin = time.time() - t0\n",
    "\n",
    "    # Różnica\n",
    "    err = np.max(np.abs(v_iter - v_lin))\n",
    "\n",
    "    print(\"Iterative PE:\")\n",
    "    print(f\"  time = {t_iter:.4f} s\")\n",
    "    print(f\"  memory ~ O(nS)\")\n",
    "\n",
    "    print(\"Linear system PE:\")\n",
    "    print(f\"  time = {t_lin:.4f} s\")\n",
    "    print(f\"  memory ~ O(nS^2)\")\n",
    "\n",
    "    print(f\"Max |v_iter - v_linear| = {err:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390510f2",
   "metadata": {},
   "source": [
    "### Dlaczego w RL używamy metod iteracyjnych?\n",
    "\n",
    "Rozwiązanie układu liniowego jest eleganckie i dokładne,\n",
    "ale jego koszt rośnie bardzo szybko wraz z liczbą stanów.\n",
    "\n",
    "Metody iteracyjne:\n",
    "- nie wymagają budowy pełnej macierzy P_π,\n",
    "- używają tylko lokalnych informacji o przejściach,\n",
    "- są naturalnym punktem wyjścia do metod bezmodelowych (TD, Q-learning).\n",
    "\n",
    "Dlatego policy evaluation w RL\n",
    "traktujemy jako **problem stałego punktu**, a nie jako klasyczne\n",
    "zadanie z algebry liniowej.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aca5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład: Gridworld 5x5\n",
    "P, nS, nA, nrow, ncol = build_gridworld_AB_P()\n",
    "\n",
    "# Polityka losowa (jednolita)\n",
    "pi = np.ones((nS, nA)) / nA\n",
    "\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f24860d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max |v_iter - v_linear| = 4.4990855485593784e-10\n"
     ]
    }
   ],
   "source": [
    "v_iter = iterative_policy_evaluation(P, pi, gamma=gamma)\n",
    "v_lin = evaluate_policy_linear_system_from_model(P, pi, gamma=gamma)\n",
    "max_diff = np.max(np.abs(v_iter - v_lin))\n",
    "print(\"Max |v_iter - v_linear| =\", max_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc962b0",
   "metadata": {},
   "source": [
    "### Sanity check (zalecane)\n",
    "\n",
    "Dla małych środowisk porównaj wynik iteracyjny z rozwiązaniem\n",
    "układu równań liniowych `(I − γP_π)v = r_π`.\n",
    "Obie metody powinny dać bardzo zbliżone wyniki (różnice rzędu `1e−6`).\n",
    "To porównanie pokazuje, że metoda iteracyjna faktycznie rozwiązuje to samo równanie co metoda liniowa — tylko innym (tańszym) sposobem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff38bc2",
   "metadata": {},
   "source": [
    "### Ćwiczenie: Policy evaluation w FrozenLake (zadanie epizodyczne)\n",
    "\n",
    "W tym ćwiczeniu stosujemy iteracyjną ewaluację polityki\n",
    "do środowiska **FrozenLake**, które jest zadaniem *epizodycznym*.\n",
    "\n",
    "- epizod kończy się, gdy agent wpadnie do dziury (`H`) lub dotrze do celu (`G`),\n",
    "- jedyna dodatnia nagroda (+1) jest przyznawana za wejście na pole `G`,\n",
    "- pozostałe przejścia mają nagrodę 0.\n",
    "\n",
    "Analizujemy **politykę losową**:\n",
    "agent w każdym stanie wybiera jedną z 4 akcji z jednakowym prawdopodobieństwem.\n",
    "\n",
    "Wartość stanu `v_π(s)` można interpretować jako:\n",
    "> *oczekiwaną (zdyskontowaną) nagrodę,\n",
    "> czyli w przybliżeniu szansę dotarcia do celu,\n",
    "> jeśli startujemy w stanie `s` i poruszamy się losowo.*\n",
    "\n",
    "**Czego się spodziewać:**\n",
    "- stany bliżej celu powinny mieć większe wartości,\n",
    "- stany prowadzące do dziur – mniejsze,\n",
    "- wartości w FrozenLake są zwykle znacznie mniejsze niż w Gridworld,\n",
    "  ponieważ nagroda pojawia się tylko na końcu epizodu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddf32d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gridworld: v_pi (polityka równoprawdopodobna), tabela 5x5:\n",
      "[[ 3.3  8.8  4.4  5.3  1.5]\n",
      " [ 1.5  3.   2.3  1.9  0.5]\n",
      " [ 0.1  0.7  0.7  0.4 -0.4]\n",
      " [-1.  -0.4 -0.4 -0.6 -1.2]\n",
      " [-1.9 -1.3 -1.2 -1.4 -2. ]]\n",
      "\n",
      "Max |v_iter - v_lin| = 4.4990855485593784e-10\n"
     ]
    }
   ],
   "source": [
    "# --- Gridworld: ewaluacja polityki równoprawdopodobnej (jak w książce) ---\n",
    "\n",
    "gamma_gw = 0.9\n",
    "# Współczynnik dyskontowania; Gridworld jest zadaniem ciągłym\n",
    "\n",
    "# Polityka równoprawdopodobna:\n",
    "# w każdym stanie każda z 4 akcji ma prawdopodobieństwo 1/4\n",
    "pi_gw_rand = np.ones((nS_gw, nA_gw), dtype=float) / nA_gw\n",
    "\n",
    "# Iteracyjnie liczymy v_pi dla tej polityki (policy evaluation)\n",
    "v_gw = iterative_policy_evaluation(P_gw, pi_gw_rand, gamma=gamma_gw)\n",
    "\n",
    "print('Gridworld: v_pi (polityka równoprawdopodobna), tabela 5x5:')\n",
    "# Wyświetlamy wartości stanów w formie siatki (jak w książce)\n",
    "pretty_matrix_as_grid(v_gw, nrow_gw, ncol_gw, decimals=1)\n",
    "\n",
    "\n",
    "# --- (opcjonalnie) sanity check: porównanie z rozwiązaniem macierzowym ---\n",
    "\n",
    "# Budujemy macierze P_pi i r_pi dla tej samej polityki\n",
    "P_pi, r_pi = build_P_r_for_policy(P_gw, pi_gw_rand)\n",
    "\n",
    "# Liczymy v_pi dokładnie, rozwiązując układ równań Bellmana\n",
    "v_gw_lin = evaluate_policy_linear_system(P_pi, r_pi, gamma_gw)\n",
    "\n",
    "# Sprawdzamy, że metoda iteracyjna i macierzowa dają ten sam wynik\n",
    "print('\\nMax |v_iter - v_lin| =',\n",
    "      float(np.max(np.abs(v_gw - v_gw_lin))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac20e1",
   "metadata": {},
   "source": [
    "## Środowisko FrozenLake (model P[s][a])\n",
    "\n",
    "W dalszej części notebooka korzystamy z modelu środowiska FrozenLake,\n",
    "zbudowanego wcześniej za pomocą funkcji `build_frozenlake_P`.\n",
    "\n",
    "- `P_fl_det` — wersja deterministyczna (bez poślizgu),\n",
    "- `P_fl_slip` — wersja śliska (akcje są losowo zaburzane),\n",
    "- `desc_fl` — mapa planszy (S, F, H, G).\n",
    "\n",
    "Przypomnienie mapy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72387e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenLake (deterministyczne): v_pi (polityka losowa), tabela 4x4:\n",
      "[[0.01 0.01 0.02 0.01]\n",
      " [0.01 0.   0.04 0.  ]\n",
      " [0.03 0.08 0.14 0.  ]\n",
      " [0.   0.17 0.43 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# --- FrozenLake (deterministyczne): ewaluacja polityki losowej ---\n",
    "\n",
    "gamma_fl = 0.99\n",
    "# FrozenLake jest zadaniem epizodycznym,\n",
    "# dlatego często przyjmuje się gamma bliskie 1\n",
    "\n",
    "# Polityka losowa:\n",
    "# w każdym stanie losujemy jedną z 4 akcji z jednakowym prawdopodobieństwem\n",
    "pi_fl_rand = np.ones((nS_fl, nA_fl), dtype=float) / nA_fl\n",
    "\n",
    "# Iteracyjnie liczymy v_pi dla tej polityki w FrozenLake\n",
    "v_fl = iterative_policy_evaluation(P_fl_det, pi_fl_rand, gamma=gamma_fl)\n",
    "\n",
    "print('FrozenLake (deterministyczne): v_pi (polityka losowa), tabela 4x4:')\n",
    "# Wyświetlamy wartości stanów jako siatkę 4x4\n",
    "pretty_matrix_as_grid(v_fl, nrow_fl, ncol_fl, decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047c8665",
   "metadata": {},
   "source": [
    "### Dlaczego wartości `v_π` są tu małe?\n",
    "\n",
    "- Oceniamy **politykę losową**, która rzadko prowadzi do celu.\n",
    "- FrozenLake jest **zadaniem epizodycznym**:\n",
    "  - jedyna dodatnia nagroda to `+1` za wejście na `G`,\n",
    "  - wiele epizodów kończy się bez żadnej nagrody.\n",
    "- Wartość `v_π(s)` jest **średnią zdyskontowaną** z wielu epizodów,\n",
    "  więc mała szansa sukcesu daje małe wartości.\n",
    "\n",
    "Wniosek:\n",
    "> Małe wartości nie oznaczają błędu — oznaczają, że dana polityka jest słaba w tym środowisku.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3422599",
   "metadata": {},
   "source": [
    "### Wskazówki (sekcja 1)\n",
    "\n",
    "- Ten fragment domyka łącznik z rozdz. 3: *tam* liczyliśmy $v_\\pi$ często „z definicji” (układ liniowy),\n",
    "  a *tu* robimy to iteracyjnie (backup Bellmana).\n",
    "- W Gridworld warto pokazać, że wynik z iteracji ≈ wynik z układu liniowego (sanity check `Max |v_iter - v_lin|`).\n",
    "- Iteracyjna ewaluacja polityki liczy v_π przez wielokrotne stosowanie backupu Bellmana — to ten sam obiekt co wcześniej, tylko obliczany metodą DP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c0a91",
   "metadata": {},
   "source": [
    "## 2. Ulepszanie polityki (policy improvement)\n",
    "\n",
    "Mając oszacowanie funkcji wartości $v_\\pi$, możemy **poprawić politykę**\n",
    "zmieniając decyzje w stanach na takie, które prowadzą do lepszych\n",
    "oczekiwanych rezultatów w przyszłości.\n",
    "\n",
    "Formalnie, dla każdego stanu wybieramy akcję maksymalizującą\n",
    "jednokrokową wartość oczekiwaną:\n",
    "$$\n",
    "\\pi'(s) = \\arg\\max_a \\sum_{s',r} p(s',r\\mid s,a)\\,[r + \\gamma v_\\pi(s')].\n",
    "$$\n",
    "\n",
    "Jest to **krok zachłanny (greedy)** względem aktualnej funkcji wartości.\n",
    "\n",
    "Remisy: jeśli kilka akcji ma takie samo Q(s,a), argmax wybierze jedną z nich (np. pierwszą). Wszystkie akcje remisujące są równie dobre w tym kroku.\n",
    "\n",
    "---\n",
    "\n",
    "### Ćwiczenie 2 — Zachłanna poprawa polityki\n",
    "\n",
    "Mając oszacowanie funkcji wartości `v_π`, konstruujemy nową politykę `π'`\n",
    "w następujący sposób:\n",
    "\n",
    "- w każdym stanie rozważamy wszystkie możliwe akcje,\n",
    "- dla każdej akcji obliczamy jej wartość oczekiwaną,\n",
    "- wybieramy akcję o największej wartości.\n",
    "\n",
    "W tym kroku:\n",
    "- **nie liczymy jeszcze funkcji wartości dla nowej polityki**,\n",
    "- **nie iterujemy**,\n",
    "- tylko odpowiadamy na pytanie:\n",
    "  \n",
    "> *„jaką akcję warto wybrać, jeśli wierzymy w aktualne `v_π`?”*\n",
    "\n",
    "To ćwiczenie uzupełnia policy evaluation:\n",
    "- **ewaluacja** odpowiadała na pytanie *„ile warte są stany?”*,\n",
    "- **improvement** odpowiada na pytanie *„jaką akcję wybrać?”*.\n",
    "\n",
    "Pojedynczy krok improvement **nie musi dać polityki optymalnej**,\n",
    "ale gwarantuje, że nowa polityka nie jest gorsza od poprzedniej.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1096a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_from_v(P, v: np.ndarray, gamma: float = 0.9) -> np.ndarray:\n",
    "    \"\"\"Zachłanna poprawa polityki (policy improvement).\n",
    "\n",
    "    (Ćwiczenie 2)\n",
    "    Zaimplementuj funkcję, która dla danego `v` buduje politykę deterministyczną `pi_det`:\n",
    "        pi_det[s] = argmax_a Q(s,a)\n",
    "\n",
    "    gdzie:\n",
    "        Q(s,a) = E[ r + gamma * v(s') ]\n",
    "\n",
    "    Wymagania:\n",
    "    - zwróć `pi_det` jako wektor intów o kształcie (nS,)\n",
    "    - użyj pełnego modelu: przejścia są w `P[s][a]` jako (p, s2, r, terminated)\n",
    "    - obsłuż `terminated` identycznie jak w policy evaluation:\n",
    "        jeśli `terminated=True`, bootstrap = 0\n",
    "\n",
    "    Wskazówki:\n",
    "    - pętla po stanach `s`, wewnątrz pętla po akcjach `a`\n",
    "    - dla każdej akcji policz `q` jako sumę po wpisach w `P[s][a]`\n",
    "\n",
    "    Wejście:\n",
    "    - P: model środowiska w postaci P[s][a] = [(p, s2, r, terminated), ...]\n",
    "    - v: wektor wartości stanów (np. v_pi), shape (nS,)\n",
    "    - gamma: współczynnik dyskontowania\n",
    "\n",
    "    Wyjście:\n",
    "    - pi_det: deterministyczna polityka, shape (nS,),\n",
    "              pi_det[s] to indeks najlepszej akcji w stanie s.\n",
    "\n",
    "    Co robimy:\n",
    "    - Dla każdego stanu s liczymy Q(s,a) dla wszystkich akcji a,\n",
    "      używając aktualnego v jako przybliżenia \"przyszłości\".\n",
    "    - Wybieramy akcję o największym Q(s,a).\n",
    "    \"\"\"\n",
    "\n",
    "    # Wyznacz nS oraz nA\n",
    "    nS = len(P)\n",
    "    nA = len(P[0])\n",
    "\n",
    "    pi_det = np.zeros(nS, dtype=np.int64)\n",
    "\n",
    "    # Dla każdego stanu policz argmax po akcjach\n",
    "    for s in range(nS):\n",
    "        q_values = np.zeros(nA, dtype=np.float64)\n",
    "\n",
    "        for a in range(nA):\n",
    "            q = 0.0\n",
    "            for prob, s2, r, terminated in P[s][a]:\n",
    "                if terminated:\n",
    "                    q += prob * r\n",
    "                else:\n",
    "                    q += prob * (r + gamma * v[s2])\n",
    "            q_values[a] = q\n",
    "\n",
    "        pi_det[s] = int(np.argmax(q_values))\n",
    "\n",
    "    return pi_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a31d26af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gridworld: polityka zachłanna względem v_pi(random):\n",
      "v_pi(random) jako siatka:\n",
      "[[ 3.31  8.79  4.43  5.32  1.49]\n",
      " [ 1.52  2.99  2.25  1.91  0.55]\n",
      " [ 0.05  0.74  0.67  0.36 -0.4 ]\n",
      " [-0.97 -0.44 -0.35 -0.59 -1.18]\n",
      " [-1.86 -1.35 -1.23 -1.42 -1.98]]\n",
      "→ ↑ ← ↑ ←\n",
      "↑ ↑ ↑ ↑ ←\n",
      "↑ ↑ ↑ ↑ ↑\n",
      "↑ ↑ ↑ ↑ ↑\n",
      "↑ ↑ ↑ ↑ ↑\n"
     ]
    }
   ],
   "source": [
    "# --- Gridworld: 1 krok improvement z v_pi(random) ---\n",
    "\n",
    "# Budujemy politykę zachłanną względem v_pi (polityka losowa)\n",
    "pi_gw_greedy = greedy_policy_from_v(P_gw, v_gw, gamma=gamma_gw)\n",
    "\n",
    "print('Gridworld: polityka zachłanna względem v_pi(random):')\n",
    "\n",
    "# Konwencja akcji Gridworld: UP, RIGHT, DOWN, LEFT\n",
    "arrows_gw = {0:'↑', 1:'→', 2:'↓', 3:'←'}\n",
    "\n",
    "print(\"v_pi(random) jako siatka:\")\n",
    "pretty_matrix_as_grid(v_gw, nrow_gw, ncol_gw, decimals=2)\n",
    "\n",
    "\n",
    "# Wizualizacja: pokazuje, jak zmienia się decyzja po jednym kroku improvement\n",
    "action_arrows(pi_gw_greedy, nrow_gw, ncol_gw, arrows_gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc8ebf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenLake (det.): polityka zachłanna względem v_pi(random):\n",
      "v_pi(random) jako siatka:\n",
      "[[0.012 0.01  0.019 0.009]\n",
      " [0.015 0.    0.039 0.   ]\n",
      " [0.033 0.084 0.138 0.   ]\n",
      " [0.    0.17  0.434 0.   ]]\n",
      "↓ → ↓ ←\n",
      "↓ ← ↓ ←\n",
      "→ ↓ ↓ ←\n",
      "← → → ←\n"
     ]
    }
   ],
   "source": [
    "# --- FrozenLake (deterministyczne): 1 krok improvement z v_pi(random) ---\n",
    "\n",
    "# To samo, ale w środowisku epizodycznym FrozenLake\n",
    "pi_fl_greedy = greedy_policy_from_v(P_fl_det, v_fl, gamma=gamma_fl)\n",
    "\n",
    "print('FrozenLake (det.): polityka zachłanna względem v_pi(random):')\n",
    "\n",
    "# Konwencja akcji FrozenLake (jak w Gym): LEFT, DOWN, RIGHT, UP\n",
    "arrows_fl = {0:'←', 1:'↓', 2:'→', 3:'↑'}\n",
    "\n",
    "print(\"v_pi(random) jako siatka:\")\n",
    "pretty_matrix_as_grid(v_fl, nrow_fl, ncol_fl, decimals=3)\n",
    "\n",
    "\n",
    "# Wizualizacja ulepszonej polityki\n",
    "action_arrows(pi_fl_greedy, nrow_fl, ncol_fl, arrows_fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3de2e49",
   "metadata": {},
   "source": [
    "### Wskazówki (sekcja 2)\n",
    "\n",
    "- Tutaj po raz pierwszy w DP pojawia się jawnie **krok greedy** (policy improvement).\n",
    "- Dobry moment, by zobaczyć: *evaluation* odpowiada na pytanie „ile to warte?”, a *improvement* — „co wybrać?”.\n",
    "- Uwaga na konwencję akcji: Gridworld ma (UP,RIGHT,DOWN,LEFT), FrozenLake (LEFT,DOWN,RIGHT,UP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86f1ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_policy(pi_det: np.ndarray, nA: int) -> np.ndarray:\n",
    "    \"\"\"Zamiana polityki deterministycznej na macierzową (one-hot).\n",
    "\n",
    "    Wejście:\n",
    "    - pi_det: (nS,) gdzie pi_det[s] to indeks akcji\n",
    "    - nA: liczba akcji\n",
    "\n",
    "    Wyjście:\n",
    "    - pi: (nS, nA) gdzie pi[s, pi_det[s]] = 1.0, a reszta 0.0\n",
    "\n",
    "    Po co?\n",
    "    - policy evaluation oczekuje pi jako rozkładu po akcjach (nS x nA),\n",
    "      a greedy_policy_from_v zwraca pi_det (nS,).\n",
    "    \"\"\"\n",
    "    nS = pi_det.shape[0]\n",
    "    pi = np.zeros((nS, nA), dtype=float)\n",
    "    pi[np.arange(nS), pi_det.astype(int)] = 1.0\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b0c0c4",
   "metadata": {},
   "source": [
    "### Przerywnik: reprezentacja polityki (π(s) vs π(a|s))\n",
    "\n",
    "W poprzednich krokach spotykasz dwie reprezentacje polityki:\n",
    "- **macierz** `pi[a|s]` o kształcie `(nS, nA)` (potrzebna do policy evaluation),\n",
    "- **wektor** `pi_det[s]` o kształcie `(nS,)` (wygodny do wizualizacji i greedy improvement).\n",
    "\n",
    "W sekcji policy iteration nauczysz się je łączyć przez konwersję (one-hot).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e76816",
   "metadata": {},
   "source": [
    "### Dlaczego na tym etapie nie robimy jeszcze pełnej iteracji?\n",
    "\n",
    "Do tej pory wykonaliśmy dwa kroki:\n",
    "\n",
    "1. **policy evaluation** – policzyliśmy, ile warte są stany dla danej polityki,\n",
    "2. **policy improvement** – zmieniliśmy decyzje, wybierając najlepszą akcję w każdym stanie.\n",
    "\n",
    "Są to **dwa osobne kroki**, które same w sobie **nie tworzą jeszcze algorytmu iteracyjnego**.\n",
    "\n",
    "---\n",
    "\n",
    "### Gdzie pojawia się problem z „ręczną” iteracją?\n",
    "\n",
    "Problem nie jest koncepcyjny, tylko **techniczno-formatowy**.\n",
    "\n",
    "- policy evaluation oczekuje polityki w postaci  \n",
    "  **rozkładu po akcjach** `π(a|s)` (macierz `nS × nA`),\n",
    "\n",
    "- policy improvement zwraca politykę w postaci  \n",
    "  **jednej decyzji na stan** `π(s)` (wektor `nS`).\n",
    "\n",
    "Są to **dwie różne reprezentacje tej samej polityki**.\n",
    "\n",
    "Dlatego po samym improvement:\n",
    "- nie możemy od razu wykonać kolejnej ewaluacji,\n",
    "- dopóki nie zamienimy polityki na postać, którą ewaluacja rozumie.\n",
    "\n",
    "---\n",
    "\n",
    "### Co trzeba zrobić, żeby iteracja miała sens?\n",
    "\n",
    "Żeby połączyć te kroki w pętlę, potrzebujemy jednego dodatkowego elementu:\n",
    "\n",
    "1. po policy improvement zamieniamy politykę deterministyczną `π(s)`  \n",
    "   na postać macierzy `π(a|s)` (np. **one-hot**),\n",
    "2. dopiero wtedy ponownie wykonujemy policy evaluation,\n",
    "3. powtarzamy schemat:\n",
    "\n",
    "**evaluate → improve → convert → evaluate → …**\n",
    "\n",
    "To dokładnie składa się na algorytm **policy iteration**.\n",
    "\n",
    "---\n",
    "\n",
    "### Dlaczego rozbijamy to na etapy?\n",
    "\n",
    "Celowo:\n",
    "- najpierw poznajesz **każdy krok osobno**,\n",
    "- widzisz, *co dokładnie* robi evaluation i improvement,\n",
    "- a dopiero potem łączymy je w jeden algorytm.\n",
    "\n",
    "Dzięki temu policy iteration nie jest „czarną skrzynką”,\n",
    "tylko naturalnym złożeniem znanych już elementów.\n",
    "\n",
    "---\n",
    "\n",
    "### Co zmienia się w policy iteration?\n",
    "\n",
    "W policy iteration:\n",
    "- pętla jest jawna,\n",
    "- konwersja reprezentacji polityki jest częścią algorytmu,\n",
    "- iterujemy aż polityka przestanie się zmieniać.\n",
    "\n",
    "Dlatego **dopiero tam** pełna iteracja jest poprawna i sensowna.\n",
    "\n",
    "---\n",
    "\n",
    "### Podsumowanie\n",
    "\n",
    "> Na tym etapie poznajemy pojedyncze kroki.  \n",
    "> Pełną iterację wykonamy dopiero w policy iteration,\n",
    "> gdy wszystkie elementy są już na swoim miejscu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3353f0",
   "metadata": {},
   "source": [
    "## 3. Policy iteration\n",
    "\n",
    "Policy iteration powtarza:\n",
    "1) **policy evaluation** — policz $v_\\pi$\n",
    "2) **policy improvement** — zbuduj lepszą politykę $\\pi'$\n",
    "\n",
    "i kończy, gdy polityka przestaje się zmieniać.\n",
    "Stopujemy, gdy krok improvement nie zmienia już żadnej decyzji: \n",
    "dla każdego stanu `argmax_a Q(s,a)` daje tę samą akcję co poprzednio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845c6d71",
   "metadata": {},
   "source": [
    "Uwaga: evaluation oczekuje polityki jako macierzy `pi[s,a]`, a improvement zwraca `pi_det[s]`,\n",
    "więc w PI potrzebujemy konwersji one-hot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2332323",
   "metadata": {},
   "source": [
    "### Ćwiczenie 3 — policy iteration\n",
    "\n",
    "Zaimplementuj `policy_iteration(P, gamma, theta)` zwracające $(\\pi_*, v_*)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197b02b",
   "metadata": {},
   "source": [
    "![Policy iteration – schemat](policy_iteration_pseudocode.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b82acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_policy(pi_det: np.ndarray, nA: int) -> np.ndarray:\n",
    "    \"\"\"Konwersja reprezentacji polityki: deterministyczna -> rozkład po akcjach.\n",
    "\n",
    "    (część Ćwiczenia 3)\n",
    "    `pi_det` ma kształt (nS,) i zawiera indeks akcji w każdym stanie.\n",
    "    Zwróć macierz `pi` o kształcie (nS, nA), gdzie:\n",
    "        pi[s, pi_det[s]] = 1\n",
    "        pozostałe = 0\n",
    "\n",
    "    Wskazówka: `np.arange(nS)` bywa wygodne.\n",
    "    \"\"\"\n",
    "\n",
    "    nS = pi_det.shape[0]\n",
    "    pi = np.zeros((nS, nA), dtype=np.float64)\n",
    "    pi[np.arange(nS), pi_det] = 1.0\n",
    "\n",
    "    return pi\n",
    "\n",
    "def policy_iteration(P, gamma: float = 0.9, theta: float = 1e-10, max_iters: int = 1_000):\n",
    "    \"\"\"Policy Iteration: evaluation -> improvement aż do stabilizacji polityki.\n",
    "\n",
    "    (Ćwiczenie 3)\n",
    "    Zaimplementuj klasyczny algorytm policy iteration w oparciu o:\n",
    "    - iterative_policy_evaluation(...)\n",
    "    - greedy_policy_from_v(...)\n",
    "    - one_hot_policy(...)\n",
    "\n",
    "    Minimalne wymagania:\n",
    "    - startuj od polityki losowej (jednostajnej) `pi` o kształcie (nS, nA)\n",
    "    - w pętli:\n",
    "        1) policz v_pi\n",
    "        2) popraw politykę zachłannie -> pi_det_new\n",
    "        3) skonwertuj do `pi_new` (one-hot), żeby dało się znów ewaluować\n",
    "        4) zatrzymaj, jeśli polityka się nie zmienia\n",
    "    - zwróć (pi_det, v), gdzie `pi_det` jest deterministyczne (łatwe do wizualizacji)\n",
    "\n",
    "    Uwaga: kryterium \"polityka się nie zmienia\" możesz zrealizować na różne sposoby\n",
    "    (np. porównując `pi_det`, albo `pi` jako macierz).\n",
    "    \"\"\"\n",
    "\n",
    "    # Wyznacz nS oraz nA\n",
    "    nS = len(P)\n",
    "    nA = len(P[0])\n",
    "\n",
    "    # Zainicjalizuj politykę losową\n",
    "    pi = np.ones((nS, nA), dtype=np.float64) / nA\n",
    "    pi_det = np.zeros(nS, dtype=np.int64)\n",
    "\n",
    "    # Pętla PI\n",
    "    for _ in range(max_iters):\n",
    "        # 1) policy evaluation\n",
    "        v = iterative_policy_evaluation(P, pi, gamma=gamma, theta=theta)\n",
    "\n",
    "        # 2) policy improvement (greedy)\n",
    "        pi_det_new = greedy_policy_from_v(P, v, gamma=gamma)\n",
    "\n",
    "        # 3) check stability\n",
    "        if np.array_equal(pi_det, pi_det_new):\n",
    "            return pi_det_new, v\n",
    "\n",
    "        # 4) update policy\n",
    "        pi_det = pi_det_new\n",
    "        pi = one_hot_policy(pi_det, nA)\n",
    "\n",
    "    return (pi_det, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "211a7ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gridworld: v* (policy iteration), tabela 5x5:\n",
      "[[22.  24.4 22.  19.4 17.5]\n",
      " [19.8 22.  19.8 17.8 16. ]\n",
      " [17.8 19.8 17.8 16.  14.4]\n",
      " [16.  17.8 16.  14.4 13. ]\n",
      " [14.4 16.  14.4 13.  11.7]]\n",
      "\n",
      "Gridworld: π* (policy iteration):\n",
      "→ ↑ ← ↑ ←\n",
      "→ ↑ ↑ ← ←\n",
      "→ ↑ ↑ ↑ ↑\n",
      "→ ↑ ↑ ↑ ↑\n",
      "→ ↑ ↑ ↑ ↑\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Gridworld: policy iteration -> (π*, v*) ---\n",
    "\n",
    "pi_gw_star, v_gw_star = policy_iteration(P_gw, gamma=gamma_gw, theta=1e-10)\n",
    "\n",
    "print('Gridworld: v* (policy iteration), tabela 5x5:')\n",
    "pretty_matrix_as_grid(v_gw_star, nrow_gw, ncol_gw, decimals=1)\n",
    "\n",
    "print('\\nGridworld: π* (policy iteration):')\n",
    "action_arrows(pi_gw_star, nrow_gw, ncol_gw, arrows_gw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba9f5fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenLake (det.): v* (policy iteration), tabela 4x4:\n",
      "[[0.95 0.96 0.97 0.96]\n",
      " [0.96 0.   0.98 0.  ]\n",
      " [0.97 0.98 0.99 0.  ]\n",
      " [0.   0.99 1.   0.  ]]\n",
      "\n",
      "FrozenLake (det.): π* (policy iteration):\n",
      "↓ → ↓ ←\n",
      "↓ ← ↓ ←\n",
      "→ ↓ ↓ ←\n",
      "← → → ←\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- FrozenLake (deterministyczne): policy iteration -> (π*, v*) ---\n",
    "\n",
    "pi_fl_star, v_fl_star = policy_iteration(P_fl_det, gamma=gamma_fl, theta=1e-10)\n",
    "\n",
    "print('FrozenLake (det.): v* (policy iteration), tabela 4x4:')\n",
    "pretty_matrix_as_grid(v_fl_star, nrow_fl, ncol_fl, decimals=2)\n",
    "\n",
    "print('\\nFrozenLake (det.): π* (policy iteration):')\n",
    "action_arrows(pi_fl_star, nrow_fl, ncol_fl, arrows_fl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bcea0a",
   "metadata": {},
   "source": [
    "### Wskazówki (sekcja 3)\n",
    "\n",
    "- Policy iteration to pętla: *evaluation → improvement* aż do stabilizacji.\n",
    "- Można podkreślić intuicję: każda iteracja „nie pogarsza” polityki (policy improvement theorem),\n",
    "  więc dochodzimy do $\\pi_*$. (Dowód formalny jest w książce.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a507e",
   "metadata": {},
   "source": [
    "Po przejściu na politykę optymalną wartości stanów rosną, bo agent przestaje działać losowo i zaczyna maksymalizować długoterminowy zysk — w świecie ciągłym daje to duże wartości, a w epizodycznym wartości bliskie prawdopodobieństwu sukcesu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a0054",
   "metadata": {},
   "source": [
    "### Ćwiczenie 4 — Value iteration (intuicja)\n",
    "\n",
    "Value iteration **nie przechowuje jawnie polityki w trakcie obliczeń**.\n",
    "Zamiast tego:\n",
    "- bezpośrednio aktualizuje wartości stanów,\n",
    "- zakładając, że w każdym kroku wybierana jest najlepsza możliwa akcja.\n",
    "\n",
    "Kluczowa różnica względem policy iteration:\n",
    "- **policy iteration** rozdziela:\n",
    "  *evaluation → improvement*,\n",
    "- **value iteration** łączy oba kroki w jedną aktualizację:\n",
    "  $\n",
    "  v_{k+1}(s) = \\max_a \\mathbb{E}[r + \\gamma v_k(s')].\n",
    "  $\n",
    "\n",
    "Dopiero po zbieżności:\n",
    "- odczytujemy politykę jako zachłanną względem `v*`.\n",
    "\n",
    "---\n",
    "\n",
    "### Jak to czytać?\n",
    "- Każda iteracja „pcha” wartości w górę,\n",
    "  bo zakładamy coraz lepsze decyzje w przyszłości.\n",
    "- Polityka jest **implicit** (ukryta w operatorze `max`),\n",
    "  a nie aktualizowana jawnie jak w policy iteration.\n",
    "\n",
    "---\n",
    "\n",
    "### Dlaczego oba algorytmy dają to samo?\n",
    "Policy iteration i value iteration rozwiązują\n",
    "**to samo równanie optymalności Bellmana**,\n",
    "tylko różnymi metodami numerycznymi.\n",
    "\n",
    "Dlatego:\n",
    "- końcowe `v*` i `π*` są takie same,\n",
    "- różni się tylko droga dojścia do rozwiązania.\n",
    "\n",
    "---\n",
    "\n",
    "### Podsumowanie\n",
    "> Value iteration bezpośrednio poprawia wartości, zakładając optymalne decyzje w przyszłości, a politykę odczytujemy dopiero na końcu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feea256",
   "metadata": {},
   "source": [
    "\n",
    "![Value iteration – schemat](value_iteration_psuedocode.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4eef37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, gamma: float = 0.9, theta: float = 1e-10, max_iters: int = 100_000):\n",
    "    \"\"\"Value Iteration: bezpośrednio przybliża v* przez backup optymalności (max).\n",
    "\n",
    "    (Ćwiczenie 4)\n",
    "    Zaimplementuj value iteration:\n",
    "        v(s) <- max_a E[ r + gamma * v(s') ]\n",
    "\n",
    "    Wymagania:\n",
    "    - iteruj aż max zmiana w iteracji < theta\n",
    "    - obsłuż `terminated` (bootstrap = 0)\n",
    "    - po zbieżności odczytaj politykę zachłanną względem v* (np. przez greedy_policy_from_v)\n",
    "    - zwróć (pi_det, v)\n",
    "\n",
    "    Wskazówka: to prawie to samo co policy evaluation, tylko zamiast średniej po akcjach masz `max`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Wyznacz nS, nA; zainicjalizuj v\n",
    "    nS = len(P)\n",
    "    nA = len(P[0])\n",
    "    v = np.zeros(nS, dtype=np.float64)\n",
    "\n",
    "    # Pętla aktualizacji wartości z max po akcjach\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "\n",
    "        for s in range(nS):\n",
    "            v_old = v[s]\n",
    "            q_values = np.zeros(nA, dtype=np.float64)\n",
    "\n",
    "            for a in range(nA):\n",
    "                q = 0.0\n",
    "                for prob, s2, r, terminated in P[s][a]:\n",
    "                    if terminated:\n",
    "                        q += prob * r\n",
    "                    else:\n",
    "                        q += prob * (r + gamma * v[s2])\n",
    "                q_values[a] = q\n",
    "\n",
    "            v[s] = np.max(q_values)\n",
    "            delta = max(delta, abs(v_old - v[s]))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Po zbieżności zbuduj pi_det\n",
    "    pi_det = greedy_policy_from_v(P, v, gamma=gamma)\n",
    "\n",
    "    return pi_det, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "078500f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gridworld: v* (value iteration), tabela 5x5:\n",
      "[[22.  24.4 22.  19.4 17.5]\n",
      " [19.8 22.  19.8 17.8 16. ]\n",
      " [17.8 19.8 17.8 16.  14.4]\n",
      " [16.  17.8 16.  14.4 13. ]\n",
      " [14.4 16.  14.4 13.  11.7]]\n",
      "\n",
      "Gridworld: π* (value iteration):\n",
      "→ ↑ ← ↑ ←\n",
      "→ ↑ ↑ ← ←\n",
      "→ ↑ ↑ ↑ ↑\n",
      "→ ↑ ↑ ↑ ↑\n",
      "→ ↑ ↑ ↑ ↑\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Gridworld: value iteration -> (π*, v*) ---\n",
    "\n",
    "pi_gw_vi, v_gw_vi = value_iteration(P_gw, gamma=gamma_gw, theta=1e-10)\n",
    "\n",
    "print('Gridworld: v* (value iteration), tabela 5x5:')\n",
    "pretty_matrix_as_grid(v_gw_vi, nrow_gw, ncol_gw, decimals=1)\n",
    "\n",
    "print('\\nGridworld: π* (value iteration):')\n",
    "action_arrows(pi_gw_vi, nrow_gw, ncol_gw, arrows_gw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6c7c51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenLake (det.): v* (value iteration), tabela 4x4:\n",
      "[[0.95 0.96 0.97 0.96]\n",
      " [0.96 0.   0.98 0.  ]\n",
      " [0.97 0.98 0.99 0.  ]\n",
      " [0.   0.99 1.   0.  ]]\n",
      "\n",
      "FrozenLake (det.): π* (value iteration):\n",
      "↓ → ↓ ←\n",
      "↓ ← ↓ ←\n",
      "→ ↓ ↓ ←\n",
      "← → → ←\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- FrozenLake (deterministyczne): value iteration -> (π*, v*) ---\n",
    "\n",
    "pi_fl_vi, v_fl_vi = value_iteration(P_fl_det, gamma=gamma_fl, theta=1e-10)\n",
    "\n",
    "print('FrozenLake (det.): v* (value iteration), tabela 4x4:')\n",
    "pretty_matrix_as_grid(v_fl_vi, nrow_fl, ncol_fl, decimals=2)\n",
    "\n",
    "print('\\nFrozenLake (det.): π* (value iteration):')\n",
    "action_arrows(pi_fl_vi, nrow_fl, ncol_fl, arrows_fl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38b6380",
   "metadata": {},
   "source": [
    "### Wskazówki (sekcja 4)\n",
    "\n",
    "- Value iteration robi bezpośrednio backup optymalności (max) i zwykle wymaga więcej iteracji,\n",
    "  ale nie ma osobnej fazy policy evaluation.\n",
    "- Dydaktycznie: pokaż, że PI i VI dochodzą do tego samego $v_*$ / $\\pi_*$ (sekcja 5).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020cd845",
   "metadata": {},
   "source": [
    "## 5. Sprawdź: policy iteration vs value iteration\n",
    "\n",
    "Obie metody powinny dać to samo $v_*$ i $\\pi_*$ (przy tej samej definicji problemu i zbieżności).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "001160ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gridworld: czy polityki identyczne? -> True\n",
      "Gridworld: max |v_PI - v_VI| -> 0.0\n",
      "\n",
      "FrozenLake (det.): czy polityki identyczne? -> True\n",
      "FrozenLake (det.): max |v_PI - v_VI| -> 0.0\n"
     ]
    }
   ],
   "source": [
    "# Porównanie Gridworld\n",
    "print('Gridworld: czy polityki identyczne? ->', bool(np.array_equal(pi_gw_star, pi_gw_vi)))\n",
    "print('Gridworld: max |v_PI - v_VI| ->', float(np.max(np.abs(v_gw_star - v_gw_vi))))\n",
    "\n",
    "# Porównanie FrozenLake (det.)\n",
    "print('\\nFrozenLake (det.): czy polityki identyczne? ->', bool(np.array_equal(pi_fl_star, pi_fl_vi)))\n",
    "print('FrozenLake (det.): max |v_PI - v_VI| ->', float(np.max(np.abs(v_fl_star - v_fl_vi))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f486d5f4",
   "metadata": {},
   "source": [
    "## Porównanie: Policy Iteration vs Value Iteration\n",
    "\n",
    "Poniżej porównujemy dwa algorytmy Dynamic Programming,\n",
    "które **rozwiązują ten sam problem optymalny** i prowadzą do tych samych wyników\n",
    "`v*` oraz `π*`, ale robią to w inny sposób.\n",
    "\n",
    "---\n",
    "\n",
    "### Co jest wspólne?\n",
    "- Oba algorytmy rozwiązują **równanie optymalności Bellmana**.\n",
    "- Oba zakładają **znany model środowiska** (`P[s][a]`).\n",
    "- Oba kończą w tym samym punkcie:\n",
    "  - ta sama funkcja wartości `v*`,\n",
    "  - ta sama polityka optymalna `π*` (z dokładnością numeryczną).\n",
    "\n",
    "---\n",
    "\n",
    "### Kluczowe różnice\n",
    "\n",
    "| Cecha | Policy Iteration (PI) | Value Iteration (VI) |\n",
    "|------|----------------------|----------------------|\n",
    "| Co jest aktualizowane | Polityka **i** wartości | Tylko wartości |\n",
    "| Policy evaluation | Dokładna (iteracyjna do zbieżności) | Brak osobnej fazy |\n",
    "| Policy improvement | Jawny krok greedy | Ukryty w operatorze `max` |\n",
    "| Liczba iteracji | Mała (kilka kroków) | Większa |\n",
    "| Intuicja | „Poprawiam strategię” | „Poprawiam wartości” |\n",
    "\n",
    "---\n",
    "\n",
    "### Jak to interpretować?\n",
    "- **Policy iteration**:\n",
    "  - najpierw dokładnie ocenia bieżącą politykę,\n",
    "  - potem ją poprawia,\n",
    "  - szybko stabilizuje się na `π*`.\n",
    "\n",
    "- **Value iteration**:\n",
    "  - w każdym kroku zakłada, że przyszłe decyzje będą optymalne,\n",
    "  - stopniowo przybliża `v*`,\n",
    "  - politykę odczytujemy dopiero na końcu.\n",
    "\n",
    "---\n",
    "\n",
    "### Kiedy który algorytm?\n",
    "- Policy iteration jest bardziej „konceptualna” i czytelna.\n",
    "- Value iteration jest prostsza w implementacji i często używana w praktyce.\n",
    "- W tablicowych MDP oba są poprawne i równoważne.\n",
    "\n",
    "---\n",
    "\n",
    "### Podsumowanie\n",
    "> Policy iteration poprawia politykę krok po kroku, a value iteration bezpośrednio poprawia wartości — ale oba prowadzą do tego samego rozwiązania optymalnego.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a5ebbc",
   "metadata": {},
   "source": [
    "## 6. Deterministyczne vs „śliskie” FrozenLake\n",
    "\n",
    "W `is_slippery=True` środowisko jest stochastyczne: ta sama akcja może prowadzić do różnych stanów następnych.\n",
    "\n",
    "To ważne, bo:\n",
    "- DP nadal działa (bo korzysta z pełnego modelu `P`),\n",
    "- metody model-free (MC/TD) będą potrzebowały wielu próbek, by oszacować te same wartości.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61c21077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenLake (slippery): v* (value iteration), tabela 4x4:\n",
      "[[0.54 0.5  0.47 0.46]\n",
      " [0.56 0.   0.36 0.  ]\n",
      " [0.59 0.64 0.62 0.  ]\n",
      " [0.   0.74 0.86 0.  ]]\n",
      "\n",
      "FrozenLake (slippery): π* (value iteration):\n",
      "← ↑ ↑ ↑\n",
      "← ← ← ←\n",
      "↑ ↓ ← ←\n",
      "← → ↓ ←\n",
      "\n",
      "Wartość startu (s=0): det = 0.9509900498999999  | slippery = 0.5420259308172148\n"
     ]
    }
   ],
   "source": [
    "# FrozenLake (śliskie): policy/value iteration nadal działają, bo mamy pełny model P\n",
    "pi_fl_slip, v_fl_slip = value_iteration(P_fl_slip, gamma=gamma_fl, theta=1e-10)\n",
    "\n",
    "print('FrozenLake (slippery): v* (value iteration), tabela 4x4:')\n",
    "pretty_matrix_as_grid(v_fl_slip, nrow_fl, ncol_fl, decimals=2)\n",
    "\n",
    "print('\\nFrozenLake (slippery): π* (value iteration):')\n",
    "action_arrows(pi_fl_slip, nrow_fl, ncol_fl, arrows_fl)\n",
    "\n",
    "# Porównanie: wartość stanu startowego (S jest zwykle w (0,0) -> s=0)\n",
    "print('\\nWartość startu (s=0): det =', float(v_fl_vi[0]), ' | slippery =', float(v_fl_slip[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8387f8ad",
   "metadata": {},
   "source": [
    "### Wskazówki (sekcja 6)\n",
    "\n",
    "- Porównanie deterministic vs slippery jest dobrym mostem do rozdz. 5–6 (MC/TD):\n",
    "  gdy nie znamy modelu, potrzebujemy próbek, a „śliskość” zwiększa wariancję obserwacji.\n",
    "- Slippery obniża wartości, bo zmniejsza kontrolę nad ruchem: nawet przy optymalnej polityce częściej wpadasz w dziury, więc spada oczekiwana (zdyskontowana) szansa dotarcia do celu.\n",
    "- Polityka dla slippery może wyglądać mniej “intuicyjnie” w pojedynczych stanach, bo uwzględnia ryzyko rozmycia akcji i często wybiera bezpieczniejsze trajektorie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc142e",
   "metadata": {},
   "source": [
    "### Jak działa `slippery` w FrozenLake?\n",
    "\n",
    "Niech:\n",
    "- \\(a\\) — akcja wybrana przez agenta,\n",
    "- \\(a_{\\text{real}}\\) — akcja faktycznie wykonana przez środowisko,\n",
    "- akcje są indeksowane cyklicznie: \\(0,1,2,3\\).\n",
    "\n",
    "W trybie `slippery=True` środowisko realizuje akcję losowo:\n",
    "$$\n",
    "P(a_{\\text{real}} \\mid a)=\n",
    "\\begin{cases}\n",
    "\\frac{1}{3}, & a_{\\text{real}} \\in \\{(a-1)\\!\\!\\!\\!\\pmod 4,\\; a,\\; (a+1)\\!\\!\\!\\!\\pmod 4\\},\\\\\n",
    "0, & \\text{w przeciwnym razie.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Pełne przejście stanu ma więc postać:\n",
    "$$\n",
    "p(s', r \\mid s, a)\n",
    "=\\sum_{a_{\\text{real}}} P(a_{\\text{real}} \\mid a)\\; p(s', r \\mid s, a_{\\text{real}}).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Krótko:**  \n",
    "`slippery` oznacza, że **każda akcja jest „rozmyta” na trzy kierunki po \\(1/3\\)** w *każdym kroku*.  \n",
    "Wzory Bellmana się nie zmieniają — zmieniają się tylko **prawdopodobieństwa w \\(P[s][a]\\)**, więc wartości i polityka uwzględniają ryzyko.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b15e3c",
   "metadata": {},
   "source": [
    "### Pytania do dyskusji\n",
    "\n",
    "- Dlaczego DP jest realistyczne tylko wtedy, gdy znamy model (albo potrafimy go nauczyć)?\n",
    "- Kiedy policy iteration jest lepsze od value iteration (i odwrotnie)?\n",
    "- Jak zmienia się optymalna polityka, gdy FrozenLake jest „śliskie”?\n",
    "- Co się zmieni, gdy przejdziemy do metod model-free (MC/TD) i nie będziemy mieli `P`?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1178a",
   "metadata": {},
   "source": [
    "# Zakończenie/Projekt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc666274",
   "metadata": {},
   "source": [
    "## Bonus (obowiązkowe): ε-greedy improvement (modyfikacja greedy)\n",
    "\n",
    "Do tej pory budowaliśmy politykę **greedy**: w każdym stanie wybieramy jedną najlepszą akcję.\n",
    "Teraz zrobimy wariant **ε-greedy**, czyli politykę stochastyczną:\n",
    "\n",
    "- z prawdopodobieństwem $1-\\varepsilon$ wybieramy akcję najlepszą,\n",
    "- z prawdopodobieństwem $\\varepsilon$ wybieramy akcję losowo (równomiernie).\n",
    "\n",
    "W praktyce (dla każdego stanu):\n",
    "- najlepsza akcja (lub najlepsze akcje przy remisach) dostają dodatkową masę $(1-\\varepsilon)$,\n",
    "- każda akcja dostaje bazowo $\\varepsilon/n_A$.\n",
    "\n",
    "**Cel ćwiczenia:**\n",
    "1) skopiować logikę greedy (liczenie $Q(s,a)$),\n",
    "2) zamiast zwracać `pi_det`, zbudować macierz `pi_eps[s,a]`.\n",
    "\n",
    "**Uwaga o remisach:** jeśli kilka akcji ma takie samo \\(Q(s,a)\\), rozdzielamy $(1-\\varepsilon)$ równomiernie między te akcje.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
