{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb853c9",
   "metadata": {},
   "source": [
    "# Rozdział 3 — Skończone MDP (Robot + Gridworld + opcjonalnie FrozenLake) (wersja pełna)\n",
    "\n",
    "Lektura: Sutton & Barto, rozdz. 3.\n",
    "\n",
    "**Cel rozdziału:** nauczyć się *modelować* środowisko jako MDP i liczyć wartości **dokładnie** (gdy znamy model przejść).\n",
    "\n",
    "W tym notebooku rozróżniamy dwa tryby:\n",
    "- **Ewaluacja polityki**: liczymy $v_\\pi$ dla *zadanej* polityki $\\pi$.\n",
    "- **Optymalizacja**: szukamy $v_*$ i $\\pi_*$ (w tym rozdziale głównie konceptualnie / na małych przykładach).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb45e5",
   "metadata": {},
   "source": [
    "> **Notatki dla prowadzącego:**\n",
    "> - W tym rozdziale kluczowe jest rozróżnienie: **model świata** (parametry, przejścia) vs **polityka** (wybór akcji).\n",
    "> - Najpierw robimy *ewaluację* (bez max), a dopiero potem pokazujemy, gdzie wchodzi *optymalność* (max/argmax).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4c7fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup (importy + helpery) ---\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_policy_linear_system(P_pi: np.ndarray, r_pi: np.ndarray, gamma: float) -> np.ndarray:\n",
    "    \"\"\"Rozwiązuje równanie Bellmana dla danej polityki w postaci macierzowej.\n",
    "\n",
    "    Dla polityki pi mamy:\n",
    "        v = r_pi + gamma * P_pi * v\n",
    "    czyli:\n",
    "        (I - gamma * P_pi) v = r_pi\n",
    "\n",
    "    Zwraca wektor v (shape: [nS]).\n",
    "    \"\"\"\n",
    "    nS = P_pi.shape[0]\n",
    "    I = np.eye(nS)\n",
    "    return np.linalg.solve(I - gamma * P_pi, r_pi)\n",
    "\n",
    "def pretty_matrix_as_grid(v: np.ndarray, nrow: int, ncol: int, decimals: int = 1):\n",
    "    \"\"\"Pomocniczo: wyświetl wektor wartości jako siatkę (nrow x ncol).\"\"\"\n",
    "    grid = v.reshape(nrow, ncol)\n",
    "    with np.printoptions(precision=decimals, suppress=True):\n",
    "        print(grid)\n",
    "\n",
    "def action_arrows(pi_det: np.ndarray, nrow: int, ncol: int):\n",
    "    \"\"\"Zamienia deterministyczną politykę (akcja w każdym stanie) na strzałki w siatce.\"\"\"\n",
    "    arrows = {0:'↑', 1:'→', 2:'↓', 3:'←', None:'·'}\n",
    "    out = []\n",
    "    for r in range(nrow):\n",
    "        row = []\n",
    "        for c in range(ncol):\n",
    "            s = r*ncol + c\n",
    "            a = int(pi_det[s]) if pi_det[s] is not None else None\n",
    "            row.append(arrows.get(a, '?'))\n",
    "        out.append(' '.join(row))\n",
    "    print('\\n'.join(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d1ed4c",
   "metadata": {},
   "source": [
    "## 0. Wspólny interfejs MDP\n",
    "\n",
    "Używamy konwencji zgodnej z `gymnasium`:\n",
    "\n",
    "- `P[s][a]` to lista wyników `(p, s2, r, terminated)`\n",
    "- `pi[s,a]` to prawdopodobieństwo wyboru akcji `a` w stanie `s`\n",
    "\n",
    "Z tego budujemy:\n",
    "- macierz przejść polityki $P_\\pi$ (rozmiar $nS\\times nS$)\n",
    "- wektor nagród $r_\\pi$ (rozmiar $nS$)\n",
    "\n",
    "Takie $P_\\pi$ i $r_\\pi$ pozwalają rozwiązać $v_\\pi$ przez algebrę liniową."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9177eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budowa P_pi oraz r_pi (rozwiązanie)\n",
    "def build_P_r_for_policy(P, pi):\n",
    "    \"\"\"Buduje (P_pi, r_pi) dla zadanej polityki pi.\n",
    "\n",
    "    P: dict, P[s][a] -> list (p, s2, r, terminated)\n",
    "    pi: ndarray (nS, nA), pi[s,a] = P(A_t=a | S_t=s)\n",
    "\n",
    "    Uwaga o terminated:\n",
    "    - Jeśli przejście jest terminalne, to w równaniu wartości przyszła wartość jest 0,\n",
    "      więc NIE dodajemy wkładu do P_pi (bo i tak mnożyłby V(s2)=0).\n",
    "    \"\"\"\n",
    "    nS = len(P)\n",
    "    # nA bierzemy z pierwszego stanu\n",
    "    nA = len(P[0])\n",
    "    P_pi = np.zeros((nS, nS), dtype=float)\n",
    "    r_pi = np.zeros(nS, dtype=float)\n",
    "\n",
    "    for s in range(nS):\n",
    "        for a in range(nA):\n",
    "            w = float(pi[s, a])\n",
    "            if w == 0.0:\n",
    "                continue\n",
    "            outcomes = P[s][a]\n",
    "            if not outcomes:\n",
    "                continue\n",
    "            for (p, s2, r, terminated) in outcomes:\n",
    "                r_pi[s] += w * p * float(r)\n",
    "                if not terminated:\n",
    "                    P_pi[s, int(s2)] += w * p\n",
    "\n",
    "    return P_pi, r_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budowa P_pi oraz r_pi (rozwiązanie)\n",
    "def build_P_r_for_policy(P, pi):\n",
    "    \"\"\"\n",
    "    Buduje (P_pi, r_pi) dla zadanej polityki π.\n",
    "\n",
    "    Wejście:\n",
    "    - P: model środowiska w formacie\n",
    "         P[s][a] -> lista (p, s2, r, terminated)\n",
    "    - pi: polityka stochastyczna w postaci macierzy (nS, nA),\n",
    "          pi[s, a] = P(A_t = a | S_t = s)\n",
    "\n",
    "    Wyjście:\n",
    "    - P_pi: macierz przejść dla polityki π (nS x nS)\n",
    "    - r_pi: wektor nagród oczekiwanych dla polityki π (nS,)\n",
    "\n",
    "    Sens:\n",
    "    Z ogólnego MDP (P[s][a]) robimy „świat widziany przez politykę π”,\n",
    "    potrzebny do rozwiązania równania:\n",
    "        v = r_pi + γ P_pi v\n",
    "    \"\"\"\n",
    "    nS = len(P)              # liczba stanów\n",
    "    nA = len(P[0])           # liczba akcji (z pierwszego stanu)\n",
    "\n",
    "    # Macierz przejść i wektor nagród dla polityki π\n",
    "    P_pi = np.zeros((nS, nS), dtype=float)\n",
    "    r_pi = np.zeros(nS, dtype=float)\n",
    "\n",
    "    # Iterujemy po wszystkich stanach\n",
    "    for s in range(nS):\n",
    "\n",
    "        # Iterujemy po wszystkich akcjach\n",
    "        for a in range(nA):\n",
    "\n",
    "            # Waga akcji a w stanie s wg polityki π\n",
    "            w = float(pi[s, a])\n",
    "\n",
    "            # Jeśli polityka nigdy nie wybiera tej akcji, pomijamy ją\n",
    "            if w == 0.0:\n",
    "                continue\n",
    "\n",
    "            outcomes = P[s][a]\n",
    "\n",
    "            # Jeśli akcja jest niedostępna (pusta lista), pomijamy\n",
    "            if not outcomes:\n",
    "                continue\n",
    "\n",
    "            # Iterujemy po wszystkich możliwych skutkach akcji a w stanie s\n",
    "            for (p, s2, r, terminated) in outcomes:\n",
    "\n",
    "                # Składnik nagrody oczekiwanej:\n",
    "                # r_pi[s] = E_π[R_{t+1} | S_t = s]\n",
    "                r_pi[s] += w * p * float(r)\n",
    "\n",
    "                # Składnik przejść:\n",
    "                # P_pi[s, s2] = P(S_{t+1} = s2 | S_t = s, π)\n",
    "                #\n",
    "                # Jeśli przejście jest terminalne:\n",
    "                # - V(s2) = 0 w równaniu Bellmana,\n",
    "                # - więc NIE dodajemy go do macierzy P_pi\n",
    "                if not terminated:\n",
    "                    P_pi[s, int(s2)] += w * p\n",
    "\n",
    "    return P_pi, r_pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb61c45",
   "metadata": {},
   "source": [
    "> **Notatki dla prowadzącego (Ćw. 0.1):**\n",
    "> - To ćwiczenie jest kluczowe: pokazuje, jak przejść od opisu świata $P$ i zachowania $\\pi$ do macierzy $P_\\pi$.\n",
    "> - Warto podkreślić różnicę: **ewaluacja polityki** = uśrednianie po akcjach (bez max).\n",
    "> - Typowa pułapka: akcje niedostępne (puste listy) oraz obsługa `terminated`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64551b8e",
   "metadata": {},
   "source": [
    "## A. Recycling Robot (mały MDP z książki)\n",
    "\n",
    "To jest 2-stanowy przykład, idealny do:\n",
    "- ćwiczenia budowy modelu `P`,\n",
    "- policzenia $v_\\pi$ dokładnie,\n",
    "- pokazania, że zmiana parametrów świata może zmienić $\\pi_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4cff45",
   "metadata": {},
   "source": [
    "**Parametry (ściąga):** `alpha` (bezpieczeństwo SEARCH w H), `beta` (bezpieczeństwo SEARCH w L), `r_search`, `r_wait`, `rescue_cost` (kara za rozładowanie w L), `gamma`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eaa950",
   "metadata": {},
   "source": [
    "### Ćwiczenie A1.1 — Zbuduj model `P` dla robota recyklingowego\n",
    "\n",
    "Zaimplementuj `build_recycling_robot_P(...)`.\n",
    "\n",
    "Konwencje:\n",
    "- stany: `0=H`, `1=L`\n",
    "- akcje: `0=SEARCH`, `1=WAIT`, `2=RECHARGE`\n",
    "- jeśli akcja jest niedostępna w danym stanie (np. RECHARGE w H), zwróć pustą listę `[]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ffa79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aba66275",
   "metadata": {},
   "source": [
    "To jest klasyczny przykład „recycling robot” z Sutton & Barto (rozdz. 3).\n",
    "\n",
    "**Stany**\n",
    "- `H` (wysoka energia)\n",
    "- `L` (niska energia)\n",
    "\n",
    "**Akcje**\n",
    "- `search`: zbiera puszki (dobra nagroda, ale może rozładować baterię)\n",
    "- `wait`: czeka (mała nagroda)\n",
    "- `recharge`: tylko w stanie `L` (wraca do `H` z nagrodą 0)\n",
    "\n",
    "Parametry jak w książce:\n",
    "- Z `H`, gdy wybierzemy `search`: zostaje w `H` z prawd. $\\alpha$, przechodzi do `L` z prawd. $1-\\alpha$\n",
    "- Z `L`, gdy wybierzemy `search`: zostaje w `L` z prawd. $\\beta$ i dostaje $r_{search}$; z prawd. $1-\\beta$ robot się „rozładowuje”, jest odnaleziony i naładowany do `H` z nagrodą $-3$\n",
    "- `wait` utrzymuje poziom energii (nagroda $r_{wait}$)\n",
    "- `recharge` z `L` przechodzi do `H` z nagrodą $0$\n",
    "\n",
    "Model zapisujemy w stylu Gymnasium:\n",
    "`P[s][a] -> lista (prob, s2, r, terminated)`.\n",
    "\n",
    "\n",
    "\n",
    "**Podsumowanie przejść**\n",
    "\n",
    "Niech `H=0`, `L=1` oraz `search=0`, `wait=1`, `recharge=2`.\n",
    "\n",
    "| stan $s$ | akcja $a$ | następny stan $s'$ | prawdopodobieństwo $p$ | nagroda $r$ |\n",
    "|---|---|---|---|---|\n",
    "| H | search | H | $\\alpha$ | $r_{search}$ |\n",
    "| H | search | L | $1-\\alpha$ | $r_{search}$ |\n",
    "| H | wait | H | $1$ | $r_{wait}$ |\n",
    "| L | search | L | $\\beta$ | $r_{search}$ |\n",
    "| L | search | H | $1-\\beta$ | `rescue_cost` (w książce: $-3$) |\n",
    "| L | wait | L | $1$ | $r_{wait}$ |\n",
    "| L | recharge | H | $1$ | $0$ |\n",
    "\n",
    "Traktujemy `recharge` w stanie `H` jako akcję **niedostępną**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c16a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd78f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1.1 tests passed ✅\n"
     ]
    }
   ],
   "source": [
    "def build_recycling_robot_P(alpha=0.8, beta=0.4, r_search=5.0, r_wait=1.0, rescue_cost=-3.0):\n",
    "    \"\"\"Return (P, nS, nA) for Recycling Robot.\n",
    "\n",
    "    States: 0=H, 1=L\n",
    "    Actions: 0=SEARCH, 1=WAIT, 2=RECHARGE (only in L)\n",
    "\n",
    "    P[s][a] -> list (p, s2, r, terminated)\n",
    "    \"\"\"\n",
    "    # 1) Rozmiar MDP: 2 stany (H,L) i 3 akcje (SEARCH, WAIT, RECHARGE)\n",
    "    nS, nA = 2, 3\n",
    "    # 2) Inicjalizacja pustej struktury przejść\n",
    "    #    P[s][a] = [] oznacza brak zdefiniowanych przejść (np. akcja niedostępna)\n",
    "    P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "    # 3) Aliasowanie indeksów dla czytelności\n",
    "\n",
    "    H, L = 0, 1\n",
    "    SEARCH, WAIT, RECHARGE = 0, 1, 2\n",
    "    # -----------------------------\n",
    "    # Stan H (wysoka energia)\n",
    "    # -----------------------------\n",
    "    # SEARCH w H: nagroda r_search; przejście do H z p=alpha, do L z p=1-alpha\n",
    "    \n",
    "    # High energy (H)\n",
    "    P[H][SEARCH] = [\n",
    "        (alpha, H, r_search, False),\n",
    "        (1 - alpha, L, r_search, False),\n",
    "    ]\n",
    "    # WAIT w H: zawsze zostajemy w H; nagroda r_wait\n",
    "    P[H][WAIT] = [\n",
    "        (1.0, H, r_wait, False),\n",
    "    ]\n",
    "    # RECHARGE w H: akcja niedostępna w tym stanie\n",
    "    P[H][RECHARGE] = []  # not available in H\n",
    "\n",
    "    # -----------------------------\n",
    "    # Stan L (niska energia)\n",
    "    # -----------------------------\n",
    "    # SEARCH w L:\n",
    "    #   - z p=beta: zostajemy w L i dostajemy r_search\n",
    "    #   - z p=1-beta: rozładowanie -> powrót do H i kara rescue_cost\n",
    "    # Low energy (L)\n",
    "    P[L][SEARCH] = [\n",
    "        (beta, L, r_search, False),\n",
    "        (1 - beta, H, rescue_cost, False),\n",
    "    ]\n",
    "    # WAIT w L: zawsze zostajemy w L; nagroda r_wait\n",
    "    P[L][WAIT] = [\n",
    "        (1.0, L, r_wait, False),\n",
    "    ]\n",
    "    # RECHARGE w L: zawsze przechodzimy do H; nagroda 0\n",
    "    P[L][RECHARGE] = [\n",
    "        (1.0, H, 0.0, False),\n",
    "    ]\n",
    "\n",
    "    return P, nS, nA\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Testy poprawności modelu\n",
    "# --------------------------------------------------\n",
    "\n",
    "# --- testy ---\n",
    "P, nS, nA = build_recycling_robot_P(alpha=0.8, beta=0.4, r_search=5.0, r_wait=1.0, rescue_cost=-3.0)\n",
    "\n",
    "assert nS == 2 and nA == 3\n",
    "assert abs(sum(p for p, *_ in P[0][0]) - 1.0) < 1e-12  # H, SEARCH\n",
    "assert abs(sum(p for p, *_ in P[0][1]) - 1.0) < 1e-12  # H, WAIT\n",
    "assert P[0][2] == []                                  # H, RECHARGE not allowed\n",
    "assert abs(sum(p for p, *_ in P[1][0]) - 1.0) < 1e-12  # L, SEARCH\n",
    "assert abs(sum(p for p, *_ in P[1][1]) - 1.0) < 1e-12  # L, WAIT\n",
    "assert abs(sum(p for p, *_ in P[1][2]) - 1.0) < 1e-12  # L, RECHARGE\n",
    "\n",
    "print(\"A1.1 tests passed ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08ab8576",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0.8, 0, 5.0, False), (0.19999999999999996, 1, 5.0, False)],\n",
       "  1: [(1.0, 0, 1.0, False)],\n",
       "  2: []},\n",
       " 1: {0: [(0.4, 1, 5.0, False), (0.6, 0, -3.0, False)],\n",
       "  1: [(1.0, 1, 1.0, False)],\n",
       "  2: [(1.0, 0, 0.0, False)]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f08dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b8322c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e53d998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 0, 1.0, False)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(P[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fcc3d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(p for p, *_ in P[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e278dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3f149ea",
   "metadata": {},
   "source": [
    "> **Notatki dla prowadzącego (A1.1):**\n",
    "> - Te testy sprawdzają głównie **poprawność probabilistyczną** (sumy prawdopodobieństw = 1) i akcje niedostępne.\n",
    "> - Warto dopowiedzieć: to jest dokładnie obiekt $p(s',r\\mid s,a)$ w równaniach Bellmana.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf93b281",
   "metadata": {},
   "source": [
    "### Co sprawdzają te testy? (krótkie podsumowanie)\n",
    "\n",
    "Każdy test sprawdza, czy dla **każdej dostępnej akcji w danym stanie** lista `P[s][a]` opisuje **pełny i poprawny rozkład prawdopodobieństwa**.\n",
    "\n",
    "Konkretnie:\n",
    "- `P[s][a]` to lista możliwych wyników po wykonaniu akcji `a` w stanie `s`,\n",
    "- pierwszy element każdej krotki to **prawdopodobieństwo** tego wyniku,\n",
    "- testy sumują **tylko te prawdopodobieństwa**,\n",
    "- ich suma musi wynosić **1**, bo *jeden z tych wyników musi się wydarzyć*.\n",
    "\n",
    "Akcje niedostępne (np. `RECHARGE` w stanie `H`) są reprezentowane przez pustą listę i nie podlegają temu sprawdzeniu.\n",
    "\n",
    "---\n",
    "\n",
    "### Jednozdaniowe podsumowanie (na zajęcia)\n",
    "\n",
    "Testy sprawdzają, czy każda dostępna akcja w danym stanie definiuje pełny i poprawny rozkład prawdopodobieństwa, bo tylko wtedy równania Bellmana mają sens.\n",
    "\n",
    "---\n",
    "\n",
    "### Co można dodać dalej?\n",
    "\n",
    "- test semantyczny: czy **przejścia i nagrody** odpowiadają opisowi zadania  \n",
    "  (np. czy `RECHARGE` w `L` zawsze prowadzi do `H` z nagrodą `0`),\n",
    "- testy porównujące wartości `Q(s,a)` dla różnych akcji w jednym stanie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d322d",
   "metadata": {},
   "source": [
    "### Ćwiczenie A1.2 — Ewaluacja zadanej polityki $v_\\pi$\n",
    "\n",
    "Zdefiniuj politykę i policz $v_\\pi$ przez algebrę liniową.\n",
    "\n",
    "Wskazówka: polityka stochastyczna to macierz `pi[s,a]` (wiersze sumują się do 1 po dostępnych akcjach).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6a83d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_pi1(H), v_pi1(L) = [42.3729 38.1356]\n",
      "v_pi2(H), v_pi2(L) = [0.     0.3125]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "P, nS, nA = build_recycling_robot_P()\n",
    "\n",
    "H, L = 0, 1\n",
    "SEARCH, WAIT, RECHARGE = 0, 1, 2\n",
    "\n",
    "# Polityka 1: w H -> SEARCH, w L -> RECHARGE\n",
    "pi1 = np.zeros((nS, nA))\n",
    "pi1[H, SEARCH] = 1.0\n",
    "pi1[L, RECHARGE] = 1.0\n",
    "\n",
    "# Polityka 2: w H -> WAIT, w L -> WAIT\n",
    "pi2 = np.zeros((nS, nA))\n",
    "pi2[H, RECHARGE] = 1.0\n",
    "pi2[L, SEARCH] = 1.0\n",
    "\n",
    "P_pi1, r_pi1 = build_P_r_for_policy(P, pi1)\n",
    "v1 = evaluate_policy_linear_system(P_pi1, r_pi1, gamma)\n",
    "\n",
    "P_pi2, r_pi2 = build_P_r_for_policy(P, pi2)\n",
    "v2 = evaluate_policy_linear_system(P_pi2, r_pi2, gamma)\n",
    "\n",
    "print(\"v_pi1(H), v_pi1(L) =\", np.round(v1, 4))\n",
    "print(\"v_pi2(H), v_pi2(L) =\", np.round(v2, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b490dd39",
   "metadata": {},
   "source": [
    "> **Notatki dla prowadzącego (A1.2):**\n",
    "> - To jest *policy evaluation*: bez żadnego `max`. Liczymy dokładnie $v_\\pi$.\n",
    "> - Zwróć uwagę, że `pi` to część agenta, a `P` to część świata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1863a",
   "metadata": {},
   "source": [
    "## Jak rozumieć ten przykład? (policy evaluation)\n",
    "\n",
    "### Co jest czym?\n",
    "- **Model `P`**: opis świata — co może się stać po wykonaniu danej akcji w danym stanie  \n",
    "  (przejścia i nagrody).\n",
    "- **Polityka `π`**: reguła wyboru akcji w każdym stanie  \n",
    "  (czyli: *co robot robi w `H` i w `L`*).\n",
    "- **Funkcja wartości `v_π`**: mówi, ile średnio zyskujemy w przyszłości,\n",
    "  jeśli **zawsze** stosujemy daną politykę.\n",
    "\n",
    "---\n",
    "\n",
    "### Co robi ten kod?\n",
    "1. Definiujemy **ten sam świat** (`P`).\n",
    "2. Definiujemy **dwie różne polityki** (`π₁`, `π₂`), czyli dwa proste „instrukcje zachowania” robota.\n",
    "3. Dla każdej polityki **dokładnie liczymy** jej funkcję wartości `v_π`\n",
    "   (rozwiązując równania Bellmana dla danej polityki).\n",
    "4. Porównujemy, **która polityka jest lepsza** i w jakich stanach.\n",
    "\n",
    "---\n",
    "\n",
    "### Czego tu NIE robimy?\n",
    "- Nie szukamy polityki optymalnej.\n",
    "- Nie używamy maksymalizacji (`max`).\n",
    "- Nie uczymy się polityki.\n",
    "\n",
    "To jest **policy evaluation**, a nie **policy optimization**.\n",
    "\n",
    "---\n",
    "\n",
    "### Jednozdaniowe podsumowanie (na zajęcia)\n",
    "> W tym przykładzie porównujemy dwie z góry ustalone polityki w tym samym świecie i liczymy,\n",
    "> jak dobre są te strategie, zanim zaczniemy je optymalizować.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9756a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4da0f27a",
   "metadata": {},
   "source": [
    "## A1.3 — Co tu właściwie robimy? (intuicja polityki optymalnej)\n",
    "\n",
    "Ponieważ robot ma tylko 2 stany, możemy znaleźć najlepszą politykę przez sprawdzenie wszystkich deterministycznych polityk.\n",
    "To daje intuicję, czym jest $\\pi_*$, **bez wprowadzania jeszcze algorytmów DP**.\n",
    "\n",
    "### Co oznacza „polityka optymalna” w tym przykładzie?\n",
    "Polityka optymalna $\\\\pi_*$ to taka polityka, która **daje największą wartość oczekiwaną** (tu: $v(H)$),\n",
    "spośród wszystkich możliwych polityk.\n",
    "\n",
    "Formalnie:\n",
    "$$\n",
    "\\\\pi_* = \\\\arg\\\\max_{\\\\pi} v_{\\\\pi}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Dlaczego możemy to zrobić „ręcznie”?\n",
    "Robot ma tylko:\n",
    "- 2 stany (`H`, `L`),\n",
    "- skończoną liczbę akcji w każdym stanie.\n",
    "\n",
    "Liczba **wszystkich deterministycznych polityk** jest bardzo mała (6),\n",
    "więc możemy:\n",
    "1. wypisać wszystkie polityki,\n",
    "2. dla każdej dokładnie policzyć $v_{\\\\pi}$ (z równań Bellmana),\n",
    "3. wybrać najlepszą.\n",
    "\n",
    "To jest **brute force**, ale w tym małym przykładzie jest:\n",
    "- poprawne,\n",
    "- przejrzyste,\n",
    "- bardzo dydaktyczne.\n",
    "\n",
    "---\n",
    "\n",
    "### Czego tu jeszcze NIE robimy?\n",
    "- Nie rozwiązujemy równań optymalności Bellmana z `max` po akcjach.\n",
    "- Nie używamy algorytmów Dynamic Programming (value iteration, policy iteration).\n",
    "\n",
    "Zamiast tego:\n",
    "- **najpierw rozumiemy, czym jest $\\\\pi_*$**,  \n",
    "- a dopiero później (w kolejnym rozdziale) uczymy się,\n",
    "  jak znaleźć $\\\\pi_*$ bez przeglądania wszystkich polityk.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d57e23b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Najlepsza polityka (wg v(H)) ma v(H) = 42.3729\n",
      "pi*(H) = SEARCH\n",
      "pi*(L) = RECHARGE\n"
     ]
    }
   ],
   "source": [
    "def all_deterministic_policies_robot():\n",
    "    \"\"\"\n",
    "    Generuje wszystkie deterministyczne polityki dla robota recyklingowego.\n",
    "\n",
    "    Ponieważ:\n",
    "    - w stanie H dostępne są 2 sensowne akcje (SEARCH, WAIT),\n",
    "    - w stanie L dostępne są 3 akcje (SEARCH, WAIT, RECHARGE),\n",
    "\n",
    "    liczba wszystkich deterministycznych polityk wynosi 2 * 3 = 6.\n",
    "\n",
    "    Każda polityka pi jest macierzą:\n",
    "        pi[s, a] = 1  -> akcja a jest zawsze wybierana w stanie s\n",
    "        pi[s, a] = 0  -> akcja a nigdy nie jest wybierana w stanie s\n",
    "    \"\"\"\n",
    "    # Indeksy stanów\n",
    "    H, L = 0, 1\n",
    "\n",
    "    # Indeksy akcji\n",
    "    SEARCH, WAIT, RECHARGE = 0, 1, 2\n",
    "\n",
    "    policies = []\n",
    "\n",
    "    # Wybieramy wszystkie możliwe akcje w stanie H\n",
    "    for aH in [SEARCH, WAIT]:\n",
    "\n",
    "        # Wybieramy wszystkie możliwe akcje w stanie L\n",
    "        for aL in [SEARCH, WAIT, RECHARGE]:\n",
    "\n",
    "            # Tworzymy pustą politykę (2 stany x 3 akcje)\n",
    "            pi = np.zeros((2, 3))\n",
    "\n",
    "            # Polityka deterministyczna:\n",
    "            # w stanie H zawsze wybieramy akcję aH\n",
    "            pi[H, aH] = 1.0\n",
    "\n",
    "            # w stanie L zawsze wybieramy akcję aL\n",
    "            pi[L, aL] = 1.0\n",
    "\n",
    "            # Dodajemy gotową politykę do listy\n",
    "            policies.append(pi)\n",
    "\n",
    "    return policies\n",
    "\n",
    "\n",
    "def best_policy_robot(P, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Wybiera najlepszą politykę spośród wszystkich deterministycznych polityk.\n",
    "\n",
    "    Kryterium:\n",
    "    - maksymalizujemy wartość v_pi(H),\n",
    "      czyli oczekiwany zdyskontowany zwrot,\n",
    "      jeśli startujemy w stanie H i stosujemy politykę pi.\n",
    "\n",
    "    Uwaga dydaktyczna:\n",
    "    - to jest brute force (sprawdzamy wszystkie polityki),\n",
    "    - działa tylko dlatego, że MDP jest bardzo małe.\n",
    "    \"\"\"\n",
    "    best_pi = None    # najlepsza znaleziona polityka\n",
    "    best_vH = None    # jej wartość v_pi(H)\n",
    "\n",
    "    # Iterujemy po wszystkich możliwych politykach\n",
    "    for pi in all_deterministic_policies_robot():\n",
    "\n",
    "        # Budujemy model świata \"widoczny\" dla danej polityki:\n",
    "        # P_pi   — macierz przejść dla tej polityki\n",
    "        # r_pi   — wektor nagród dla tej polityki\n",
    "        P_pi, r_pi = build_P_r_for_policy(P, pi)\n",
    "\n",
    "        # Liczymy dokładnie funkcję wartości v_pi,\n",
    "        # rozwiązując układ równań Bellmana:\n",
    "        # v = r_pi + gamma * P_pi * v\n",
    "        v = evaluate_policy_linear_system(P_pi, r_pi, gamma)\n",
    "\n",
    "        # Interesuje nas wartość w stanie H\n",
    "        vH = float(v[0])\n",
    "\n",
    "        # Sprawdzamy, czy ta polityka jest lepsza od dotychczasowej\n",
    "        if best_vH is None or vH > best_vH:\n",
    "            best_vH = vH\n",
    "            best_pi = pi\n",
    "\n",
    "    return best_pi, best_vH\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Uruchomienie: znalezienie polityki optymalnej\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Budujemy model świata robota\n",
    "P, nS, nA = build_recycling_robot_P()\n",
    "\n",
    "# Szukamy najlepszej polityki (brute force)\n",
    "pi_star, vH_star = best_policy_robot(P, gamma=0.9)\n",
    "\n",
    "# Mapowanie indeksów akcji na czytelne nazwy\n",
    "action_name = {0: \"SEARCH\", 1: \"WAIT\", 2: \"RECHARGE\"}\n",
    "\n",
    "print(\"Najlepsza polityka (wg v(H)) ma v(H) =\", round(vH_star, 4))\n",
    "print(\"pi*(H) =\", action_name[int(np.argmax(pi_star[0]))])\n",
    "print(\"pi*(L) =\", action_name[int(np.argmax(pi_star[1]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ac440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a81bf685",
   "metadata": {},
   "source": [
    "> **Notatki dla prowadzącego (A1.3):**\n",
    "> - To jest prosty, ale bardzo czysty most do pojęcia $\\pi_*$.\n",
    "> - Podkreśl: tutaj *nie używamy DP*, tylko \"brute force\" bo MDP jest malutkie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f2aaa",
   "metadata": {},
   "source": [
    "\n",
    "### Jednozdaniowe podsumowanie (na zajęcia)\n",
    "> W tym ćwiczeniu znajdujemy politykę optymalną przez sprawdzenie wszystkich możliwych strategii,\n",
    "> co daje intuicję, czym jest $\\\\pi_*$, zanim poznamy algorytmy Dynamic Programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37ef56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ff97a50",
   "metadata": {},
   "source": [
    "Notatka dotycząca - continuing vs episodic tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0491c6ba",
   "metadata": {},
   "source": [
    "## Uwaga: zadanie ciągłe (continuing) vs epizodyczne (episodic)\n",
    "\n",
    "Ten przykład robota recyklingowego jest **zadaniem ciągłym (continuing task)**.\n",
    "\n",
    "### Co to znaczy w praktyce?\n",
    "- Nie ma stanu terminalnego.\n",
    "- Robot działa **bez końca**.\n",
    "- Funkcja wartości $v_\\pi(s)$ opisuje **zdyskontowaną sumę nagród w nieskończonej przyszłości**:\n",
    "  $$\n",
    "  v_\\pi(s) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1}\\right].\n",
    "  $$\n",
    "\n",
    "Dlatego wartości, takie jak `v(H) ≈ 42`, mogą być **znacznie większe niż pojedyncze nagrody** (np. `r_search = 5`).\n",
    "\n",
    "---\n",
    "\n",
    "### Dla porównania: zadanie epizodyczne\n",
    "W zadaniu epizodycznym:\n",
    "- istnieje stan terminalny,\n",
    "- epizod się kończy,\n",
    "- suma nagród jest skończona **nawet bez dyskontowania**,\n",
    "- często można przyjąć $\\gamma = 1$.\n",
    "\n",
    "Przykład: klasyczny gridworld z polem terminalnym.\n",
    "\n",
    "---\n",
    "\n",
    "### Dlaczego tu potrzebujemy $\\gamma < 1$?\n",
    "W zadaniu ciągłym:\n",
    "- bez dyskontowania ($\\gamma = 1$),\n",
    "- suma nagród mogłaby być nieskończona.\n",
    "\n",
    "Dyskontowanie:\n",
    "- zapewnia, że $v_\\pi(s)$ jest skończone,\n",
    "- formalizuje fakt, że **nagrody bliższe w czasie są ważniejsze**.\n",
    "\n",
    "---\n",
    "\n",
    "### Jednozdaniowe podsumowanie (na zajęcia)\n",
    "> Robot recyklingowy to zadanie ciągłe: nie ma końca epizodu, więc funkcja wartości mierzy długoterminowy, zdyskontowany zysk, a nie wynik pojedynczego epizodu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1b43c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44baded3",
   "metadata": {},
   "source": [
    "## A1.4 — Sweep parametrów środowiska: kiedy zmienia się π*(L)?\n",
    "Teraz zmieniamy `beta` i `rescue_cost` i obserwujemy, jak zmienia się optymalna decyzja w stanie `L`.\n",
    "To pokazuje, że **parametry świata** wpływają na to, jaka polityka jest najlepsza.\n",
    "\n",
    "### Cel tego eksperymentu\n",
    "Celem jest pokazanie, że **polityka optymalna nie jest stała**, lecz zależy od\n",
    "**parametrów świata (MDP)**.\n",
    "\n",
    "Nie zmieniamy algorytmu ani definicji optymalności —\n",
    "zmieniamy tylko to, *jak działa środowisko*,\n",
    "i obserwujemy, jak zmienia się najlepsza decyzja robota w stanie `L`.\n",
    "\n",
    "---\n",
    "\n",
    "### Co jest stałe, a co zmieniamy?\n",
    "**Stałe:**\n",
    "- `alpha = 0.8` — SEARCH w `H` jest dość bezpieczne\n",
    "- `r_search = 5`, `r_wait = 1`\n",
    "- `gamma = 0.9` (zadanie ciągłe)\n",
    "\n",
    "**Zmieniamy:**\n",
    "- `beta` — jak bezpieczne jest SEARCH w stanie `L`\n",
    "- `rescue_cost` — jak bolesna jest awaria przy SEARCH w `L`\n",
    "\n",
    "Dla każdej pary `(beta, rescue_cost)`:\n",
    "- wyznaczamy politykę optymalną `π*`,\n",
    "- wypisujemy decyzje w stanach `H` i `L`.\n",
    "\n",
    "---\n",
    "\n",
    "### Jak czytać tabelę wyników?\n",
    "Każdy wiersz odpowiada **innemu światu** (innemu MDP),\n",
    "ale rozwiązujemy **to samo zadanie optymalizacji**.\n",
    "\n",
    "Kolumny:\n",
    "- `beta`, `rescue_cost` — parametry świata,\n",
    "- `pi*(H)` — najlepsza decyzja w stanie wysokiej energii,\n",
    "- `pi*(L)` — najlepsza decyzja w stanie niskiej energii.\n",
    "\n",
    "---\n",
    "\n",
    "### Najważniejsze obserwacje\n",
    "1. **`π*(H)` jest zawsze `SEARCH`.**  \n",
    "   Przy tych parametrach opłaca się szukać, gdy energia jest wysoka.\n",
    "\n",
    "2. **`π*(L)` zmienia się zależnie od parametrów świata.**\n",
    "   - Małe `beta` (SEARCH w `L` jest ryzykowne) → `RECHARGE`\n",
    "   - Bardzo ujemny `rescue_cost` (duża kara) → `RECHARGE`\n",
    "   - Duże `beta` (SEARCH w `L` prawie zawsze się udaje) → `SEARCH`\n",
    "\n",
    "3. Widać **efekt progu**:\n",
    "   niewielka zmiana parametrów może zmienić najlepszą decyzję w stanie `L`.\n",
    "\n",
    "---\n",
    "\n",
    "### Wniosek pojęciowy\n",
    "Parametry świata **nie są polityką**,  \n",
    "ale wpływają na **wartości akcji `Q(s,a)`**.\n",
    "\n",
    "Polityka optymalna `π*` wynika z porównania tych wartości,\n",
    "więc gdy parametry zmieniają ich ranking,\n",
    "**zmienia się także `π*`.**\n",
    "\n",
    "---\n",
    "\n",
    "### Jednozdaniowe podsumowanie (na zajęcia)\n",
    "> Ten eksperyment pokazuje, że polityka optymalna nie jest „na stałe”,\n",
    "> lecz zależy od tego, jak ryzykowne i kosztowne są decyzje w danym świecie.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de443ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustawienia stałe: alpha= 0.8 r_search= 5.0 r_wait= 1.0 gamma= 0.9\n",
      "beta   rescue_cost   pi*(H)      pi*(L)\n",
      "----   ----------    --------    --------\n",
      "0.1           -1.0    SEARCH     RECHARGE\n",
      "0.1           -3.0    SEARCH     RECHARGE\n",
      "0.1           -6.0    SEARCH     RECHARGE\n",
      "0.1          -10.0    SEARCH     RECHARGE\n",
      "0.3           -1.0    SEARCH     RECHARGE\n",
      "0.3           -3.0    SEARCH     RECHARGE\n",
      "0.3           -6.0    SEARCH     RECHARGE\n",
      "0.3          -10.0    SEARCH     RECHARGE\n",
      "0.5           -1.0    SEARCH     SEARCH  \n",
      "0.5           -3.0    SEARCH     RECHARGE\n",
      "0.5           -6.0    SEARCH     RECHARGE\n",
      "0.5          -10.0    SEARCH     RECHARGE\n",
      "0.7           -1.0    SEARCH     SEARCH  \n",
      "0.7           -3.0    SEARCH     RECHARGE\n",
      "0.7           -6.0    SEARCH     RECHARGE\n",
      "0.7          -10.0    SEARCH     RECHARGE\n",
      "0.9           -1.0    SEARCH     SEARCH  \n",
      "0.9           -3.0    SEARCH     SEARCH  \n",
      "0.9           -6.0    SEARCH     SEARCH  \n",
      "0.9          -10.0    SEARCH     SEARCH  \n",
      "\n",
      "Wskazówka: obserwuj, kiedy π*(L) zmienia się z SEARCH na RECHARGE.\n"
     ]
    }
   ],
   "source": [
    "# A1.4 — Sweep parametrów: kiedy zmienia się π*(L)\n",
    "\n",
    "alpha = 0.8        # bezpieczeństwo SEARCH w H\n",
    "r_search = 5.0     # nagroda za SEARCH\n",
    "r_wait = 1.0       # nagroda za WAIT\n",
    "gamma = 0.9        # dyskontowanie (zadanie ciągłe)\n",
    "\n",
    "beta_list = [0.1, 0.3, 0.5, 0.7, 0.9]        # bezpieczeństwo SEARCH w L\n",
    "rescue_list = [-1.0, -3.0, -6.0, -10.0]     # kara za awarię w L\n",
    "\n",
    "action_name = {0: \"SEARCH\", 1: \"WAIT\", 2: \"RECHARGE\"}  # mapowanie akcji\n",
    "\n",
    "print(\"Ustawienia stałe:\", \"alpha=\", alpha, \"r_search=\", r_search,\n",
    "      \"r_wait=\", r_wait, \"gamma=\", gamma)\n",
    "print(\"beta   rescue_cost   pi*(H)      pi*(L)\")\n",
    "print(\"----   ----------    --------    --------\")\n",
    "\n",
    "# Dla każdej pary (beta, rescue_cost) sprawdzamy, jaka polityka jest optymalna\n",
    "for beta in beta_list:\n",
    "    for rescue_cost in rescue_list:\n",
    "\n",
    "        # budujemy świat (MDP) z tymi parametrami\n",
    "        P, _, _ = build_recycling_robot_P(alpha=alpha, beta=beta,\n",
    "                                          r_search=r_search,\n",
    "                                          r_wait=r_wait,\n",
    "                                          rescue_cost=rescue_cost)\n",
    "\n",
    "        # znajdujemy politykę optymalną π* (brute force)\n",
    "        pi_star, _ = best_policy_robot(P, gamma=gamma)\n",
    "\n",
    "        # odczytujemy decyzje w H i L\n",
    "        aH = action_name[int(np.argmax(pi_star[0]))]\n",
    "        aL = action_name[int(np.argmax(pi_star[1]))]\n",
    "\n",
    "        # wypisujemy wynik\n",
    "        print(f\"{beta:0.1f}     {rescue_cost:>10.1f}    {aH:<8}   {aL:<8}\")\n",
    "\n",
    "print(\"\\nWskazówka: obserwuj, kiedy π*(L) zmienia się z SEARCH na RECHARGE.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9859fa39",
   "metadata": {},
   "source": [
    "**Komentarz (krótko):**\n",
    "- `pi*(H)` wychodzi tu stale jako `SEARCH`.\n",
    "- `pi*(L)` przełącza się między `SEARCH` i `RECHARGE` zależnie od `beta` i `rescue_cost`.\n",
    "- Im mniejsze `beta` (bardziej ryzykowne SEARCH w L) lub im bardziej ujemny `rescue_cost`, tym częściej wygrywa `RECHARGE`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b9648",
   "metadata": {},
   "source": [
    "### Notatki dla prowadzącego (A1.4)\n",
    "\n",
    "- To jest kluczowy moment, by powiedzieć:\n",
    "  **parametry świata ≠ polityka**, ale zmieniają ranking `Q(s,a)`.\n",
    "\n",
    "- Warto poprosić studentów o interpretację 2–3 konkretnych wierszy:\n",
    "  np.:\n",
    "  - `beta = 0.5, rescue = -1` vs `beta = 0.5, rescue = -3`\n",
    "  - `beta = 0.9, rescue = -10` (dlaczego SEARCH nadal wygrywa?)\n",
    "\n",
    "- To bardzo naturalnie prowadzi do:\n",
    "  *„jak znaleźć π* bez brute-force?”* → Dynamic Programming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78f280",
   "metadata": {},
   "source": [
    "## B. Gridworld 5x5 (Example 3.5 vs 3.8)\n",
    "\n",
    "Tutaj świadomie pokazujemy **dwa różne problemy**:\n",
    "1) **Ewaluacja**: polityka równoprawdopodobna (equiprobable) i liczenie $v_\\pi$.\n",
    "2) **Optymalizacja**: liczenie $v_*$ i polityki optymalnej $\\pi_*$.\n",
    "\n",
    "Różnica: w (1) mamy uśrednianie po akcjach (bez `max`), a w (2) pojawia się `max/argmax`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394c003",
   "metadata": {},
   "source": [
    "### Opis środowiska (skrót)\n",
    "\n",
    "- Siatka 5x5, akcje: `0=UP`, `1=RIGHT`, `2=DOWN`, `3=LEFT`.\n",
    "- Jeśli ruch wychodzi poza planszę: zostajemy w miejscu i dostajemy nagrodę `-1`.\n",
    "- Są dwa specjalne pola `A` i `B`:\n",
    "  - z `A` niezależnie od akcji przechodzimy do `A'` z nagrodą `+10`,\n",
    "  - z `B` niezależnie od akcji przechodzimy do `B'` z nagrodą `+5`.\n",
    "- W tym przykładzie używamy $gamma=0.9$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f951929d",
   "metadata": {},
   "source": [
    "### Ćwiczenie B1 — Zbuduj model `P` dla Gridworld (A/B teleport)\n",
    "\n",
    "Zaimplementuj `build_gridworld_AB_P()` zwracające `(P, nS, nA, nrow, ncol)`.\n",
    "\n",
    "Wskazówki:\n",
    "- `nrow=ncol=5`, `nS=25`, `nA=4`.\n",
    "- Akcja zawsze deterministyczna: w `P[s][a]` jest pojedynczy wpis z `p=1.0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d55c0f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1 tests passed ✅\n"
     ]
    }
   ],
   "source": [
    "def build_gridworld_AB_P():\n",
    "    \"\"\"Gridworld 5x5 z A/B teleportami (jak w rozdz. 3 książki).\"\"\"\n",
    "    nrow, ncol = 5, 5                      # rozmiar planszy\n",
    "    nS, nA = nrow * ncol, 4                # 25 stanów, 4 akcje\n",
    "    P = {s: {a: [] for a in range(nA)} for s in range(nS)}  # P[s][a] -> lista wyników\n",
    "\n",
    "    # pozycje specjalne: A->A' (+10), B->B' (+5)\n",
    "    A, Aprime, reward_A = (0, 1), (4, 1), 10.0\n",
    "    B, Bprime, reward_B = (0, 3), (2, 3), 5.0\n",
    "\n",
    "    # mapowanie stan <-> (wiersz, kolumna)\n",
    "    def s2pos(s): return (s // ncol, s % ncol)\n",
    "    def pos2s(r, c): return r * ncol + c\n",
    "\n",
    "    # akcje: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT (wektory ruchu)\n",
    "    moves = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}\n",
    "\n",
    "    for s in range(nS):\n",
    "        r, c = s2pos(s)                    # gdzie jesteśmy na planszy?\n",
    "\n",
    "        # TELEPORTY MAJĄ PRIORYTET: z A/B zawsze teleport, niezależnie od akcji\n",
    "        if (r, c) == A:\n",
    "            s2 = pos2s(*Aprime)\n",
    "            for a in range(nA):\n",
    "                P[s][a] = [(1.0, s2, reward_A, False)]\n",
    "            continue\n",
    "\n",
    "        if (r, c) == B:\n",
    "            s2 = pos2s(*Bprime)\n",
    "            for a in range(nA):\n",
    "                P[s][a] = [(1.0, s2, reward_B, False)]\n",
    "            continue\n",
    "\n",
    "        # STANDARDOWE RUCHY: deterministycznie przesuwamy się zgodnie z akcją\n",
    "        for a in range(nA):\n",
    "            dr, dc = moves[a]\n",
    "            r2, c2 = r + dr, c + dc\n",
    "\n",
    "            # OFF-GRID: zostajemy w miejscu i dostajemy -1\n",
    "            if (r2 < 0) or (r2 >= nrow) or (c2 < 0) or (c2 >= ncol):\n",
    "                s2 = s\n",
    "                reward = -1.0\n",
    "            else:\n",
    "                s2 = pos2s(r2, c2)\n",
    "                reward = 0.0\n",
    "\n",
    "            P[s][a] = [(1.0, s2, reward, False)]  # deterministyczne przejście\n",
    "\n",
    "    return P, nS, nA, nrow, ncol\n",
    "\n",
    "\n",
    "# --- testy podstawowe ---\n",
    "P, nS, nA, nrow, ncol = build_gridworld_AB_P()\n",
    "assert nS == 25 and nA == 4 and nrow == 5 and ncol == 5\n",
    "for s in range(nS):\n",
    "    for a in range(nA):\n",
    "        assert len(P[s][a]) == 1\n",
    "        p, s2, r, term = P[s][a][0]\n",
    "        assert abs(p - 1.0) < 1e-12\n",
    "        assert term is False\n",
    "print(\"B1 tests passed ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68893942",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c8513368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1 semantic tests passed ✅\n"
     ]
    }
   ],
   "source": [
    "# --- testy semantyczne (logika świata, nie tylko format) ---\n",
    "\n",
    "P, nS, nA, nrow, ncol = build_gridworld_AB_P()\n",
    "\n",
    "# pomocniczo: mapowanie (r,c) <-> s\n",
    "def pos2s(r, c): \n",
    "    return r * ncol + c\n",
    "\n",
    "A = (0, 1); Aprime = (4, 1); reward_A = 10.0\n",
    "B = (0, 3); Bprime = (2, 3); reward_B = 5.0\n",
    "\n",
    "sA = pos2s(*A)\n",
    "sAprime = pos2s(*Aprime)\n",
    "sB = pos2s(*B)\n",
    "sBprime = pos2s(*Bprime)\n",
    "\n",
    "# 1) Teleport A: niezależnie od akcji zawsze idziemy do A' z nagrodą +10\n",
    "for a in range(nA):\n",
    "    assert P[sA][a] == [(1.0, sAprime, reward_A, False)]\n",
    "\n",
    "# 2) Teleport B: niezależnie od akcji zawsze idziemy do B' z nagrodą +5\n",
    "for a in range(nA):\n",
    "    assert P[sB][a] == [(1.0, sBprime, reward_B, False)]\n",
    "\n",
    "# 3) Off-grid: z narożnika (0,0) ruch UP lub LEFT -> zostajemy i dostajemy -1\n",
    "s00 = pos2s(0, 0)\n",
    "UP, RIGHT, DOWN, LEFT = 0, 1, 2, 3\n",
    "\n",
    "assert P[s00][UP]   == [(1.0, s00, -1.0, False)]\n",
    "assert P[s00][LEFT] == [(1.0, s00, -1.0, False)]\n",
    "\n",
    "# 4) Normalny ruch: z (0,0) ruch RIGHT -> (0,1) z nagrodą 0 (uwaga: (0,1) to A,\n",
    "# ale test dotyczy przejścia Z (0,0), więc jest normalne)\n",
    "s01 = pos2s(0, 1)\n",
    "assert P[s00][RIGHT] == [(1.0, s01, 0.0, False)]\n",
    "\n",
    "print(\"B1 semantic tests passed ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "76b88486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 21, 10.0, False)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[sA][a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45231b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0e359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "668855fa",
   "metadata": {},
   "source": [
    "> **Notatki dla prowadzącego (B1):**\n",
    "> - Studenci często mylą priorytet teleportów vs ruchów — tu teleporty nadpisują wszystko.\n",
    "> - Ten przykład dobrze ćwiczy \"świat jako funkcja\" (przepisy) zamieniany na tablicę przejść.\n",
    "\n",
    "\n",
    "### Po co to ćwiczenie? (dydaktycznie)\n",
    "\n",
    "To ćwiczenie uczy, jak zamienić opis słowny środowiska na formalny model MDP `P[s][a]`.\n",
    "To kluczowy krok przed równaniami Bellmana:\n",
    "\n",
    "- najpierw kodujemy **świat** (przepisy ruchu, teleporty, kary),\n",
    "- potem, w kolejnych ćwiczeniach, liczymy **wartości** dla zadanej polityki (średnia) albo dla optymalnej (max).\n",
    "\n",
    "Najważniejsza pułapka: **teleporty A/B mają priorytet** i nadpisują zwykły ruch — niezależnie od wybranej akcji.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01973b4a",
   "metadata": {},
   "source": [
    "### B2 — Polityka równoprawdopodobna (equiprobable) i $v_\\pi$\n",
    "\n",
    "Tu liczymy $v_\\pi$ dla polityki, która w każdym stanie wybiera każdy kierunek z prawdopodobieństwem 0.25.\n",
    "To jest klasyczny przykład **policy evaluation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1dff645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_pi jako tabela 5x5 (zaokrąglenie):\n",
      "[[ 3.3  8.8  4.4  5.3  1.5]\n",
      " [ 1.5  3.   2.3  1.9  0.5]\n",
      " [ 0.1  0.7  0.7  0.4 -0.4]\n",
      " [-1.  -0.4 -0.4 -0.6 -1.2]\n",
      " [-1.9 -1.3 -1.2 -1.4 -2. ]]\n"
     ]
    }
   ],
   "source": [
    "# Budujemy model świata Gridworld (A/B teleporty)\n",
    "P, nS, nA, nrow, ncol = build_gridworld_AB_P()\n",
    "\n",
    "gamma = 0.9  # dyskontowanie (jak w przykładach z książki)\n",
    "\n",
    "# Polityka równoprawdopodobna:\n",
    "# w każdym stanie każda z 4 akcji ma prawdopodobieństwo 1/4\n",
    "pi = np.ones((nS, nA), dtype=float) / nA\n",
    "\n",
    "# Budujemy model przejść i nagród \"widoczny\" dla tej polityki\n",
    "P_pi, r_pi = build_P_r_for_policy(P, pi)\n",
    "\n",
    "# Dokładnie liczymy funkcję wartości v_pi (układ równań Bellmana)\n",
    "v_pi = evaluate_policy_linear_system(P_pi, r_pi, gamma)\n",
    "\n",
    "# Wyświetlamy v_pi jako tabelę 5x5 (jak w książce)\n",
    "print(\"v_pi jako tabela 5x5 (zaokrąglenie):\")\n",
    "pretty_matrix_as_grid(v_pi, nrow, ncol, decimals=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba57833b",
   "metadata": {},
   "source": [
    "> **Notatki dla prowadzącego (B2):**\n",
    "> - To jest dobry moment na porównanie do książkowej tabeli: wartości powinny się zgadzać (po zaokrągleniu).\n",
    "> - Podkreśl: tu nie ma żadnej optymalizacji — to wartości dla z góry danej, losowej polityki.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a3671",
   "metadata": {},
   "source": [
    "## B3 — v* i polityka optymalna π* (podgląd)\n",
    "Teraz (tylko jako podgląd) policzymy $v_*$ oraz $\\pi_*$.\n",
    "\n",
    "**Uwaga:** algorytm (value iteration) omówimy formalnie w rozdziale 4 (programowanie dynamiczne).\n",
    "W tym rozdziale traktujemy go jako *narzędzie*, żeby zobaczyć różnicę między $v_\\pi$ i $v_*$.\n",
    "### Opis zadania\n",
    "W tym ćwiczeniu **po raz pierwszy rozwiązujemy problem optymalny**:\n",
    "nie oceniamy już danej polityki, lecz szukamy **najlepszych możliwych decyzji**\n",
    "w każdym stanie Gridworldu.\n",
    "\n",
    "Robimy to przy pomocy algorytmu *value iteration*, traktując go tutaj\n",
    "wyłącznie jako **narzędzie poglądowe**.\n",
    "\n",
    "Celem jest zobaczenie różnicy między:\n",
    "- wartością dla danej polityki (`v_π`, ćwiczenie B2),\n",
    "- wartością optymalną (`v*`) i polityką optymalną (`π*`).\n",
    "\n",
    "---\n",
    "\n",
    "### Notatka dla prowadzącego\n",
    "- Tu **pojawia się `max`** — to jest równanie optymalności Bellmana.\n",
    "- Warto jasno powiedzieć:\n",
    "  *to jest już inny problem niż policy evaluation*.\n",
    "- Jeśli padnie pytanie „czemu używamy DP już teraz”:\n",
    "  → odpowiedź: **żeby zobaczyć efekt**, a formalną teorię robimy w rozdziale 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "78f5e7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V* jako tabela 5x5 (zaokrąglenie):\n",
      "[[24.5 27.2 24.5 23.6 21.2 19.1]\n",
      " [22.  24.5 23.6 26.2 23.6 21.2]\n",
      " [21.2 23.6 26.2 29.1 26.2 23.6]\n",
      " [19.1 21.2 23.6 26.2 23.6 21.2]\n",
      " [17.2 19.1 21.2 23.6 21.2 19.1]\n",
      " [15.5 17.2 19.1 21.2 19.1 17.2]]\n",
      "\n",
      "Polityka optymalna (strzałki):\n",
      "→ ↑ ← ↓ ↓ ↓\n",
      "↑ ↑ → ↓ ↓ ↓\n",
      "→ → → ↑ ← ←\n",
      "↑ ↑ ↑ ↑ ↑ ↑\n",
      "↑ ↑ ↑ ↑ ↑ ↑\n",
      "↑ ↑ ↑ ↑ ↑ ↑\n"
     ]
    }
   ],
   "source": [
    "# B3 — v* i π* (podgląd różnicy względem v_pi)\n",
    "\n",
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-10, max_iter=100000):\n",
    "    # Value iteration: rozwiązujemy równanie optymalności Bellmana\n",
    "    V = np.zeros(nS, dtype=float)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        delta = 0.0\n",
    "        V_new = V.copy()\n",
    "\n",
    "        for s in range(nS):\n",
    "            best = None  # najlepsza wartość po akcjach\n",
    "\n",
    "            for a in range(nA):\n",
    "                outcomes = P[s][a]\n",
    "                if not outcomes:\n",
    "                    continue\n",
    "\n",
    "                # liczymy Q(s,a)\n",
    "                q = 0.0\n",
    "                for (p, s2, r, terminated) in outcomes:\n",
    "                    q += p * (r + gamma * (0.0 if terminated else V[int(s2)]))\n",
    "\n",
    "                # wybieramy maksimum (tu jest „greedy”)\n",
    "                if best is None or q > best:\n",
    "                    best = q\n",
    "\n",
    "            V_new[s] = 0.0 if best is None else best\n",
    "            delta = max(delta, abs(V_new[s] - V[s]))\n",
    "\n",
    "        V = V_new\n",
    "        if delta < tol:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "def greedy_policy_from_V(P, nS, nA, V, gamma=0.9):\n",
    "    # Odczytujemy π* jako argmax_a Q(s,a) względem V*\n",
    "    pi_det = np.zeros(nS, dtype=int)\n",
    "\n",
    "    for s in range(nS):\n",
    "        best_a = 0\n",
    "        best_q = None\n",
    "\n",
    "        for a in range(nA):\n",
    "            outcomes = P[s][a]\n",
    "            if not outcomes:\n",
    "                continue\n",
    "\n",
    "            q = 0.0\n",
    "            for (p, s2, r, terminated) in outcomes:\n",
    "                q += p * (r + gamma * (0.0 if terminated else V[int(s2)]))\n",
    "\n",
    "            if best_q is None or q > best_q:\n",
    "                best_q = q\n",
    "                best_a = a\n",
    "\n",
    "        pi_det[s] = best_a\n",
    "\n",
    "    return pi_det\n",
    "\n",
    "\n",
    "# budujemy Gridworld (A/B teleporty)\n",
    "P, nS, nA, nrow, ncol = build_gridworld_AB_P()\n",
    "gamma = 0.9\n",
    "\n",
    "# liczymy wartość optymalną v*\n",
    "V_star = value_iteration(P, nS, nA, gamma=gamma)\n",
    "\n",
    "# odczytujemy politykę optymalną π*\n",
    "pi_star = greedy_policy_from_V(P, nS, nA, V_star, gamma=gamma)\n",
    "\n",
    "# wizualizacja wyników\n",
    "print(\"V* jako tabela 5x5 (zaokrąglenie):\")\n",
    "pretty_matrix_as_grid(V_star, nrow, ncol, decimals=1)\n",
    "\n",
    "print(\"\\nPolityka optymalna (strzałki):\")\n",
    "action_arrows(pi_star, nrow, ncol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe5c70",
   "metadata": {},
   "source": [
    "> **Notatki dla prowadzącego (B3):**\n",
    "> - Warto jasno powiedzieć: tu wchodzi `max` (Bellman optimality). To jest inny problem niż B2.\n",
    "> - Jeśli ktoś pyta \"czemu używamy DP już teraz\": odpowiedź: jako podgląd różnicy, a teorię robimy w rozdz. 4.\n",
    "> - Tutaj nie oceniamy już konkretnej strategii, tylko rozwiązujemy problem decyzyjny: w każdym stanie wybieramy akcję, która maksymalizuje długoterminową wartość."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55a08f",
   "metadata": {},
   "source": [
    "### B2 vs B3 — o co chodzi w różnicy?\n",
    "\n",
    "- **B2 (policy evaluation)**: liczymy `v_π` dla *z góry zadanej* polityki `π`.  \n",
    "  W równaniu Bellmana pojawia się **średnia po akcjach** (bo polityka losuje akcje).\n",
    "\n",
    "- **B3 (optymalność)**: liczymy `v*` i `π*`, czyli najlepsze możliwe zachowanie.  \n",
    "  W równaniu Bellmana pojawia się **max po akcjach** (bo wybieramy najlepszą decyzję).\n",
    "\n",
    "Jedno zdanie intuicji:  \n",
    "> W B2 pytamy „jak dobra jest dana strategia?”, a w B3 pytamy „jaka strategia jest najlepsza?”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812cfad4",
   "metadata": {},
   "source": [
    "\n",
    "### Przejście do rozdziału 4 (Dynamic Programming)\n",
    "\n",
    "W B3 użyliśmy value iteration jako narzędzia, żeby zobaczyć efekt optymalności;  \n",
    "w rozdziale 4 pokażemy formalnie, **skąd bierze się ten algorytm, dlaczego działa i jak systematycznie znajduje `v*` oraz `π*`.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc96ff",
   "metadata": {},
   "source": [
    "### Ćwiczenia tablicowe (z książki): 3.14–3.16\n",
    "\n",
    "Na zajęciach (bez kodu) przerobimy wybrane zadania 3.14–3.16. W notebooku zostawiamy tylko krótką listę kontrolną:\n",
    "\n",
    "- (3.14) sprawdź/wyprowadź postać równań Bellmana w małym MDP,\n",
    "- (3.15) interpretacja $v_\\pi$ i wpływu $\\gamma$,\n",
    "- (3.16) różnica między $v_\\pi$ a $v_*$ oraz rola `max/argmax`.\n",
    "\n",
    "Rozwiązania: na tablicy oraz w materiałach prowadzącego.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0262eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca032262",
   "metadata": {},
   "source": [
    "## C. (Opcjonalnie) FrozenLake jako MDP\n",
    "\n",
    "FrozenLake będzie naszym środowiskiem bazowym w kolejnych rozdziałach.\n",
    "Tu tylko pokazujemy, że:\n",
    "- FrozenLake udostępnia model przejść `env.unwrapped.P` w dokładnie tym samym formacie,\n",
    "- więc możemy policzyć $v_\\pi$ \"modelowo\" (gdy znamy `P`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8077aaeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenLake 4x4: v_pi (zawsze RIGHT), jako siatka:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "Przykład P[0][RIGHT]: [(1.0, 4, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "# Uwaga: ta sekcja jest opcjonalna. Jeśli nie masz gymnasium, pomiń.\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except Exception as e:\n",
    "    print(\"Brak gymnasium:\", e)\n",
    "    gym = None\n",
    "\n",
    "if gym is not None:\n",
    "    env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "    P = env.unwrapped.P\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    gamma = 0.9\n",
    "\n",
    "    # Przykładowa polityka: zawsze w prawo (RIGHT=1)\n",
    "    pi = np.zeros((nS, nA))\n",
    "    pi[:, 1] = 1.0\n",
    "\n",
    "    P_pi, r_pi = build_P_r_for_policy(P, pi)\n",
    "    v = evaluate_policy_linear_system(P_pi, r_pi, gamma)\n",
    "\n",
    "    print(\"FrozenLake 4x4: v_pi (zawsze RIGHT), jako siatka:\")\n",
    "    pretty_matrix_as_grid(v, 4, 4, decimals=2)\n",
    "\n",
    "    # Podgląd przejść dla stanu 0 i akcji RIGHT\n",
    "    print(\"\\nPrzykład P[0][RIGHT]:\", P[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a0ec766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 4, 0.0, False)],\n",
       "  2: [(1.0, 1, 0.0, False)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 1: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 5, 0.0, True)],\n",
       "  2: [(1.0, 2, 0.0, False)],\n",
       "  3: [(1.0, 1, 0.0, False)]},\n",
       " 2: {0: [(1.0, 1, 0.0, False)],\n",
       "  1: [(1.0, 6, 0.0, False)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 3: {0: [(1.0, 2, 0.0, False)],\n",
       "  1: [(1.0, 7, 0.0, True)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 3, 0.0, False)]},\n",
       " 4: {0: [(1.0, 4, 0.0, False)],\n",
       "  1: [(1.0, 8, 0.0, False)],\n",
       "  2: [(1.0, 5, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(1.0, 5, 0.0, True)],\n",
       "  1: [(1.0, 10, 0.0, False)],\n",
       "  2: [(1.0, 7, 0.0, True)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 12, 0.0, True)],\n",
       "  2: [(1.0, 9, 0.0, False)],\n",
       "  3: [(1.0, 4, 0.0, False)]},\n",
       " 9: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 10, 0.0, False)],\n",
       "  3: [(1.0, 5, 0.0, True)]},\n",
       " 10: {0: [(1.0, 9, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 11, 0.0, True)],\n",
       "  3: [(1.0, 6, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(1.0, 12, 0.0, True)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 14, 0.0, False)],\n",
       "  3: [(1.0, 9, 0.0, False)]},\n",
       " 14: {0: [(1.0, 13, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 15, 1.0, True)],\n",
       "  3: [(1.0, 10, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a61835",
   "metadata": {},
   "source": [
    "> **Notatki dla prowadzącego (C):**\n",
    "> - Ta sekcja ma być krótka: tylko pokazuje, że Gym daje dostęp do `P`.\n",
    "> - Na kolejnych zajęciach wrócimy do FrozenLake, gdy zaczniemy DP i RL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f6be2",
   "metadata": {},
   "source": [
    "## D. (Opcjonalnie) Pole balancing (CartPole) jako MDP\n",
    "\n",
    "CartPole to też MDP, ale stan jest **ciągły** (wektor 4 liczb), więc tablicowe metody z tego rozdziału nie skalują.\n",
    "To dobry most do późniejszych tematów (aproksymacja funkcji / deep RL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d303e7",
   "metadata": {},
   "source": [
    "Ten fragment nie jest ćwiczeniem — to tylko podgląd, jak wygląda MDP z ciągłym stanem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ce73e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation (stan) = [ 0.01369617 -0.02302133 -0.04590265 -0.04834723]\n",
      "Observation space = Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action space = Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# Opcjonalny podgląd: przestrzenie stanu i akcji w CartPole\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except Exception as e:\n",
    "    print(\"Brak gymnasium:\", e)\n",
    "    gym = None\n",
    "\n",
    "if gym is not None:\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    obs, info = env.reset(seed=0)\n",
    "    print(\"Observation (stan) =\", obs)\n",
    "    print(\"Observation space =\", env.observation_space)\n",
    "    print(\"Action space =\", env.action_space)\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972782a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fb4c830",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "- Umiesz zakodować model MDP jako `P[s][a]`.\n",
    "- Umiesz policzyć $v_\\pi$ dokładnie przez rozwiązanie układu liniowego.\n",
    "- Widzisz różnicę: **ewaluacja** ($v_\\pi$) vs **optymalność** ($v_*$, $\\pi_*$).\n",
    "- W kolejnych zajęciach (rozdz. 4) pokażemy, jak systematycznie liczyć $v_*$ i $\\pi_*$ metodami DP.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
