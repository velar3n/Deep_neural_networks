{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505590aa",
   "metadata": {},
   "source": [
    "Autor: Natalia Kiełbasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e57bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eff914",
   "metadata": {},
   "source": [
    "## Opis projektu - Sweep parametrów środowiska w części A1.4:\n",
    "\n",
    "### Co już mamy w pliku z rodziałem 3:\n",
    "Tutaj mamy bazując na A1.3 i A1.2 wyliczenie optymalnych polityk w zależności od zmieniających się parametrów środowiska. Jest to świetny przykład tego, że w trakcie jak zmienia nam się nasze otoczenie/środowisko, to nasza polityka (sposób działania) może przestać być optymalna. Tutaj (A1.4) mamy na bardzo prostym przykładzie - naszym modelu odkurzacza, który mam nadzieję już w miarę wszyscy rozumiemy, pokazane jak to się zmienia. \n",
    "W aktualnym przykładzie zmieniamy tylko beta dla 6 wartości oraz rescue_cost dla 4 wartości:\n",
    "\n",
    "- beta_list = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "- rescue_list = [-1.0, -3.0, -6.0, -10.0]\n",
    "\n",
    "Następnie iterujemy po nich i sprawdzamy dla każdej kombinacji optymalną politykę:\n",
    "\n",
    "- Ustawienia stałe: alpha= 0.8 r_search= 5.0 r_wait= 1.0 gamma= 0.9 \n",
    "- Obliczenia: beta rescue_cost pi(H) pi(L): 0.1 -1.0 SEARCH RECHARGE0.1 -3.0 SEARCH RECHARGE0.1 -6.0 SEARCH RECHARGE0.1 -10.0 SEARCH RECHARGE0.3 -1.0 SEARCH RECHARGE0.3 -3.0 SEARCH RECHARGE0.3 -6.0 SEARCH RECHARGE0.3 -10.0 SEARCH RECHARGE0.5 -1.0 SEARCH SEARCH 0.5 -3.0 SEARCH RECHARGE0.5 -6.0 SEARCH RECHARGE0.5 -10.0 SEARCH RECHARGE0.7 -1.0 SEARCH SEARCH 0.7 -3.0 SEARCH RECHARGE0.7 -6.0 SEARCH RECHARGE0.7 -10.0 SEARCH RECHARGE0.9 -1.0 SEARCH SEARCH 0.9 -3.0 SEARCH SEARCH 0.9 -6.0 SEARCH SEARCH 0.9 -10.0 SEARCH SEARCH Wskazówka: obserwuj, kiedy π*(L) zmienia się z SEARCH na RECHARGE.\n",
    "\n",
    "Widać na przykład, że nasz najprostszy działający i logiczny model - rozładowane to ładujemy, naładowane to odkurzamy nie zawsze jest optymalny. Da się to pewnie wytłumaczyć, np. skrajne sytuacje takie jak skrajnie zużyta bateria i szybko rozładowująca się (znamy z laptopów :).\n",
    "\n",
    "### Zadanie:\n",
    "W zadaniu chodzi o to, aby w tak prostym przykładzie pobawić się parametrami środowiska (alpha, beta, rescue_cost, gamma, r_wait, r_search) i zobaczyć jak nimi manipulując możemy uzyskiwać różne kombinacje optymanych polityk.\n",
    "1. Wykonać ręcznie w zadaniu A1.2 ewaluację kilku ręcznie wstawionych polityk i zobaczyć, które będą najlepsze. Pogrubione możemy zmieniać  sobie na inne możliwe akcje, aby zobaczyć co daje najlepszy wynik.\n",
    "- Polityka 1: w H -> SEARCH, w L -> RECHARGE\n",
    "- pi1 = np.zeros((nS, nA))\n",
    "- pi1[H, SEARCH] = 1.0\n",
    "- pi1[L, RECHARGE] = 1.0\n",
    "2. Następnie można przejść do A1.3, gdzie jest główny algorytm znajdujący optymalną politykę przy zadanych parametrach.\n",
    "3. (główna część zadania) W części A1.4 wykonujemy \"sweepy\" po różnych zestawach parametrów środowiska, oczywiście nie po wszystkich, tylklo proszę wybrać kilka i spróbować ocenić z czego to wynika/zinterepretować.\n",
    "4. Ważne! Trzeba jasno napisać:\n",
    "- co jest częścią środowiska\n",
    "- co jest częścią agenta\n",
    "- jaką mamy politykę\n",
    "- jak wyliczamy funkcję wartości V_{pi}\n",
    "- jak ewaluujemy politykę"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cafb60f",
   "metadata": {},
   "source": [
    "### A1.2 - ewaluacja własnych polityk\n",
    "\n",
    "- polityka: czyli reguła która mówi agentowi jakie działania może podjąć w danym stanie\n",
    "- $v_\\pi$: wartość oczekiwanej nagrody w danym stanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6bfb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_linear_system(P_pi: np.ndarray, r_pi: np.ndarray, gamma: float) -> np.ndarray:\n",
    "    \"\"\"Rozwiązuje równanie Bellmana dla danej polityki w postaci macierzowej.\n",
    "\n",
    "    Dla polityki pi mamy:\n",
    "        v = r_pi + gamma * P_pi * v\n",
    "    czyli:\n",
    "        (I - gamma * P_pi) v = r_pi\n",
    "\n",
    "    Zwraca wektor v (shape: [nS]).\n",
    "    \"\"\"\n",
    "    nS = P_pi.shape[0]\n",
    "    I = np.eye(nS)\n",
    "    return np.linalg.solve(I - gamma * P_pi, r_pi)\n",
    "\n",
    "def pretty_matrix_as_grid(v: np.ndarray, nrow: int, ncol: int, decimals: int = 1):\n",
    "    \"\"\"Pomocniczo: wyświetl wektor wartości jako siatkę (nrow x ncol).\"\"\"\n",
    "    grid = v.reshape(nrow, ncol)\n",
    "    with np.printoptions(precision=decimals, suppress=True):\n",
    "        print(grid)\n",
    "\n",
    "def action_arrows(pi_det: np.ndarray, nrow: int, ncol: int):\n",
    "    \"\"\"Zamienia deterministyczną politykę (akcja w każdym stanie) na strzałki w siatce.\"\"\"\n",
    "    arrows = {0:'↑', 1:'→', 2:'↓', 3:'←', None:'·'}\n",
    "    out = []\n",
    "    for r in range(nrow):\n",
    "        row = []\n",
    "        for c in range(ncol):\n",
    "            s = r*ncol + c\n",
    "            a = int(pi_det[s]) if pi_det[s] is not None else None\n",
    "            row.append(arrows.get(a, '?'))\n",
    "        out.append(' '.join(row))\n",
    "    print('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63009c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budowa P_pi oraz r_pi (rozwiązanie)\n",
    "def build_P_r_for_policy(P, pi):\n",
    "    \"\"\"\n",
    "    Buduje (P_pi, r_pi) dla zadanej polityki π.\n",
    "\n",
    "    Wejście:\n",
    "    - P: model środowiska w formacie\n",
    "         P[s][a] -> lista (p, s2, r, terminated)\n",
    "    - pi: polityka stochastyczna w postaci macierzy (nS, nA),\n",
    "          pi[s, a] = P(A_t = a | S_t = s)\n",
    "\n",
    "    Wyjście:\n",
    "    - P_pi: macierz przejść dla polityki π (nS x nS)\n",
    "    - r_pi: wektor nagród oczekiwanych dla polityki π (nS,)\n",
    "\n",
    "    Sens:\n",
    "    Z ogólnego MDP (P[s][a]) robimy „świat widziany przez politykę π”,\n",
    "    potrzebny do rozwiązania równania:\n",
    "        v = r_pi + γ P_pi v\n",
    "    \"\"\"\n",
    "    nS = len(P)              # liczba stanów\n",
    "    nA = len(P[0])           # liczba akcji (z pierwszego stanu)\n",
    "\n",
    "    # Macierz przejść i wektor nagród dla polityki π\n",
    "    P_pi = np.zeros((nS, nS), dtype=float)\n",
    "    r_pi = np.zeros(nS, dtype=float)\n",
    "\n",
    "    # Iterujemy po wszystkich stanach\n",
    "    for s in range(nS):\n",
    "\n",
    "        # Iterujemy po wszystkich akcjach\n",
    "        for a in range(nA):\n",
    "\n",
    "            # Waga akcji a w stanie s wg polityki π\n",
    "            w = float(pi[s, a])\n",
    "\n",
    "            # Jeśli polityka nigdy nie wybiera tej akcji, pomijamy ją\n",
    "            if w == 0.0:\n",
    "                continue\n",
    "\n",
    "            outcomes = P[s][a]\n",
    "\n",
    "            # Jeśli akcja jest niedostępna (pusta lista), pomijamy\n",
    "            if not outcomes:\n",
    "                continue\n",
    "\n",
    "            # Iterujemy po wszystkich możliwych skutkach akcji a w stanie s\n",
    "            for (p, s2, r, terminated) in outcomes:\n",
    "\n",
    "                # Składnik nagrody oczekiwanej:\n",
    "                # r_pi[s] = E_π[R_{t+1} | S_t = s]\n",
    "                r_pi[s] += w * p * float(r)\n",
    "\n",
    "                # Składnik przejść:\n",
    "                # P_pi[s, s2] = P(S_{t+1} = s2 | S_t = s, π)\n",
    "                #\n",
    "                # Jeśli przejście jest terminalne:\n",
    "                # - V(s2) = 0 w równaniu Bellmana,\n",
    "                # - więc NIE dodajemy go do macierzy P_pi\n",
    "                if not terminated:\n",
    "                    P_pi[s, int(s2)] += w * p\n",
    "\n",
    "    return P_pi, r_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4acb98d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1.1 tests passed ✅\n"
     ]
    }
   ],
   "source": [
    "# Robot zdefiniowany na zajęciach. Ma 2 możliwe stany i może wykonać 3 akcje\n",
    "def build_recycling_robot_P(alpha=0.8, beta=0.4, r_search=5.0, r_wait=1.0, rescue_cost=-3.0):\n",
    "    \"\"\"Return (P, nS, nA) for Recycling Robot.\n",
    "\n",
    "    States: 0=H, 1=L\n",
    "    Actions: 0=SEARCH, 1=WAIT, 2=RECHARGE (only in L)\n",
    "\n",
    "    P[s][a] -> list (p, s2, r, terminated)\n",
    "    \"\"\"\n",
    "    # 1) Rozmiar MDP: 2 stany (H,L) i 3 akcje (SEARCH, WAIT, RECHARGE)\n",
    "    nS, nA = 2, 3\n",
    "    # 2) Inicjalizacja pustej struktury przejść\n",
    "    #    P[s][a] = [] oznacza brak zdefiniowanych przejść (np. akcja niedostępna)\n",
    "    P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "    # 3) Aliasowanie indeksów dla czytelności\n",
    "\n",
    "    H, L = 0, 1\n",
    "    SEARCH, WAIT, RECHARGE = 0, 1, 2\n",
    "    # -----------------------------\n",
    "    # Stan H (wysoka energia)\n",
    "    # -----------------------------\n",
    "    # SEARCH w H: nagroda r_search; przejście do H z p=alpha, do L z p=1-alpha\n",
    "    \n",
    "    # High energy (H)\n",
    "    P[H][SEARCH] = [\n",
    "        (alpha, H, r_search, False),\n",
    "        (1 - alpha, L, r_search, False),\n",
    "    ]\n",
    "    # WAIT w H: zawsze zostajemy w H; nagroda r_wait\n",
    "    P[H][WAIT] = [\n",
    "        (1.0, H, r_wait, False),\n",
    "    ]\n",
    "    # RECHARGE w H: akcja niedostępna w tym stanie\n",
    "    P[H][RECHARGE] = []  # not available in H\n",
    "\n",
    "    # -----------------------------\n",
    "    # Stan L (niska energia)\n",
    "    # -----------------------------\n",
    "    # SEARCH w L:\n",
    "    #   - z p=beta: zostajemy w L i dostajemy r_search\n",
    "    #   - z p=1-beta: rozładowanie -> powrót do H i kara rescue_cost\n",
    "    # Low energy (L)\n",
    "    P[L][SEARCH] = [\n",
    "        (beta, L, r_search, False),\n",
    "        (1 - beta, H, rescue_cost, False),\n",
    "    ]\n",
    "    # WAIT w L: zawsze zostajemy w L; nagroda r_wait\n",
    "    P[L][WAIT] = [\n",
    "        (1.0, L, r_wait, False),\n",
    "    ]\n",
    "    # RECHARGE w L: zawsze przechodzimy do H; nagroda 0\n",
    "    P[L][RECHARGE] = [\n",
    "        (1.0, H, 0.0, False),\n",
    "    ]\n",
    "\n",
    "    return P, nS, nA\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Testy poprawności modelu\n",
    "# --------------------------------------------------\n",
    "\n",
    "# --- testy ---\n",
    "P, nS, nA = build_recycling_robot_P(alpha=0.8, beta=0.4, r_search=5.0, r_wait=1.0, rescue_cost=-3.0)\n",
    "\n",
    "assert nS == 2 and nA == 3\n",
    "assert abs(sum(p for p, *_ in P[0][0]) - 1.0) < 1e-12  # H, SEARCH\n",
    "assert abs(sum(p for p, *_ in P[0][1]) - 1.0) < 1e-12  # H, WAIT\n",
    "assert P[0][2] == []                                  # H, RECHARGE not allowed\n",
    "assert abs(sum(p for p, *_ in P[1][0]) - 1.0) < 1e-12  # L, SEARCH\n",
    "assert abs(sum(p for p, *_ in P[1][1]) - 1.0) < 1e-12  # L, WAIT\n",
    "assert abs(sum(p for p, *_ in P[1][2]) - 1.0) < 1e-12  # L, RECHARGE\n",
    "\n",
    "print(\"A1.1 tests passed ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33188a0d",
   "metadata": {},
   "source": [
    "Ręczne wypisanie przykładowych polityk i ich ewaluacja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721249c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_pi1(H), v_pi1(L) = [42.3729 38.1356]\n",
      "v_pi2(H), v_pi2(L) = [0.     0.3125]\n",
      "v_pi3(H), v_pi3(L) = [10.  9.]\n",
      "v_pi4(H), v_pi4(L) = [39.4634 33.6098]\n",
      "v_pi5(H), v_pi5(L) = [24.2857 10.    ]\n",
      "v_pi6(H), v_pi6(L) = [10.    8.75]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "P, nS, nA = build_recycling_robot_P()\n",
    "\n",
    "H, L = 0, 1\n",
    "SEARCH, WAIT, RECHARGE = 0, 1, 2\n",
    "\n",
    "\n",
    "# POLITYKI PODANE NA ZAJĘCIACH\n",
    "# Polityka 1: w H -> SEARCH, w L -> RECHARGE\n",
    "pi1 = np.zeros((nS, nA))\n",
    "pi1[H, SEARCH] = 1.0\n",
    "pi1[L, RECHARGE] = 1.0\n",
    "\n",
    "# Polityka 2: w H -> WAIT, w L -> WAIT\n",
    "pi2 = np.zeros((nS, nA))\n",
    "pi2[H, RECHARGE] = 1.0\n",
    "pi2[L, SEARCH] = 1.0\n",
    "\n",
    "\n",
    "P_pi1, r_pi1 = build_P_r_for_policy(P, pi1)\n",
    "v1 = evaluate_policy_linear_system(P_pi1, r_pi1, gamma)\n",
    "\n",
    "P_pi2, r_pi2 = build_P_r_for_policy(P, pi2)\n",
    "v2 = evaluate_policy_linear_system(P_pi2, r_pi2, gamma)\n",
    "\n",
    "print(\"v_pi1(H), v_pi1(L) =\", np.round(v1, 4))\n",
    "print(\"v_pi2(H), v_pi2(L) =\", np.round(v2, 4))\n",
    "\n",
    "\n",
    "# WŁASNE PRZYKŁADOWE POLITYKI\n",
    "# Polityka 3:\n",
    "pi3 = np.zeros((nS, nA))\n",
    "pi3[H, WAIT] = 1.0\n",
    "pi3[L, RECHARGE] = 1.0\n",
    "\n",
    "# Polityka 4:\n",
    "pi4 = np.zeros((nS, nA))\n",
    "pi4[H, SEARCH] = 1.0\n",
    "pi4[L, SEARCH] = 1.0\n",
    "\n",
    "# Polityka 5:\n",
    "pi5 = np.zeros((nS, nA))\n",
    "pi5[H, SEARCH] = 1.0\n",
    "pi5[L, WAIT] = 1.0\n",
    "\n",
    "# Polityka 6:\n",
    "pi6 = np.zeros((nS, nA))\n",
    "pi6[H, WAIT] = 1.0\n",
    "pi6[L, SEARCH] = 1.0\n",
    "\n",
    "\n",
    "P_pi3, r_pi3 = build_P_r_for_policy(P, pi3)\n",
    "v3 = evaluate_policy_linear_system(P_pi3, r_pi3, gamma)\n",
    "\n",
    "P_pi4, r_pi4 = build_P_r_for_policy(P, pi4)\n",
    "v4 = evaluate_policy_linear_system(P_pi4, r_pi4, gamma)\n",
    "\n",
    "P_pi5, r_pi5 = build_P_r_for_policy(P, pi5)\n",
    "v5 = evaluate_policy_linear_system(P_pi5, r_pi5, gamma)\n",
    "\n",
    "P_pi6, r_pi6 = build_P_r_for_policy(P, pi6)\n",
    "v6 = evaluate_policy_linear_system(P_pi6, r_pi6, gamma)\n",
    "\n",
    "print(\"v_pi3(H), v_pi3(L) =\", np.round(v3, 4))\n",
    "print(\"v_pi4(H), v_pi4(L) =\", np.round(v4, 4))\n",
    "print(\"v_pi5(H), v_pi5(L) =\", np.round(v5, 4))\n",
    "print(\"v_pi6(H), v_pi6(L) =\", np.round(v6, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbcda2",
   "metadata": {},
   "source": [
    "### Wnioski z tej ewaluacji:\n",
    "- polityka 1 daje największą oczekiwaną sumę nagród\n",
    "- z własnych polityk polityka 4 też dała wysoką sumę nagród. Była to politykka która zakładała szukanie w każdym stanie (bardzo agresywna)\n",
    "- ogólnie bardzo bezpieczne polityki, które głównie czekają mają kiepskie wyniki, co ma sens patrząc na to że nie prowadzą nas do przodu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01661a3a",
   "metadata": {},
   "source": [
    "### A1.3: przenosimy tu funkcje z oryginalnego zadania z ćwiczeń"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "370d5aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Najlepsza polityka (wg v(H)) ma v(H) = 42.3729\n",
      "pi*(H) = SEARCH\n",
      "pi*(L) = RECHARGE\n"
     ]
    }
   ],
   "source": [
    "def all_deterministic_policies_robot():\n",
    "    \"\"\"\n",
    "    Generuje wszystkie deterministyczne polityki dla robota recyklingowego.\n",
    "\n",
    "    Ponieważ:\n",
    "    - w stanie H dostępne są 2 sensowne akcje (SEARCH, WAIT),\n",
    "    - w stanie L dostępne są 3 akcje (SEARCH, WAIT, RECHARGE),\n",
    "\n",
    "    liczba wszystkich deterministycznych polityk wynosi 2 * 3 = 6.\n",
    "\n",
    "    Każda polityka pi jest macierzą:\n",
    "        pi[s, a] = 1  -> akcja a jest zawsze wybierana w stanie s\n",
    "        pi[s, a] = 0  -> akcja a nigdy nie jest wybierana w stanie s\n",
    "    \"\"\"\n",
    "    # Indeksy stanów\n",
    "    H, L = 0, 1\n",
    "\n",
    "    # Indeksy akcji\n",
    "    SEARCH, WAIT, RECHARGE = 0, 1, 2\n",
    "\n",
    "    policies = []\n",
    "\n",
    "    # Wybieramy wszystkie możliwe akcje w stanie H\n",
    "    for aH in [SEARCH, WAIT]:\n",
    "\n",
    "        # Wybieramy wszystkie możliwe akcje w stanie L\n",
    "        for aL in [SEARCH, WAIT, RECHARGE]:\n",
    "\n",
    "            # Tworzymy pustą politykę (2 stany x 3 akcje)\n",
    "            pi = np.zeros((2, 3))\n",
    "\n",
    "            # Polityka deterministyczna:\n",
    "            # w stanie H zawsze wybieramy akcję aH\n",
    "            pi[H, aH] = 1.0\n",
    "\n",
    "            # w stanie L zawsze wybieramy akcję aL\n",
    "            pi[L, aL] = 1.0\n",
    "\n",
    "            # Dodajemy gotową politykę do listy\n",
    "            policies.append(pi)\n",
    "\n",
    "    return policies\n",
    "\n",
    "\n",
    "def best_policy_robot(P, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Wybiera najlepszą politykę spośród wszystkich deterministycznych polityk.\n",
    "\n",
    "    Kryterium:\n",
    "    - maksymalizujemy wartość v_pi(H),\n",
    "      czyli oczekiwany zdyskontowany zwrot,\n",
    "      jeśli startujemy w stanie H i stosujemy politykę pi.\n",
    "\n",
    "    Uwaga dydaktyczna:\n",
    "    - to jest brute force (sprawdzamy wszystkie polityki),\n",
    "    - działa tylko dlatego, że MDP jest bardzo małe.\n",
    "    \"\"\"\n",
    "    best_pi = None    # najlepsza znaleziona polityka\n",
    "    best_vH = None    # jej wartość v_pi(H)\n",
    "\n",
    "    # Iterujemy po wszystkich możliwych politykach\n",
    "    for pi in all_deterministic_policies_robot():\n",
    "\n",
    "        # Budujemy model świata \"widoczny\" dla danej polityki:\n",
    "        # P_pi   — macierz przejść dla tej polityki\n",
    "        # r_pi   — wektor nagród dla tej polityki\n",
    "        P_pi, r_pi = build_P_r_for_policy(P, pi)\n",
    "\n",
    "        # Liczymy dokładnie funkcję wartości v_pi,\n",
    "        # rozwiązując układ równań Bellmana:\n",
    "        # v = r_pi + gamma * P_pi * v\n",
    "        v = evaluate_policy_linear_system(P_pi, r_pi, gamma)\n",
    "\n",
    "        # Interesuje nas wartość w stanie H\n",
    "        vH = float(v[0])\n",
    "\n",
    "        # Sprawdzamy, czy ta polityka jest lepsza od dotychczasowej\n",
    "        if best_vH is None or vH > best_vH:\n",
    "            best_vH = vH\n",
    "            best_pi = pi\n",
    "\n",
    "    return best_pi, best_vH\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Uruchomienie: znalezienie polityki optymalnej\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Budujemy model świata robota\n",
    "P, nS, nA = build_recycling_robot_P()\n",
    "\n",
    "# Szukamy najlepszej polityki (brute force)\n",
    "pi_star, vH_star = best_policy_robot(P, gamma=0.9)\n",
    "\n",
    "# Mapowanie indeksów akcji na czytelne nazwy\n",
    "action_name = {0: \"SEARCH\", 1: \"WAIT\", 2: \"RECHARGE\"}\n",
    "\n",
    "print(\"Najlepsza polityka (wg v(H)) ma v(H) =\", round(vH_star, 4))\n",
    "print(\"pi*(H) =\", action_name[int(np.argmax(pi_star[0]))])\n",
    "print(\"pi*(L) =\", action_name[int(np.argmax(pi_star[1]))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b2132",
   "metadata": {},
   "source": [
    "Czyli wcześniejsza polityka 1 faktycznie była najlepsza z deterministycznych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca04e843",
   "metadata": {},
   "source": [
    "### A1.4: wykonujemy sweepy po różnych zwestawach parametrów\n",
    "\n",
    "Zmieniamy parametray środowiska (alpha, beta, rescue_cost, gamma, r_wait, r_search) i patrzymy jak nimi manipulując możemy uzyskiwać różne kombinacje optymanych polityk. Potem próbujemy ocenić z czego to wynika/zinterepretować.\n",
    "\n",
    "Uwzględniamy:\n",
    "- co jest częścią środowiska\n",
    "- co jest częścią agenta\n",
    "- jaką mamy politykę\n",
    "- jak wyliczamy funkcję wartości V_{pi}\n",
    "- jak ewaluujemy politykę\n",
    "\n",
    "1. Co jest częścią środowiska:  wszystko, co działa niezależnie od decyzji agenta\n",
    "    - alpha - prawdopodobieństwo, że SEARCH w stanie H zakończy się sukcesem\n",
    "    - beta - prawdopodobieństwo, że SEARCH w stanie L zakończy się sukcesem\n",
    "    - rescue_cost - kara za przejście do stanu utknięcia\n",
    "    - r_search, r_wait - nagrody za akcje\n",
    "\n",
    "2. Co jest częścią agenta: \n",
    "    - gamma - współczynnik dyskontowania, czyli jak bardzo agent \"ceni\" przyszłość vs teraźniejszość (to jego perspektywa czasowa)\n",
    "    - polityka - strategia wyboru akcji (patrzymy jak różne parametry mają wpływ na optymalne polityki)\n",
    "\n",
    "3. Polityka jest wybierana przez best_policy_robot() (zwraca najlepszą politykę dla danych parametrów)\n",
    "\n",
    "4. Funkcja wartości jest obliczana przez best_policy_robot()\n",
    "\n",
    "5. Politykę ewaluujemy jak napisano w zadaniu: maksymalizując wartość v_pi(H), czyli oczekiwany zdyskontowany zwrot, jeśli startujemy w stanie H i stosujemy politykę pi. Jest to jest brute force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9871939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1.4 — Sweep parametrów: kiedy zmienia się π*(L)\n",
    "action_name = {0: \"SEARCH\", 1: \"WAIT\", 2: \"RECHARGE\"}  # mapowanie akcji\n",
    "\n",
    "# Wartości domyślne zgodne z build_recycling_robot_P() i best_policy_robot()\n",
    "alpha_default = 0.8\n",
    "beta_default = 0.4\n",
    "r_search_default = 5.0\n",
    "r_wait_default = 1.0\n",
    "rescue_cost_default = -3.0\n",
    "gamma_default = 0.9\n",
    "\n",
    "# Wartości eksperymentalne\n",
    "alpha_list = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "beta_list = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "r_search_list = [0.5, 2.0, 5.0, 10.0]\n",
    "r_wait_list= [0.0, 1.0, 3.0, 5.0]\n",
    "rescue_list = [-1.0, -3.0, -6.0, -10.0, -20.0]\n",
    "gamma_list = [0.2, 0.5, 0.7, 0.85, 0.9, 0.95, 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a7bd825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustawienia stałe: alpha= 0.8 r_search= 5.0 r_wait= 1.0 gamma= 0.9\n",
      "beta   rescue_cost   pi*(H)      pi*(L)\n",
      "----   ----------    --------    --------\n",
      "0.1           -1.0    SEARCH     RECHARGE\n",
      "0.1           -3.0    SEARCH     RECHARGE\n",
      "0.1           -6.0    SEARCH     RECHARGE\n",
      "0.1          -10.0    SEARCH     RECHARGE\n",
      "0.1          -20.0    SEARCH     RECHARGE\n",
      "0.3           -1.0    SEARCH     RECHARGE\n",
      "0.3           -3.0    SEARCH     RECHARGE\n",
      "0.3           -6.0    SEARCH     RECHARGE\n",
      "0.3          -10.0    SEARCH     RECHARGE\n",
      "0.3          -20.0    SEARCH     RECHARGE\n",
      "0.5           -1.0    SEARCH     SEARCH  \n",
      "0.5           -3.0    SEARCH     RECHARGE\n",
      "0.5           -6.0    SEARCH     RECHARGE\n",
      "0.5          -10.0    SEARCH     RECHARGE\n",
      "0.5          -20.0    SEARCH     RECHARGE\n",
      "0.7           -1.0    SEARCH     SEARCH  \n",
      "0.7           -3.0    SEARCH     RECHARGE\n",
      "0.7           -6.0    SEARCH     RECHARGE\n",
      "0.7          -10.0    SEARCH     RECHARGE\n",
      "0.7          -20.0    SEARCH     RECHARGE\n",
      "0.9           -1.0    SEARCH     SEARCH  \n",
      "0.9           -3.0    SEARCH     SEARCH  \n",
      "0.9           -6.0    SEARCH     SEARCH  \n",
      "0.9          -10.0    SEARCH     SEARCH  \n",
      "0.9          -20.0    SEARCH     RECHARGE\n"
     ]
    }
   ],
   "source": [
    "# EKSPERYMENT 1: beta vs rescue_cost, czyli to co było na zajęciach\n",
    "\n",
    "print(\"Ustawienia stałe:\", \"alpha=\", alpha_default, \"r_search=\", r_search_default,\n",
    "      \"r_wait=\", r_wait_default, \"gamma=\", gamma_default)\n",
    "print(\"beta   rescue_cost   pi*(H)      pi*(L)\")\n",
    "print(\"----   ----------    --------    --------\")\n",
    "\n",
    "# Dla każdej pary (beta, rescue_cost) sprawdzamy, jaka polityka jest optymalna\n",
    "for beta in beta_list:\n",
    "    for rescue_cost in rescue_list:\n",
    "\n",
    "        # budujemy świat (MDP) z tymi parametrami\n",
    "        P, _, _ = build_recycling_robot_P(beta=beta, rescue_cost=rescue_cost)\n",
    "\n",
    "        # znajdujemy politykę optymalną π* (brute force)\n",
    "        pi_star, _ = best_policy_robot(P)\n",
    "\n",
    "        # odczytujemy decyzje w H i L\n",
    "        aH = action_name[int(np.argmax(pi_star[0]))]\n",
    "        aL = action_name[int(np.argmax(pi_star[1]))]\n",
    "\n",
    "        # wypisujemy wynik\n",
    "        print(f\"{beta:0.1f}     {rescue_cost:>10.1f}    {aH:<8}   {aL:<8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06881e84",
   "metadata": {},
   "source": [
    "### Obserwacje:\n",
    "- polityka dla H nie zmienia się (zawsze SEARCH), więc wydaje się stabilna\n",
    "- przy niskim beta dla L agent nie wybiera search co ma sens patrząć na to że czym mniejsze beta, tym mniejsza szansa, że search w L zakończy się sukcesem\n",
    "- tak samo dla zwiększanie ujemnego rescue_cost powoduje przejście w L z search do recharge, wysoka kara za awarię zniechęca do bartdziej ryzykownego search w stanie L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdf2baf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustawienia stałe: beta= 0.4 r_search= 5.0 r_wait= 1.0 gamma= 0.9\n",
      "alpha   rescue_cost   pi*(H)      pi*(L)\n",
      "----   ----------    --------    --------\n",
      "0.1           -1.0    SEARCH     SEARCH  \n",
      "0.1           -3.0    SEARCH     RECHARGE\n",
      "0.1           -6.0    SEARCH     RECHARGE\n",
      "0.1          -10.0    SEARCH     RECHARGE\n",
      "0.1          -20.0    SEARCH     RECHARGE\n",
      "0.3           -1.0    SEARCH     SEARCH  \n",
      "0.3           -3.0    SEARCH     RECHARGE\n",
      "0.3           -6.0    SEARCH     RECHARGE\n",
      "0.3          -10.0    SEARCH     RECHARGE\n",
      "0.3          -20.0    SEARCH     RECHARGE\n",
      "0.5           -1.0    SEARCH     SEARCH  \n",
      "0.5           -3.0    SEARCH     RECHARGE\n",
      "0.5           -6.0    SEARCH     RECHARGE\n",
      "0.5          -10.0    SEARCH     RECHARGE\n",
      "0.5          -20.0    SEARCH     RECHARGE\n",
      "0.7           -1.0    SEARCH     RECHARGE\n",
      "0.7           -3.0    SEARCH     RECHARGE\n",
      "0.7           -6.0    SEARCH     RECHARGE\n",
      "0.7          -10.0    SEARCH     RECHARGE\n",
      "0.7          -20.0    SEARCH     RECHARGE\n",
      "0.9           -1.0    SEARCH     RECHARGE\n",
      "0.9           -3.0    SEARCH     RECHARGE\n",
      "0.9           -6.0    SEARCH     RECHARGE\n",
      "0.9          -10.0    SEARCH     RECHARGE\n",
      "0.9          -20.0    SEARCH     RECHARGE\n"
     ]
    }
   ],
   "source": [
    "# EKSPERYMENT 2: alfa vs rescue_cost\n",
    "\n",
    "print(\"Ustawienia stałe:\", \"beta=\", beta_default, \"r_search=\", r_search_default,\n",
    "      \"r_wait=\", r_wait_default, \"gamma=\", gamma_default)\n",
    "print(\"alpha   rescue_cost   pi*(H)      pi*(L)\")\n",
    "print(\"----   ----------    --------    --------\")\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    for rescue_cost in rescue_list:\n",
    "\n",
    "        P, _, _ = build_recycling_robot_P(alpha=alpha, rescue_cost=rescue_cost)\n",
    "        pi_star, _ = best_policy_robot(P)\n",
    "\n",
    "        aH = action_name[int(np.argmax(pi_star[0]))]\n",
    "        aL = action_name[int(np.argmax(pi_star[1]))]\n",
    "\n",
    "        print(f\"{alpha:0.1f}     {rescue_cost:>10.1f}    {aH:<8}   {aL:<8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a7676",
   "metadata": {},
   "source": [
    "### Obserwacje:\n",
    "- stan H jest taki sam niezależnie od parametrów alpha i rescue cost, tak samo jak w poprzednim przykładzie. Chociaż tu jest to na tyle ciekawe że nawet niskie wartości alpha tego nie zmieniają\n",
    "- dla dużego alpha agent mocno preferuje recharge w stanie L, co może wskazywać na to że szybko się kończy energia potrzebna do zzostania w stanie H, więc częstsze recharge są potrzebne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1366a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustawienia stałe: rescue_cost= -3.0 r_search= 5.0 r_wait= 1.0 gamma= 0.9\n",
      "alpha   beta         pi*(H)      pi*(L)\n",
      "----   ----------    --------    --------\n",
      "0.1            0.1    SEARCH     RECHARGE\n",
      "0.1            0.3    SEARCH     RECHARGE\n",
      "0.1            0.5    SEARCH     RECHARGE\n",
      "0.1            0.7    SEARCH     SEARCH  \n",
      "0.1            0.9    SEARCH     SEARCH  \n",
      "0.3            0.1    SEARCH     RECHARGE\n",
      "0.3            0.3    SEARCH     RECHARGE\n",
      "0.3            0.5    SEARCH     RECHARGE\n",
      "0.3            0.7    SEARCH     SEARCH  \n",
      "0.3            0.9    SEARCH     SEARCH  \n",
      "0.5            0.1    SEARCH     RECHARGE\n",
      "0.5            0.3    SEARCH     RECHARGE\n",
      "0.5            0.5    SEARCH     RECHARGE\n",
      "0.5            0.7    SEARCH     SEARCH  \n",
      "0.5            0.9    SEARCH     SEARCH  \n",
      "0.7            0.1    SEARCH     RECHARGE\n",
      "0.7            0.3    SEARCH     RECHARGE\n",
      "0.7            0.5    SEARCH     RECHARGE\n",
      "0.7            0.7    SEARCH     SEARCH  \n",
      "0.7            0.9    SEARCH     SEARCH  \n",
      "0.9            0.1    SEARCH     RECHARGE\n",
      "0.9            0.3    SEARCH     RECHARGE\n",
      "0.9            0.5    SEARCH     RECHARGE\n",
      "0.9            0.7    SEARCH     RECHARGE\n",
      "0.9            0.9    SEARCH     SEARCH  \n"
     ]
    }
   ],
   "source": [
    "# EKSPERYMENT 3: Wpływ alpha i beta\n",
    "\n",
    "print(\"Ustawienia stałe:\", \"rescue_cost=\", rescue_cost_default, \"r_search=\", r_search_default,\n",
    "      \"r_wait=\", r_wait_default, \"gamma=\", gamma_default)\n",
    "print(\"alpha   beta         pi*(H)      pi*(L)\")\n",
    "print(\"----   ----------    --------    --------\")\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    for beta in beta_list:\n",
    "\n",
    "        P, _, _ = build_recycling_robot_P(alpha=alpha, beta=beta)\n",
    "        pi_star, _ = best_policy_robot(P)\n",
    "\n",
    "        aH = action_name[int(np.argmax(pi_star[0]))]\n",
    "        aL = action_name[int(np.argmax(pi_star[1]))]\n",
    "\n",
    "        print(f\"{alpha:0.1f}     {beta:>10.1f}    {aH:<8}   {aL:<8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfbcec6",
   "metadata": {},
   "source": [
    "### Obserwacje:\n",
    "- nadal dominuje search w H\n",
    "- duże beta (> 0.7) sprzyja polityce search w stanie L, pokazuje to że dopiero wtedy ryzyko szukania bez recharge wydaje się warte, inaczej recharge jest preferowane\n",
    "- ogólnie widać że głównie beta wpływa na wybór search w L, co ma sens patrząc na to że beta pokazuje szansę sukcesu szukania w L, a alpha zzamiast tego w H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaabb801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustawienia stałe: rescue_cost= -3.0 alpha= 0.8 beta= 0.4 gamma= 0.9\n",
      "r_search r_wait         pi*(H)      pi*(L)\n",
      "----    ----------    --------    --------\n",
      "0.5            0.0    SEARCH     RECHARGE\n",
      "0.5            1.0    WAIT       SEARCH  \n",
      "0.5            3.0    WAIT       SEARCH  \n",
      "0.5            5.0    WAIT       SEARCH  \n",
      "2.0            0.0    SEARCH     RECHARGE\n",
      "2.0            1.0    SEARCH     RECHARGE\n",
      "2.0            3.0    WAIT       SEARCH  \n",
      "2.0            5.0    WAIT       SEARCH  \n",
      "5.0            0.0    SEARCH     RECHARGE\n",
      "5.0            1.0    SEARCH     RECHARGE\n",
      "5.0            3.0    SEARCH     RECHARGE\n",
      "5.0            5.0    SEARCH     WAIT    \n",
      "10.0            0.0    SEARCH     RECHARGE\n",
      "10.0            1.0    SEARCH     RECHARGE\n",
      "10.0            3.0    SEARCH     RECHARGE\n",
      "10.0            5.0    SEARCH     RECHARGE\n"
     ]
    }
   ],
   "source": [
    "# EKSPERYMENT 4: Wpływ nagród r_search i r_wait za akcje\n",
    "\n",
    "print(\"Ustawienia stałe:\", \"rescue_cost=\", rescue_cost_default, \"alpha=\", alpha_default,\n",
    "      \"beta=\", beta_default, \"gamma=\", gamma_default)\n",
    "print(\"r_search r_wait         pi*(H)      pi*(L)\")\n",
    "print(\"----    ----------    --------    --------\")\n",
    "\n",
    "for r_search in r_search_list:\n",
    "    for r_wait in r_wait_list:\n",
    "\n",
    "        P, _, _ = build_recycling_robot_P(r_search=r_search, r_wait=r_wait)\n",
    "        pi_star, _ = best_policy_robot(P)\n",
    "\n",
    "        aH = action_name[int(np.argmax(pi_star[0]))]\n",
    "        aL = action_name[int(np.argmax(pi_star[1]))]\n",
    "\n",
    "        print(f\"{r_search:0.1f}     {r_wait:>10.1f}    {aH:<8}   {aL:<8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93078d5f",
   "metadata": {},
   "source": [
    "### Obserwacje:\n",
    "- w odróżnieniu od wcześniejszych przykładów, widzimy wreszcie zmiany w H, pokazuje to że zmiany nagród mają tutaj większe znaczenie\n",
    "- całościowo też pierwszy raz pojawiaja się WAIT - dla niskiego r_search w H (bo wtedy szukanie nie jest tak bardzo nagradzane) i z jakiegoś powodu dla r_search i r_wait = 5.0. Myślę że można to podsumować jako - jeśli nagroda zza czekanie jest podobna lub większa niż nagroda za szukanie, zaczyna pojawiać się dominancja stanu wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "282abb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustawienia stałe: beta= 0.4 r_search= 5.0 r_wait= 1.0 alpha= 0.8\n",
      "gamma   rescue_cost   pi*(H)      pi*(L)\n",
      "----   ----------    --------    --------\n",
      "0.20           -1.0    SEARCH     SEARCH  \n",
      "0.20           -3.0    SEARCH     WAIT    \n",
      "0.20           -6.0    SEARCH     WAIT    \n",
      "0.20          -10.0    SEARCH     WAIT    \n",
      "0.20          -20.0    SEARCH     WAIT    \n",
      "0.50           -1.0    SEARCH     SEARCH  \n",
      "0.50           -3.0    SEARCH     RECHARGE\n",
      "0.50           -6.0    SEARCH     RECHARGE\n",
      "0.50          -10.0    SEARCH     RECHARGE\n",
      "0.50          -20.0    SEARCH     RECHARGE\n",
      "0.70           -1.0    SEARCH     SEARCH  \n",
      "0.70           -3.0    SEARCH     RECHARGE\n",
      "0.70           -6.0    SEARCH     RECHARGE\n",
      "0.70          -10.0    SEARCH     RECHARGE\n",
      "0.70          -20.0    SEARCH     RECHARGE\n",
      "0.85           -1.0    SEARCH     RECHARGE\n",
      "0.85           -3.0    SEARCH     RECHARGE\n",
      "0.85           -6.0    SEARCH     RECHARGE\n",
      "0.85          -10.0    SEARCH     RECHARGE\n",
      "0.85          -20.0    SEARCH     RECHARGE\n",
      "0.90           -1.0    SEARCH     RECHARGE\n",
      "0.90           -3.0    SEARCH     RECHARGE\n",
      "0.90           -6.0    SEARCH     RECHARGE\n",
      "0.90          -10.0    SEARCH     RECHARGE\n",
      "0.90          -20.0    SEARCH     RECHARGE\n",
      "0.95           -1.0    SEARCH     RECHARGE\n",
      "0.95           -3.0    SEARCH     RECHARGE\n",
      "0.95           -6.0    SEARCH     RECHARGE\n",
      "0.95          -10.0    SEARCH     RECHARGE\n",
      "0.95          -20.0    SEARCH     RECHARGE\n",
      "0.99           -1.0    SEARCH     RECHARGE\n",
      "0.99           -3.0    SEARCH     RECHARGE\n",
      "0.99           -6.0    SEARCH     RECHARGE\n",
      "0.99          -10.0    SEARCH     RECHARGE\n",
      "0.99          -20.0    SEARCH     RECHARGE\n"
     ]
    }
   ],
   "source": [
    "# EKSPERYMENT 5: Wpływ kary za awarię i dyskontowania\n",
    "\n",
    "print(\"Ustawienia stałe:\", \"beta=\", beta_default, \"r_search=\", r_search_default,\n",
    "      \"r_wait=\", r_wait_default, \"alpha=\", alpha_default)\n",
    "print(\"gamma   rescue_cost   pi*(H)      pi*(L)\")\n",
    "print(\"----   ----------    --------    --------\")\n",
    "\n",
    "for gamma in gamma_list:\n",
    "    for rescue_cost in rescue_list:\n",
    "\n",
    "        P, _, _ = build_recycling_robot_P(rescue_cost=rescue_cost)\n",
    "        pi_star, _ = best_policy_robot(P, gamma=gamma)\n",
    "\n",
    "        aH = action_name[int(np.argmax(pi_star[0]))]\n",
    "        aL = action_name[int(np.argmax(pi_star[1]))]\n",
    "\n",
    "        print(f\"{gamma:0.2f}     {rescue_cost:>10.1f}    {aH:<8}   {aL:<8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333aef93",
   "metadata": {},
   "source": [
    "### Obserwacje:\n",
    "- stan H jest znowu zdominowany przezz search\n",
    "- niskie gamma pokazuje że agent ma dosyć agresyną politykę, co jest spowodowane tym że działa w małym przedziale czasowym i że tak powiem nie przejmuje się tak bardzzo przyszłością (jest ogólnie krótkowzroczny). Unika wtedy recharge bo to bardziej długoterminowa korzyść i czasami w stanie L ucieka się do czekania, pewnie żeby odroczyć decyzję pry wysokim rescue_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5eb7875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ustawienia stałe: beta= 0.4 r_search= 5.0 r_wait= 1.0 alpha= 0.8 rescue_cost= -3.0\n",
      "gamma   pi*(H)      pi*(L)\n",
      "----    --------    --------\n",
      "0.20    SEARCH     WAIT    \n",
      "0.50    SEARCH     RECHARGE\n",
      "0.70    SEARCH     RECHARGE\n",
      "0.85    SEARCH     RECHARGE\n",
      "0.90    SEARCH     RECHARGE\n",
      "0.95    SEARCH     RECHARGE\n",
      "0.99    SEARCH     RECHARGE\n"
     ]
    }
   ],
   "source": [
    "# EKSPERYMENT 6: Wpływ samego gamma\n",
    "\n",
    "print(\"Ustawienia stałe:\", \"beta=\", beta_default, \"r_search=\", r_search_default,\n",
    "      \"r_wait=\", r_wait_default, \"alpha=\", alpha_default, \"rescue_cost=\", rescue_cost_default)\n",
    "print(\"gamma   pi*(H)      pi*(L)\")\n",
    "print(\"----    --------    --------\")\n",
    "\n",
    "for gamma in gamma_list:\n",
    "\n",
    "    P, _, _ = build_recycling_robot_P()\n",
    "    pi_star, _ = best_policy_robot(P, gamma=gamma)\n",
    "\n",
    "    aH = action_name[int(np.argmax(pi_star[0]))]\n",
    "    aL = action_name[int(np.argmax(pi_star[1]))]\n",
    "\n",
    "    print(f\"{gamma:0.2f}    {aH:<8}   {aL:<8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce23e2e",
   "metadata": {},
   "source": [
    "### Obserwacje:\n",
    "- jak wyżej, zmiany od \"standarodwej\" polityki search w H i recharge w L widać głównie przy małym gamma. Jest to spowodowane \"krótkowzrocznością\" agenta przy niskim gamma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
